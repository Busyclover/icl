---
title: "The Network Structure of the London Underground and Alternative Zoning Approaches"
author: "Jim Leach"
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    keep_md: yes
    theme: readable
    toc: yes
    toc_float: yes
  word_document:
    toc: no
bibliography:
- refs/refs.bib
- refs/extra.bib
---

```{r knitr_setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      include = T, eval = T, echo = T)
samples <- 5
figure <- 1
table <- 1
```

<br>

<p style="border:1.5px; border-style:solid; border-color:#000000; padding: 1em;">This `HTML` report should be viewed using a modern web browser such as Mozilla Firefox or Google Chrome. Printing is possible but will not produce an optimal reading experience. The `R` code used to perform this analysis can be viewed in this report using the _Code_ buttons to toggle code viewing.</p>

<br>

# Report overview

#### Details

* Name: James Leach
* CID: 01135629
* Title: The Network Structure of the London Underground and Alternative Zoning Approaches
* GitHub Repository: [jim89/icl/final_report](https://github.com/Jim89/icl/tree/master/final_report)
* Word Count: TODO


#### Document structure

This document is broken down in the several sections:

The [introduction](#introduction) presents some background, discusses the motivating issue for this work, provides a brief summary of some relevant literature, and briefly outlines the approach and how it builds on existing literature. 

The [methodology](#methodology) section describes the analytical steps, and provides the `R` code used in the analysis to make this work reproducible (the necessary data are also provided). 

The [results](#results) section contains summary statistics and visualisations for the analysis, and the conclusion provides some final remarks and recommendations. 

Additional tables and visualisations are presented in the [appendices](#appendices) and a full list of [references](#references) is provided. 

<br>

# Abstract

_Building on existing theoretical and analytical literature, this report presents the results of an analysis into developing new fare zones for the Transport for London ("TfL") Underground ("tube") network. This analysis has been performed in order to understand if novel, data-driven methods could be used to create fare zones on the tube._

_TfL will loose its government-provided, operational subsidy in 2019, and is facing a significant financial tightening as a result. New fare zones, could help to offset some of this tightening, providing increased revenues without significantly impacting customers._

_Modelling the tube network as a graph, zones were created using node-importance metrics as well as traditional community-detection algorithms. The initial results are promising: the new approaches show good results when considered both financially and in graph-theoretical terms._

_However, work remains to be done if methods such as these are to be implemented by TfL. The socio-economic and political impact of changes investigated here would be required (an analysis that was beyond the scope of this report) to understand all the strengths and weaknesses that methods such as these present._

# Introduction

## Background

### Transport for London

[Transport for London](https://tfl.gov.uk/) ("TfL") is a government body responsible for almost all roads and buses, many train services, as well as other forms of public transport (including taxis and boats) in the UK capital. 

Operating in a city of around 8 million people [@census_comparing_2015], TfL also has a huge financial responsibility. In the 2015 financial year, TfL reported total revenue of £5.289 billion, of which £2.559 billion came directly from ticket fares on the London Underground [@tfl_annual_2015]. One of the most famous aspects of the London transport system (along with the iconic "black cab" taxis), the Underground ("tube") operates across 11 train lines, 270 stations, and carried 1.34 billion passengers in 2015/16 [@tfl_facts_2016]. 

### Fares & subsidisation

Fares are often set lower than the costs of running services [@montella_multimodal_2014], setting up a requirement for some kind of subsidy, and TfL is no exception. 

Whilst setting fares lower than costs can be part of a capacity-management strategy [@turvey_simple_1975] (i.e. encouraging greater use of the service), this is likely not the case for TfL which has seen demand increase year-on-year. Based on current figures it is likely that (without significant service improvements) demand will outstrip capacity within the next twenty years [@_tube_2016]. 

However, as noted by Cervero [-@cervero_flat_1981], subsidisation can lead a sequence of events that, over time, actually reduce revenue. Fares kept low through subsidisation result in relative fare revenue decreasing as costs increase. This fact means that subsidisation continues to be a requirement. 

Political pressures have resulted in efforts by TfL to keep fares low, and TfL is a heavily subsidised organisation. A significant portion of its yearly income comes in the form of a subsidy direct from central government: an estimated 23% in 2015/16 [@tfl_how_2015].

### Motivating Issue

TfL will loose its operational subsidy from 2019 onwards [@tfl_annual_2015]. 

It _must_, therefore, seek actions which dramatically cut its costs, or grow its income. With demand pressures building, and political pressure to maintain low prices [@_sadiq_2016], this may prove to be a difficult prospect. 

Transport for London must seek new, innovative strategies to maintain the infrastructure that is so critical to the smooth running of the capital whilst operating in an increasingly tight financial situation.

## Literature Review

### Pricing public transport

The pricing of public transport has long been studied. Noting that pricing required a fine balancing of managing demand and capacity, @turvey_simple_1975 found that distance-based pricing was preferable to flat-fares with longer trips requiring larger fares. This principal is the basis for the current fare zones employed by TfL. 

@cervero_flat_1981 introduced the concept of 'fairness' in how customers subsidise each other. He found that, under certain models, short-distance customers could end up subsidising those on longer journeys (something he did not regard as fair) whereas other models (such as peak vs. off-peak prices) were better at eliminating this effect. Again, these principals have been incorporated in to the current pricing schemes that TfL uses. Cervero also discussed how the fare-model choice was highly dependent on the policy objective of the operator. For example, 'fare comprehension' fares would be greatly different to 'deficit reduction' fares. 

@daskin_quadratic_1988 built on this work, and provided a list of requirements for pricing schemes. They should be:

* sensitive to the many issues involved in fares (both from the provider's _and_ the user's perspective);
* based on appropriate economic, political, and analytical theory;
* easy for the end-user to understand;
* feasible to implement; and
* easy to test and analyse in the presence of alternatives.

Daskin et. al. developed a model to test several alternative pricing models, and found that zone-based pricing was most optimal when price-sensitivity was not strongly related to trip distance. 

@van_vuuren_optimal_2002 reported that customers were much less sensitive to price during peak hours than outside of these times (stating 'commuters usually do not have many alternatives').  

@sharaby_impact_2012 investigated how fare integration (for example, TfL's Oyster scheme) affected travel behaviour. Fare integration is defined as the ability to "transfer" within the transport system from one form of transport to another (e.g. from bus to train), or between multiple routes in one form of transport (e.g. changing tube lines), without incurring a cost penalty. They found that the use of fare integration, along with a zone-based pricing policy led to increased ticket purchases along with increased customer satisfaction. However, in their study, integrated pricing was also coupled with an overall reduction in fares.
 
### Zone-based pricing

Zone-based pricing is the idea that a transport network be divided in to zones based on some measure, either analytical, political, administrative, or socio-economic. Journey prices are based on the number of zones crossed, more zones (i.e. the greater the distance travelled) generally meaning a more expensive journey. 

@montella_multimodal_2014 provided a brief overview how fare zones are created  and provided a four step process to finalising the fare zones (this study being focussed on the first of these):

> 1. Define initial zones
2. Optimise of fares across these zones
3. Evaluate the results
4. Use the results to find the best zone/fare combination

They noted three main types of zones: concentric rings; circular rings/sectors (similar to concentric rings, but with radial "cuts" in the rings to define new zones); and alveolar zones (zones defined across the area). 

Montella and D'Acierno noted that concentric ring zones are typically used in capital cities where there is a
single, central district of increased importance surrounded by areas which decrease in importance the further they are from the centre. Fares are then charged based on how important the zones entered/exited are (i.e. trips to the centre are more expensive). Transport for London has used such an approach since the 1980's [@_london_2016].

Highly complex arrangements of fare zones are not practical [@jansson_is_2012]. Considering the city and suburbs surrounding Oslo, Jansson and Angell showed that zone simplification was possible: Oslo previously had eighty-eight fare zones before simplifying down to just ten. They also derived formulae for the optimal fare based on the operator's cost(s), the (social and financial) cost(s) associated with extra passengers using the service, and the journey distance. They found that optimal prices can be approximated with a zone-based system with travel to more important zones (in their context: '[zones] closer to the city centre') incurring higher fares. 

@jansson_is_2012 also described several alternative zoning approaches which they investigated. Their results were as follows:

* __Ring zones__ resulted in only small increases in price for travelling between zones, but generated too high a number of zones.
* __Dense, suburban, and rural zones__ suffered from a similar problem to ring zones: in order to keep "jumps" in fare for crossing zones minimal, too many zones had to be introduced in the city centre.
* __A large central, with small surrounding zones__ was found to be successful. This model simplified transit in the both the city centre (where the majority of journeys occurred) and in the suburbs. However, there were more significant jumps in fare for travelling between zones.
* __Three main zones__ spanning the entire area was _very_ simple, but found to reduce fare revenue so much that extensive subsidisation was required.
* __A single zone, with distance-based pricing__ was also simple to implement and easy for customers to understand, but resulted in high prices for short trips (typically those in the city centre) which would have been unpopular.
* __Munipal zones__ based on existing political boundaries were also easy for customers to understand, but resulted in some short journeys crossing many zones (and therefore being expensive), and long journeys remaining in just one zone (and therefore being cheap). This countered existing transport pricing theory, and so was seen as a negative result.

### Smart-cards, data & analytics

After its public launch [@ctt_londons_2003], TfL's Oyster smart-card scheme grew massively. Oyster card technology was soon used in all London train stations [@ctt_foundations_2006] and the majority of journeys were made using an Oyster card [@ctt_10_2007]. 

@mcdonald_multipurpose_2000 provided an overview of some key benefits of smart-card technologies, finding that they reduce fraud, ticket failure and fare collection expenses; and increase ease-of-use for the customer. They were also found to enable the use of flexible fare polices, something also noted by [@borndorfer_models_2012].

@chen_promises_2016 used smart-card data to investigate travel patterns and how they can be used to guide investment. Whilst recognising the need to validate assumptions and inferences, they also noted that combining this data with other sources (including mobile device tracking data) could be critical for understanding route choices, or for inferring journey purposes. @pelletier_smart_2011 performed similar work using smart-card data to group passengers based on the similarity in their travel patterns and to investigate the differences in travel on weekdays and weekends.

@bagchi_potential_2005, however, noted that analysis of smart-card data should not be considered a 'utopian[n] replacement' for traditional approaches, and showed that smart-card data should be supplemented with other sources (e.g. customer surveys) in order to develop a holistic approach to understanding the transport system.

### Graphs and importance

@yeung_physics_2013 used graph analysis to perform route optimisation for the London Underground, finding that the "best" route was dependant not only on the network structure, but also on the routes taken by other users of the network. Although not focussed on transport, @yang_defining_2015 provided a set of useful criteria for evaluation communities/groups detected in graphs.

Similarly, the measurement of station importance is also not revolutionary. @shimamoto_evaluating_2008 sought to define important stations based on the risks of overcrowding at stations when failures occurred in the network. Modelling both passenger arrival and "failure" to board a train probabilistically, they found that certain key stations (namely Victoria) were found to be the most important in a number of different scenarios.

## Approach

Building on the literature summarised above, this analysis sought a new approach to designing fare zones. These approaches extend distance-based price theory and also include measures such as origin/destination importance into the zoning approach. 

Graph-analysis methods have been used to model the TfL Underground network as a graph with stations as _nodes_ [@NewmanBook:2010] and links between adjacent stations as _edges_ [@NewmanBook:2010]. The creation of fare zones - the first step in creating a pricing model @montella_multimodal_2014 - using the graph properties of the network has been explored and the results evaluated. 

This was done to understand if data-driven methods could be used as one of the "innovative strategies" TfL will require in order to meet its new financial requirements following the loss of its subsidy: re-zoning could provide the basis for changing pricing in order to increase fare revenue.

Using open-source data of the graph structure of the tube network and smart-card-enabled journeys through it, two main approaches were developed:

* _Designing zones based on station importance_: Using common algorithms for identifying important nodes, stations were grouped based on their importance.
* _Designing zones based on graph communities_: Using community-detection algorithms, groups of stations were found in the network.

In each case the resulting groups were used to define new, alveolar fare zones [@montella_multimodal_2014] and the results compared with the existing fare zones using a variety of methods.

Making an assumption that inter-zone (for example zone 1 to zone 2) fares remained constant between the current zones and the new approaches, differences in fare revenue between the new approaches and existing fares were estimated and compared. Standard techniques for measuring similarity (via Salton's cosine method [-@Salton:1986:IMI:576628]) were also used to compare the new and existing zones. 

For zones defined through community-detection algorithms, graph theoretical measures proposed by @yang_defining_2015 were also used to evaluate the new zones. 

Finally, the zones generated via these alternative approaches were rated against criteria set out by @daskin_quadratic_1988 for measuring the suitability of pricing systems.

The use of the tube network graph structure and smart-card data in this way also builds on the existing analytical literature, including efforts to use graph theory for route optimisation [@yeung_physics_2013], and the evaluation of the importance of stations in the network [@shimamoto_evaluating_2008].

# Methodology

```{r setup, echo = FALSE}
# Load packages
library(igraph)
library(readr)
library(readxl)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(purrr)
library(DT)
library(networkD3)
library(leaflet)
library(knitr)
library(ggbeeswarm)

# Set up colour objects
bakerloo <- rgb(red = 137, green = 78, blue = 36, maxColorValue = 255)
central <- rgb(red = 220, green = 36, blue = 31, maxColorValue = 255)
circle <- rgb(red = 255, green = 206, blue = 0, maxColorValue = 255)
district <- rgb(red = 0, green = 114, blue = 41, maxColorValue = 255)
hc <- rgb(red = 215, green = 153, blue = 175, maxColorValue = 255)
jubilee <- rgb(red = 134, green = 143, blue = 152, maxColorValue = 255)
metropolitan <- rgb(red = 117, green = 16, blue = 86, maxColorValue = 255)
northern <- rgb(red = 0, green = 0, blue = 0, maxColorValue = 255)
picadilly <- rgb(red = 0, green = 25, blue = 168, maxColorValue = 255)
victoria <- rgb(red = 0, green = 160, blue = 226, maxColorValue = 255)
wc <- rgb(red = 118, green = 208, blue = 189, maxColorValue = 255)
dlr <- rgb(red = 0, green = 175, blue = 173, maxColorValue = 255)
overground <- rgb(red = 232, green = 106, blue = 16, maxColorValue = 255)

tfl_colour <- c(bakerloo, central, circle, district,
            hc, jubilee, metropolitan, northern, picadilly,
            victoria, wc, dlr, overground)

# Set up ggplot2 theme object for prettier plots
theme_jim <-  theme(legend.position = "bottom",
                    axis.text.y = element_text(size = 16, colour = "black"),
                    axis.text.x = element_text(size = 16, colour = "black"),
                    legend.text = element_text(size = 16),
                    legend.title = element_text(size = 16),
                    title = element_text(size = 16),
                    strip.text = element_text(size = 16, colour = "black"),
                    strip.background = element_rect(fill = "white"),
                    panel.grid.minor.x = element_blank(),
                    panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
                    panel.grid.minor.y = element_line(colour = "lightgrey", linetype = "dotted"),
                    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
                    panel.margin.y = unit(0.1, units = "in"),
                    panel.background = element_rect(fill = "white", colour = "lightgrey"),
                    panel.border = element_rect(colour = "black", fill = NA))
```

## Data Used

The following data were obtained and used for the purposes of this analysis:

* Transport for London Underground Station adjacency, fare zone and line information available as open-source data hosted on [Wikimedia](https://commons.wikimedia.org/wiki/London_Underground_geographic_maps/CSV#Routes) [-@wikimedia_london_2009]. Referred to as the "raw graph structure data"; 
* Transport for London Underground inter-station distances [@tfl_distances_2013], made available by Transport for London under a Freedom of Information request in [2013](https://www.whatdotheyknow.com/request/distances_between_adjacent_oyste). Referred to as the "distances data"; and
* Sample journey data provided by Transport for London [-@tfl_oyster_2009]. The data cover 5% of all Oyster card journeys made in one week in November 2009. The data are available through the Transport for London [API](https://tfl.gov.uk/info-for/open-data-users/our-feeds#on-this-page-6). Referred to as the "journey data".

Examples of these data can be seen in [appendix one](#one). The list of `R` packages used to perform this analysis can be found in [appendix two](#two).

## Graph creation

### Prepare and clean data

The raw graph structure [@wikimedia_london_2009] and the distances data [@tfl_distances_2013] were loaded and some basic processing and cleaning (e.g. standardisation of station names) was performed.

```{r load_and_clean}
# Load the data
### Graph structure
# Basic linkage data from Wiki
adjacency <- read_csv("./data/geo/adjacency.csv",
                      col_types = cols(station1 = col_integer(),
                                       station2 = col_integer(),
                                       line = col_integer()))

# Line lookup values
station_lk <- read_csv("./data/geo/station_lk.csv",
                       col_types = cols(line = col_integer(),
                                        name = col_character(),
                                        colour = col_character(),
                                        stripe = col_character()))

# Station details
station_details <- read_csv("./data/geo/stations_geo.csv",
                            col_types = cols(id = col_integer(),
                                             latitude = col_double(),
                                             longitude = col_double(),
                                             name = col_character(),
                                             display_name = col_character(),
                                             zone = col_double(),
                                             total_lines = col_integer(),
                                             rail = col_integer()))

### Distances data from FOI
# DLR name lookup
dlr_abbr <- read_excel("./data/distances/formatted/FOI Request Station Abbreviations_CLN.xls")

# Distances between stations
stations_dist <- read_excel("./data/distances/formatted/Inter Station Train Times_CLN.xls")

# DLR distance matrix
dlr_dist <- read_excel("./data/distances/formatted/Distance Martix DLR 2013_CLN.xlsx")


# Clean the data
# Set up clean lower-case name, and rounded zone-number in station details
station_details <- station_details %>% mutate(name_cln = tolower(name),
                                              zone_cln = ceiling(zone))

# Set up adjacency list with names of stations, rather than ID as keys
links <- adjacency %>% left_join(station_details %>% select(id, name),
                                 by = c("station1" = "id")) %>% 
    select(-station1) %>% 
    rename(station1 = name) %>% 
    left_join(station_details %>% select(id, name),
              by = c("station2" = "id")) %>% 
    select(-station2) %>% 
    rename(station2 = name) %>% 
    mutate(station1 = tolower(station1),
           station2 = tolower(station2),
           station1 = str_trim(station1),
           station2 = str_trim(station2))

# Clean up DLR distances table
dlr_dist_long <- dlr_dist %>% gather(station, dist, -Metres) %>% 
    rename(from = Metres,
           to = station) %>% 
    left_join(dlr_abbr, by = c("from" = "abbr")) %>% 
    select(-from) %>% 
    rename(station1 = station) %>% 
    left_join(dlr_abbr, by = c("to" = "abbr")) %>% 
    select(-to) %>% 
    rename(station2 = station) %>% 
    na.omit() %>% 
    filter(dist > 0) %>% 
    mutate(dist = dist / 1000,
           line = "Docklands Light Railway") %>% 
    select(line, station1, station2, dist) %>% 
    ungroup()

# Clean up station-station distances table
station_dist_long <- stations_dist %>% 
    group_by(Line, `Station from (A)`, `Station to (B)`) %>% 
    summarise(dist = median(`Distance (Kms)`)) %>% 
    rename(line = Line,
           dist = dist,
           station1 = `Station from (A)`,
           station2 = `Station to (B)`) %>% 
    ungroup()

# Standardise station names to match graph structure data
distances <- bind_rows(dlr_dist_long, station_dist_long) %>% 
    mutate(station1 = tolower(station1),
           station2 = tolower(station2),
           station1 = str_trim(station1),
           station2 = str_trim(station2),
           station1 = gsub("edgware", "edgeware", station1),
           station2 = gsub("edgware", "edgeware", station2),
           station1 = gsub("regents park", "regent's park", station1),
           station2 = gsub("regents park", "regent's park", station2),
           station1 = gsub("piccadilly", "picadilly", station1),
           station2 = gsub("piccadilly", "picadilly", station2),
           station1 = gsub("st james park", "st. james's park", station1),
           station2 = gsub("st james park", "st. james's park", station2),
           station1 = gsub("kings cross|kings cross st pancras", "king's cross st. pancras", station1),
           station2 = gsub("kings cross|kings cross st pancras", "king's cross st. pancras", station2),
           station1 = gsub("earls court", "earl's court", station1),
           station2 = gsub("earls court", "earl's court", station2),
           station1 = gsub("highbury & islington", "highbury", station1),
           station2 = gsub("highbury & islington", "highbury", station2),
           station1 = gsub("paddington \\(.*\\)", "paddington", station1),
           station2 = gsub("paddington \\(.*\\)", "paddington", station2),
           station1 = gsub("st johns wood", "st. john's wood", station1),
           station2 = gsub("st johns wood", "st. john's wood", station2),
           station1 = gsub("queens park", "queen's park", station1),
           station2 = gsub("queens park", "queen's park", station2),
           station1 = gsub("heathrow 123", "heathrow terminals 1, 2 & 3", station1),
           station2 = gsub("heathrow 123", "heathrow terminals 1, 2 & 3", station2),
           station1 = gsub("heathrow four", "heathrow terminal 4", station1),
           station2 = gsub("heathrow four", "heathrow terminal 4", station2),
           station1 = gsub("hammersmith \\(.*\\)", "hammersmith", station1),
           station2 = gsub("hammersmith \\(.*\\)", "hammersmith", station2),
           station1 = gsub("st pauls", "st. paul's", station1),
           station2 = gsub("st pauls", "st. paul's", station2))
```

After ensuring that (wherever possible) all station-station distances were present, these data were joined with the data of the graph structure. The graph structure _data_ was then transformed in to a graph object, retaining the distance between adjacent stations as an edge attribute.

```{r create_initial_graph}
# Create "reversed" distances to account for possible different combos of stations
distances <- bind_rows(distances, distances %>% 
                           select(line, station2, station1, dist) %>% 
                           rename(station3 = station1,
                                  station1 = station2) %>% 
                           rename(station2 = station3))

# Add distances to station linkages data
links <- links %>% 
    left_join(distances %>% select(-line), by = c("station1" = "station1",
                                "station2" = "station2"))

# Clean up missing distances - set to average
links <- links %>%
    mutate(dist = round(ifelse(is.na(dist), 
                               mean(links$dist, na.rm = T), 
                               dist),
                        2)) %>% 
    distinct()


# Create "reversed" links to account for bi-directional travel!
links <- bind_rows(links, links %>% 
                            select(line, station2, station1, dist) %>% 
                            rename(station3 = station1,
                                   station1 = station2) %>% 
                            rename(station2 = station3))

# Clean up intermediate tables)
rm(dlr_abbr, dlr_dist, dlr_dist_long, station_dist_long, adjacency,
   stations_dist)

# Chop out line and distinct
graph_data <- links %>% 
    distinct() %>% 
    select(-line)

# Make the graph - retain distance as edge attribute but do not weight edges
tfl_graph <- graph_data %>% 
    graph_from_data_frame(directed = TRUE)

# Clean up
rm(graph_data)
```

The journey data [@tfl_oyster_2009] were also loaded and filtered. These data contained a sample of _all_ journeys made with an Oyster card in one week of November 2009 (including bus journeys). As such these data were filtered to include only those journeys made on the London Underground, and only those that were pay-as-you-go ("PAYG"). 

```{r load_filter_journeys, cache = T}
# Journey data
journeys <- read_csv("./data/journeys/Nov09JnyExport.csv",
                     col_types = cols(downo = col_integer(),
                                      daytype = col_character(),
                                      SubSystem = col_character(),
                                      StartStn = col_character(),
                                      EndStation = col_character(),
                                      EntTime = col_integer(),
                                      EntTimeHHMM = col_character(),
                                      ExTime = col_integer(),
                                      EXTimeHHMM = col_character(),
                                      ZVPPT = col_character(),
                                      JNYTYP = col_character(),
                                      DailyCapping = col_character(),
                                      FFare = col_integer(),
                                      DFare = col_integer(),
                                      RouteID = col_character(),
                                      FinalProduct = col_character()))
names(journeys) <- names(journeys) %>% tolower()

# Filter to just completed tube journeys that were PAYG
journeys <- journeys %>%
     filter(subsystem == "LUL",
            startstn != "Unstarted",
            endstation != "Unfinished",
            endstation != "Not Applicable",
            finalproduct == "PAYG")
```

Further cleaning and standardisation of station names was performed to ensure that they matched those present in the graph structure data.

```{r clean_journeys, cache = TRUE}
# Clean up station names
journeys <- journeys %>% 
    # Start station names
    mutate(start_cln = startstn,
           start_cln = gsub("Earls Court", "Earl's Court", start_cln),
           start_cln = gsub("Highbury", "Highbury & Islington", start_cln),
           start_cln = gsub("St James's Park", "St. James's Park", start_cln),
           start_cln = gsub("St Pauls", "St. Paul's", start_cln),
           start_cln = gsub("Kings Cross [MT]", "King's Cross St. Pancras", start_cln),
           start_cln = gsub("Piccadilly Circus", "Picadilly Circus", start_cln),
           start_cln = gsub("Hammersmith [DM]", "Hammersmith", start_cln),
           start_cln = gsub("Bromley By Bow", "Bromley-By-Bow", start_cln),
           start_cln = gsub("Canary Wharf E2", "Canary Wharf", start_cln),
           start_cln = gsub("Edgware Road [BM]", "Edgware Road (B)", start_cln),
           start_cln = gsub("Great Portland St", "Great Portland Street", start_cln),
           start_cln = gsub("Waterloo JLE", "Waterloo", start_cln),
           start_cln = gsub("Shepherd's Bush Mkt", "Shepherd's Bush (H)", start_cln),
           start_cln = gsub("Shepherd's Bush Und", "Shepherd's Bush (C)", start_cln),
           start_cln = gsub("Harrow On The Hill", "Harrow-on-the-Hill", start_cln),
           start_cln = gsub("Harrow Wealdstone", "Harrow & Wealdston", start_cln),
           start_cln = gsub("Heathrow Term [45]", "Heathrow Terminal 4", start_cln),
           start_cln = gsub("Heathrow Terms 123", "Heathrow Terminals 1, 2 & 3", start_cln),
           start_cln = gsub("Tottenham Court Rd", "Tottenham Court Road", start_cln),
           start_cln = gsub("High Street Kens", "High Street Kensington", start_cln),
           start_cln = gsub("Regents Park", "Regent's Park", start_cln),
           start_cln = gsub("Queens Park", "Queen's Park", start_cln),
           start_cln = gsub("St Johns Wood", "St. John's Wood", start_cln),
           start_cln = gsub("Wood Lane", "White City", start_cln),
           start_cln = gsub("Totteridge", "Totteridge & Whetstone", start_cln),
           start_cln = gsub("Watford Met", "Watford", start_cln),
           start_cln = tolower(start_cln)) %>% 
    # End station names
    mutate(end_cln = endstation,
           end_cln = gsub("Earls Court", "Earl's Court", end_cln),
           end_cln = gsub("Highbury", "Highbury & Islington", end_cln),
           end_cln = gsub("St James's Park", "St. James's Park", end_cln),
           end_cln = gsub("St Pauls", "St. Paul's", end_cln),
           end_cln = gsub("Kings Cross [MT]", "King's Cross St. Pancras", end_cln),
           end_cln = gsub("Piccadilly Circus", "Picadilly Circus", end_cln),
           end_cln = gsub("Hammersmith [DM]", "Hammersmith", end_cln),
           end_cln = gsub("Bromley By Bow", "Bromley-By-Bow", end_cln),
           end_cln = gsub("Canary Wharf E2", "Canary Wharf", end_cln),
           end_cln = gsub("Edgware Road [BM]", "Edgware Road (B)", end_cln),
           end_cln = gsub("Great Portland St", "Great Portland Street", end_cln),
           end_cln = gsub("Waterloo JLE", "Waterloo", end_cln),
           end_cln = gsub("Shepherd's Bush Mkt", "Shepherd's Bush (H)", end_cln),
           end_cln = gsub("Shepherd's Bush Und", "Shepherd's Bush (C)", end_cln),
           end_cln = gsub("Harrow On The Hill", "Harrow-on-the-Hill", end_cln),
           end_cln = gsub("Harrow Wealdstone", "Harrow & Wealdston", end_cln),
           end_cln = gsub("Heathrow Term [45]", "Heathrow Terminal 4", end_cln),
           end_cln = gsub("Heathrow Terms 123", "Heathrow Terminals 1, 2 & 3", end_cln),
           end_cln = gsub("Tottenham Court Rd", "Tottenham Court Road", end_cln),
           end_cln = gsub("High Street Kens", "High Street Kensington", end_cln),
           end_cln = gsub("Regents Park", "Regent's Park", end_cln),
           end_cln = gsub("Queens Park", "Queen's Park", end_cln),
           end_cln = gsub("St Johns Wood", "St. John's Wood", end_cln),
           end_cln = gsub("Wood Lane", "White City", end_cln),
           end_cln = gsub("Totteridge", "Totteridge & Whetstone", end_cln),
           end_cln = gsub("Watford Met", "Watford", end_cln),
           end_cln = tolower(end_cln))

# Add on ID-information and zone
journeys <- journeys %>% 
    left_join(station_details %>% select(name_cln, id, zone_cln), 
              by = c("start_cln" = "name_cln")) %>% 
    rename(start_id = id,
           start_zone = zone_cln) %>% 
    left_join(station_details %>% select(name_cln, id, zone_cln),
              by = c("end_cln" = "name_cln")) %>% 
    rename(end_id = id,
           end_zone = zone_cln)
```

### Estimate journey routes

In order to model the graph structure with edges weighted by trips between adjacent stations, it was necessary to estimate routes taken through the network based on sample journey data. These routes could then be used to correctly weight edges in the graph based on the number of journeys utilising each edge (e.g. a journey from A-D might use edges A-B, B-C, and C-D and each of these edges required weighting). 

As the journey data only provided the start and end point, and not the route taken, it was necessary to determine the route for each journey. This was done by applying Dijkstra's [-@dijkstra_note_1959] algorithm to the initial graph structure, weighting the edges by the distance between adjacent stations as listed in the distances data. Whilst this method did not necessarily return the _exact_ route taken by the customer on each journey, it provided a useful approximation in the absence of such data.

Journeys in the journey data were de-duplicated, ensuring that a route for a particular journey was only calculated once. A simple function was defined to apply Dijkstra's algorithm and return the route path in an ordered list (with the first element corresponding to the starting station, the second element the second station, and so on up to the final station). 

```{r get_routes, cache = T}
# Dedupe journeys
distinct_journeys <- journeys %>% select(start_cln, end_cln) %>% distinct()

# Define a function that will calcualte the path and return it as an ordered list
get_shortest_path <- function(node1, node2, graph = tfl_graph) {
    path <- shortest_paths(graph, node1, node2, weights = E(graph)$dist)$vpath
    path <- path[[1]]
    path <- path %>% as.list() %>% names()
    return(path)
}

routes <- distinct_journeys %>% 
    mutate(path = map2(start_cln, end_cln, get_shortest_path)) %>% 
    unnest(path)

```

### Aggregate routes

The routes were then used to determine the number of daily trips between all adjacent stations in the graph. These daily trips were then to be used to weight the edges of the graph. 

For example, the route for a journey from Earl's Court to Pimlico was found to go from Earl's Court through Gloucester Road, South Kensington, Sloane Square, and Victoria to Pimlico. Therefore a single journey from Earl's Court to Pimlico needed to be used to add weight to several edges in the graph. 

It was therefore necessary to summarise and aggregate _journeys_ at a _route_ level to correctly weight the graph to be used for the analysis. Per journey, all pairs of adjacent stations in the path between the start and the end were determined.

```{r summarise_routes}
# Set up routes per journey   
routes <- routes %>% 
    group_by(start_cln, end_cln) %>% 
    mutate(to = lead(path, 1)) %>% 
    na.omit() %>% 
    rename(from = path)
```
   
The number of occurrences for all pairs of adjacent stations was then calculated. As the sample data covered 5% of journeys for a week long period, it was necessary to adjust the total counts to arrive at an approximate value for total daily trips between each pair of adjacent stations.

```{r summarise_route_journeys}    
# Total up all the routes and summarise trips between stations
routes <- journeys %>% 
    select(start_cln, end_cln) %>% 
    left_join(routes) %>% 
    group_by(from, to) %>% 
    summarise(daily_trips = n(),
              daily_trips = daily_trips / 7,
              daily_trips = ceiling(daily_trips * 20))
```

This summary was then joined with the graph structure data to add the daily trips between each pair of adjacent stations. Some pairs of adjacent stations in the graph structure were missing an estimation of the daily trips. This is because they were infrequently visited, outlying stations and no journeys were found to go through them. Such pairs of stations were set to have a number of trips equal to 25% of the average daily trips on their joining tube line.

```{r add_}
# Join in with links data
links <- links %>% 
    left_join(routes, by = c("station1" = "from",
                                   "station2" = "to"))
## Fill in the blanks
# Find average trips per line
avg_per_line <- links %>% 
    group_by(line) %>% 
    summarise(avg_trips = mean(daily_trips, na.rm = T)*0.25)

# Fill in the blanks with a coalesce
links <- links %>% left_join(avg_per_line) %>% 
    mutate(daily_trips = coalesce(daily_trips, avg_trips)) %>% 
    select(-avg_trips)
```

### Create final graph structure

The graph object was then re-created, this time using the daily trips between adjacent stations as the edge weight attribute. A visualisation of the resulting graph structure is presented in figure `r figure`.

```{r create_final_graph}
# Chop out line and distinct, weight by daily trips
graph_data <- links %>% 
    distinct() %>% 
    rename(weight = daily_trips) %>% 
    select(-line)

# Make the graph - retain distance as edge attribute but do not weight edges
tfl_graph <- graph_data %>% 
    graph_from_data_frame(directed = TRUE)

# Clean up
rm(graph_data)
```

#### Figure `r figure`: TfL Underground graph structure

_The graph structure of the TfL Underground in a force directed [@kobourov_spring_2012] layout. Nodes are sized based on the number of visits per day and coloured based on the fare zone which they are assigned to. Link widths are proportional to the number of route/journeys passing between the two adjoining stations each day. (Note: this figure is interactive)._

```{r show_graph}
# Get the data in the right form
d3_data <- igraph_to_networkD3(tfl_graph, group = rep(1, length(V(tfl_graph))))

# Convert to tibble and separate objects
d3_links <- as_data_frame(d3_data$links)
d3_nodes <- as_data_frame(d3_data$nodes)

# Add node id and convert name to character
d3_nodes <- d3_nodes %>% mutate(node_id = row_number()-1,
                                name = as.character(name))

# Join on the zone info to the nodes (stations)
d3_nodes <- d3_nodes %>% 
    left_join(station_details %>% select(name, name_cln, zone_cln),
              by = c("name" = "name_cln")) %>% 
    select(name, name.y, zone_cln, node_id) %>% 
    rename(name = name.y,
           name_cln = name,
           zone = zone_cln) %>% 
    mutate(name = paste(name, "- Zone", zone))

# Get estimated visits
daily_visits <- journeys %>% 
    select(start_cln, end_cln) %>% 
    gather(x, name_cln) %>% 
    count(name_cln) %>% 
    mutate(n = n/7,
           n = n*20) %>% 
    ungroup() %>% 
    mutate(n = (n - mean(n))/sd(n))

# Tag stations with daily visits
d3_nodes <- d3_nodes %>% left_join(daily_visits, by = "name_cln")

## Add daily_trips from links
# First need to add names to d3_links
d3_links <- d3_links %>% 
    left_join(d3_nodes %>% select(node_id, name_cln), 
              by = c("source" = "node_id")) %>% 
    rename(station1 = name_cln)

d3_links <- d3_links %>% 
    left_join(d3_nodes %>% select(node_id, name_cln), 
              by = c("target" = "node_id")) %>% 
    rename(station2 = name_cln)

# Add daily trips from links
d3_links <- d3_links %>% 
    left_join(links, by = c("station1", "station2")) %>% 
    select(source, target, daily_trips, dist)

# Set up custom colour scale
ColourScale <- 'd3.scale.ordinal()
            .domain(["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"])
           .range(["#1B9E77", 
                    "#D95F02", 
                    "#7570B3",
                    "#E7298A",
                    "#66A61E", 
                    "#E6AB02", 
                    "#A6761D", 
                    "#666666", 
                    "#6a3d9a", 
                    "#c51b7d"]);'

# Make the plot
forceNetwork(Links = d3_links, Nodes = d3_nodes, Nodesize = "n",
             Value = "daily_trips", 
             linkWidth = JS("function(d){return Math.log(d.value)-5;}"),
             Source = 'source', Target = 'target', NodeID = 'name', charge = -75,
             radiusCalculation = JS("d.nodesize+5"),
             Group = 'zone', colourScale = JS(ColourScale),
             zoom = T, opacity = .9, legend = F, width = 800)
```

`r figure <- figure + 1`

## Alternative zoning approaches

Building on the statements of @montella_multimodal_2014 that zones are created based on station importance, or administrative arguments [@jansson_is_2012], and seeking to avoid arbitrary decisions about the creation of fare zones [@_this_2016], the use of graph-based approaches extends existing analytical approaches to understanding transport systems [@yeung_physics_2013].

### Importance-based zones

#### Station importance

The first approach was to to determine the importance of a station and use this information to group stations in to zones. Adopting a different approach that used by @shimamoto_evaluating_2008, common graph-theoretical measures of node importance were calculated for each station:

* __Degree__ [@opsahl_node_2010] measures the number of edges incident on a node. In this context it was thought of as, for each station, how many direct links it has to other stations.
* __Betweenness__ [@freeman_centrality_1979; @brandes_faster_2001] measures, for each node, how many other nodes are connected on their shortest path through that node. In this context it was thought of as how likely it is that travellers will visit or pass through a station on other journeys, or how much "connectivity" it provides to the network.
* __Closeness__ [@freeman_centrality_1979; @newman_scientific_2001] measures how many steps exist between each node and all other nodes in the network. In this context, it can be thought of as how central the station is in the overall network (i.e. does it lie at the end of a long branch, or close to the middle of London).
* __Eigenvector centrality__ [@bonacich_power_1987] measures the importance of a node based on its own _and_ its neighbours importance. In this context it was thought of as a combination of how busy/connected a station is, and how busy/connected its adjacent stations are.

Each of these measures in some way captured existing theory for the creation of fare zones.

```{r centralities}
# Degree centrality
deg_cent <- degree(tfl_graph, loops = TRUE)
V(tfl_graph)$deg <- deg_cent

# Betweenness centrality
bet_cent <- betweenness(tfl_graph, directed = T)
V(tfl_graph)$bet <- bet_cent

# Closeness centrality
clo_cent <- closeness(tfl_graph)
V(tfl_graph)$close <- clo_cent

# Eigenvector centrality
eig_cent <- eigen_centrality(tfl_graph, directed = T, scale = T)
V(tfl_graph)$eig <- eig_cent$vector
```

#### Define zones

In order to define fare zones based on station importance, it was necessary to group stations together. To make these zones comparable to their real-life equivalents a simple decile rank was applied to each measure of station importance to form ten zones. The real life zones were not equally distributed (i.e. there were more zone 1 stations than zone 10). This was not the case for the decile-ranked zones, a weakness of this approach.

```{r centrality_stats}
# Convert graph object to data frame of centrality stats
station_centrality_stats <- data_frame(station = names(V(tfl_graph)),
                            deg = V(tfl_graph)$deg,
                            eig = V(tfl_graph)$eig,
                            bet = V(tfl_graph)$bet,
                            clo = V(tfl_graph)$close)

# Use decile-ranks to form 10 fare zones
station_importance_zones <- station_centrality_stats %>% 
    gather(method, value, -station) %>% 
    group_by(method) %>% 
    mutate(zone = ntile(desc(value), 10)) %>% 
    select(-value) %>% 
    spread(method, zone)
```

### Graph-community zones

#### Discovering communities

The second approach to creating fare zones, was to directly use the structure of graph to find _communities_ of related stations. There are a range of community detection algorithms, each with varying performance characteristics [@lancichinetti_community_2009]. 

Three alternative algorithms were used in this analysis:

* __Walktrap__ [@pons_computing_2005]. This algorithm detects communities by performing random walks on the graph. The algorithm records the nodes visited in each walk and uses the results to partition the graph in to communities of nodes that frequently occurred in the same walk. In this context, such communities can be imagined to be groups of stations located in close proximity, perhaps on the same tube line.
* __Edge betweenness__ [@newman_finding_2004]. This algorithm detects communities based on the _betweeness_ metric of the edges in the graph (similar to node betweenness mentioned above). The algorithm iteratively removes the edge with the largest betweenness value, dividing the graph into groups and ultimately into individual nodes. Recording this process, it uses the results to construct communities. In this context, such communities are defined by stations which serve as "hubs" connecting outlying branches of the graph to the more highly connected centre.
* __Spinglass__ [@reichardt_statistical_2006]. This algorithm uses a more abstract approach to community detection. Using ideas from statistical physics/mechanics it identifies groups of nodes which are in some way "similar". In this context "similarity" was defined as a combination of a station's connections to other stations and how frequently it was travelled to or through in the journey data. 

For each algorithm, ten station communities were created in order to draw comparisons with the existing ten fare zones. Both the _walktrap_ and _edge betweenness_ algorithms are hierarchical (i.e. some communities may be super/subsets of one another), meaning that is possible by combining smaller communities to form ten overall communities. The _spinglass_ algorithm has a parameter which was used to pre-specify the number of communities.

```{r graph_communities}
# Hierarchical methods - use cut_at to chop up
# Use distances to help create clusters
clust_w <- cluster_walktrap(tfl_graph, steps = 10, weights = E(tfl_graph)$dist)
clust_eb <- cluster_edge_betweenness(tfl_graph, directed = T, weights = E(tfl_graph)$dist)
# n.b. spins sets number of clusters
clust_sg <- cluster_spinglass(tfl_graph, spins = 10, weights = E(tfl_graph)$dist)

# Create dataframe of results
station_cluster_stats <- data_frame(station = names(V(tfl_graph)),
                                   eb = cut_at(clust_eb, no = 10),
                                   walk = cut_at(clust_w, no = 10),
                                   sg = membership(clust_sg))
```

## Comparisons with reality

### Basic similarity measures

Salton's cosine similarity [-@Salton:1986:IMI:576628] measure was used to compare the differences in importance-based and existing fare zones (as zone 1 was the most - and zone 10 the least - important in both cases). 

Given that zone labels for the the community-based zones were created in an arbitrary manner (i.e. the zones are purely categorical, not ordinal), this was not done for the community-based approach.

```{r cosine}
cosines <- station_importance_zones %>% 
         left_join(station_details %>% select(name_cln, zone_cln),
                    by = c("station" = "name_cln")) %>% 
         rename(Betweenness = bet,
                Closeness = clo,
                Degree = deg,
                Eigenvector = eig,
                Zone = zone_cln) %>% 
         select(-station) %>% 
         as.matrix() %>% 
         lsa::cosine() %>% 
         round(3)
```

### Financial approach

Using the journeys data, it was possible to calculate the total zone-zone fares charged to Oyster PAYG customers on each day of the week.

```{r journey_summary}
# Group and total
journey_summaries <- journeys %>% 
    group_by(downo,
             daytype,
             start_zone,
             end_zone) %>% 
    summarise(journeys = n(),
              total_rev = sum(dfare, na.rm = TRUE) / 100) %>% 
    ungroup() %>% 
    mutate(journeys_scaled = 20 * journeys,
           total_rev_scaled = 20 * total_rev,
           rough_cpj = total_rev_scaled / journeys_scaled)
```

Using a similar approach, it was also possible to estimate the cost of a single trip from any one zone to any other single zone.

```{r zone_costs}
# Get rough zone-zone costs
zone_costs <- journeys %>% 
    group_by(start_zone,
             end_zone) %>% 
    summarise(journeys = n(),
              total_rev = sum(dfare, na.rm = TRUE) / 100) %>% 
    ungroup() %>% 
    mutate(journeys_scaled = 20 * journeys,
           total_rev_scaled = 20 * total_rev,
           cpj = total_rev_scaled / journeys_scaled) %>% 
    select(start_zone, end_zone, cpj)
```

These zone-zone costs were then used (along with the sample journeys data) to estimate the total daily fare received under each alternative zoning approach. This was done by: 

* assuming the cost of each zone-zone ticket (based on the sample journey data) remained constant under each model; 
* iteratively changing the zone of each station based on the alternative approach being considered; and
* calculating the total fares charged per day based on the fixed zone-zone costs and alternative zoning approach being considered.

```{r fare_estimations}
# Create function to ease iteration over approaches
calculate_daily_fares <- function(how) {
    # the `how` argument will be one of the methods
    # either "current", "bet", "deg", "eig", or "clo"

    # Return "current" fares if requested
if ( how == "current" ) {
    ans <- journey_summaries %>%
            group_by(downo, daytype) %>% 
            summarise(total_fare_rev = sum(total_rev),
                      total_fare_rev_scale = sum(total_rev_scaled)) %>% 
            mutate(how = "current") %>% 
            ungroup()
} else {
    # Calculate temporary zones for the desired metric
    temp_zones <- station_centrality_stats %>% 
        gather(measure, value, -station) %>% 
        group_by(measure) %>% 
        mutate(zone = ntile(desc(value), 10)) %>% 
        filter(measure == how) %>% 
        ungroup() %>% 
        select(station, zone)

    # Get journies data
    temp_journeys <- journeys %>% 
        select(start_cln, end_cln, ffare, dfare, downo, daytype)

    # Join on new zones based on importance metric
    temp_journeys_to_zones <- temp_journeys %>% 
        left_join(temp_zones, by = c("start_cln" = "station")) %>% 
        rename(start_zone = zone) %>% 
        left_join(temp_zones, by = c("end_cln" = "station")) %>% 
        rename(end_zone = zone)     
        
    # Join on pricing info from current zone-zone costs
    temp_journeys_to_zones_to_prices <- temp_journeys_to_zones %>% 
        left_join(zone_costs, by = c("start_zone", "end_zone"))
    
    # Create daily summary
    ans <- temp_journeys_to_zones_to_prices %>% 
        group_by(downo, daytype) %>% 
        summarise(total_fare_rev = sum(cpj)) %>% 
        mutate(total_fare_rev_scale = 20 * total_fare_rev) %>% 
        ungroup() %>% 
        mutate(how = how)
}    

# Push back the summary as the result
return(ans)
}

# Set up list of methods
methods <- c("current", "deg", "eig", "bet", "clo")

# Loop over each method/zoning approach
fare_estimations <- lapply(methods, calculate_daily_fares) %>% bind_rows()
```

The differences in estimated fares were also compared statistically using Tukey's [-@tukey_comparing_1949] "Honest Significant Difference" method.

```{r hsd}
fit <- aov(total_fare_rev_scale ~ how, data = fare_estimations)
tukey <- TukeyHSD(fit, conf.level = 0.95)
tukey <- broom::tidy(tukey) %>% 
            select(-term, -conf.low, -conf.high) %>% 
            mutate(comparison = gsub("clo", "Closeness", comparison),
                   comparison = gsub("bet", "Betweenness", comparison),
                   comparison = gsub("eig", "Eigenvector", comparison),
                   comparison = gsub("deg", "Degree", comparison),
                   comparison = gsub("current", "Existing Zones", comparison),
                   estimate = round(estimate, 0),
                   adj.p.value = round(adj.p.value, 3))
```

Again, as the zone labels for the community-based zones were entirely categorical (unlike the ordinal labels for the existing and importance-based zones) this approach was taken only for the importance based zones. The results of this analysis are presented in the results section.

### Graph theoretical approach

Metrics suggested by @yang_defining_2015 were used to measure the "goodness" of the detected communities for the community-based approach. The same metrics were calculated for the existing fare zones and the results compared.

Yang and Leskovec state that "good" communities are small, with many links between nodes _within_ the community and a small number of links connected to other communities.

Amongst other metrics, they define the _density_ and _separability_ of a community:

\begin{equation}
Density = \dfrac{m_S}{n_s(n_s-1)/2}
\end{equation}

\begin{equation}
Separability = \dfrac{m_S}{c_S}
\end{equation}

Where $n_S$ is the number of nodes in the community, $m_S$ the number of edges entirely within the community and $c_S$ the number of edges link the community to another community.

These metrics were calculated for each of the communities detected, as well as for the existing fare zones.

```{r community_metrics}
get_cluster_stats <- function(how, data) {
# Function argument 'how' defines which community detection method (inc. current zone)
# should be used 
    
# Take created "zones" (clusters) and join in actual zones, gather into long table
    tmp_zones <- data %>% 
        left_join(station_details %>% select(name_cln, zone_cln),
                  by = c("station" = "name_cln")) %>% 
        gather(method, value, -station) %>% 
        filter(method == how) %>% 
        select(station, value) %>% 
        rename(zone = value)
    
    # Get stations per zone (n_s)
    stations_per_zone <- tmp_zones %>% count(zone)
    
    # Join in "zones" to links (edges)
    tmp_links <- links %>% 
        left_join(tmp_zones, by = c("station1" = "station")) %>% 
        rename(zone1 = zone) %>% 
        left_join(tmp_zones, by = c("station2" = "station")) %>% 
        rename(zone2 = zone)
    
    # Add zone-zone comparisons to get m_s and c_s
    tmp_links <- tmp_links %>% 
        mutate(ms = ifelse(zone1 == zone2, 1, 0),
               cs = ifelse(zone1 != zone2, 1, 0))
    
    # Aggregate across zone
    zone_summary <- tmp_links %>% 
        group_by(zone1) %>% 
        summarise(ms = sum(ms),
                  cs = sum(cs)) %>% 
        left_join(stations_per_zone, by = c("zone1" = "zone"))
    
    # Perform summaries
    zone_summary <- zone_summary %>% 
        mutate(Separability = ms / cs,
               Density = ms / (n*(n - 1) / 2),
               method = how)
    
    return(zone_summary)
}

methods <- c("zone_cln", "eb", "walk", "sg")
community_metrics <- lapply(methods, get_cluster_stats, 
                            data = station_cluster_stats) %>% bind_rows()
```

The statistical differences in these metrics were calculated for each of the community detection algorithms, again using Tukey's Honest Significant Differences [@tukey_comparing_1949].

```{r tukey_dens_sep}
fit_sep <- aov(Separability ~ method, community_metrics)
tukey_sep <- TukeyHSD(fit_sep, conf.level = .95) %>% 
             broom::tidy() %>% 
             select(-term, -conf.low, -conf.high) %>% 
             mutate(comparison = gsub("eb", "Edge betweenness", comparison),
                    comparison = gsub("walk", "Walktrap", comparison),
                    comparison = gsub("sg", "Spinglass", comparison),
                    comparison = gsub("zone_cln", "Existing Zones", comparison),
                    estimate = round(estimate, 3),
                    adj.p.value = round(adj.p.value, 4))

fit_den <- aov(Density ~ method, community_metrics)
tukey_den <- TukeyHSD(fit_den, conf.level = .95) %>% 
             broom::tidy() %>% 
             select(-term, -conf.low, -conf.high) %>% 
             mutate(comparison = gsub("eb", "Edge betweenness", comparison),
                    comparison = gsub("walk", "Walktrap", comparison),
                    comparison = gsub("sg", "Spinglass", comparison),
                    comparison = gsub("zone_cln", "Existing Zones", comparison),
                    estimate = round(estimate, 3),
                    adj.p.value = round(adj.p.value, 4))

```

### Zone Rankings

Each of the zones created under the two approaches were qualitatively compared with the current fare zones, using the criteria set out by @daskin_quadratic_1988. Given the ease with which these zoning approaches can be implemented, Daskin's criteria of 'feasibility of implementation' was not considered. The following scale was used to perform this ranking:

* +2 - significantly better than existing zones;
* +1 - marginally better;
* 0 - no change;
* -1 - marginally worse than existing zones; and
* -2 - significantly worse than existing zones

```{r zone_rankings}
# Set up daskin issues

daskin <- c("Sensitivity to issues",
            "Theoretical basis",
            "Ease of understanding",
            "Ease of evaluation")

# Set up importance-zone rankings
importance_daskin_rankings <- tibble(Method = c("Betweenness", "Closeness", "Degree", "Eigenvector"),
                                     `Sensitivity to issues` = c(1, 0, -2, 2),
                                     `Theoretical basis` = c(0, 2, -1, 0),
                                     `Ease of understanding` = c(-1, -1, 0, -2),
                                     `Ease of evaluation` = c(-2, -1, -2, 0),
                                     Comment = str_wrap(c("Sensitive to pressures at key stations, but zones too geographically spread",
                                                 "Captures distance-based pricing, but may be hard to comprehend",
                                                 "Too inensitive to many transport and pricing issues",
                                                 "Captures both distance and importance issues, but too complex to understand")))

# Set up graph community-zone rankings
community_daskin_rankings <- tibble(Method = c("Edge Betweenness", "Spinglass", "Walktrap"),
                                     `Sensitivity to issues` = c(1, 0, -2),
                                     `Theoretical basis` = c(0, 1, -2),
                                     `Ease of understanding` = c(1, 1, -1),
                                     `Ease of evaluation` = c(0, 0, -2),
                                     Comment = str_wrap(c("Captures notion of 'hubs' defining central zone, with radiating branches",
                                                 "Shows promise if other measures of similarity (e.g. economic) could be included",
                                                 "Performs poorly on all measures.")))
```

# Results

## Importance-based zones

Station importance was found to vary substantially based on the metric used to calculate it. Figure `r figure` shows the distribution of (normalised) station importance for each line in the TfL underground for each of the importance metrics used. 

For example, eigenvector centrality measures were clearly dominated by a few stand-out stations, with only small variations otherwise. Some lines did "better" on some measures than others, e.g. the Docklands Light Railway generally had a high closeness score, but a very low betweenness. The impact of such differences is explored below.

#### Figure `r figure`: Normalised station importance metrics

```{r importance_plot, fig.align='centre', out.width = 800, out.height=1000}
links %>% 
    select(line, station1, station2) %>% 
    gather(key, station, -line) %>% 
    select(-key) %>% 
    distinct() %>% 
    left_join(station_lk) %>% 
    left_join(station_centrality_stats) %>% 
    select(name, station, deg, eig, bet, clo) %>% 
    rename(Betweenness = bet,
           `Eigenvector Centrality` = eig,
           Closeness = clo,
           `Degree Centrality` = deg) %>% 
    gather(method, value, -name, -station) %>% 
    group_by(method) %>% 
    mutate(value_norm = (value - mean(value)) / sd(value),
           name = gsub(" Line", "", name),
           name = gsub("East London", "Overground", name)) %>% 
    ggplot(aes(y = name, x = value_norm, colour = name)) +
    geom_quasirandom(alpha = .75) +
    scale_colour_manual(values = c(bakerloo, central, circle, district, dlr, hc,
                                 jubilee, metropolitan, northern, overground,
                                 picadilly, victoria, wc)) +
    facet_wrap(~method) +
    xlab("") +
    ylab("") +
    guides(fill = FALSE,
           colour = FALSE) +
   theme_minimal()
```

`r figure <- figure + 1`

### Basic similarity

The importance-based zones did not appear to be geographically distinct from one another ([appendix three](#three)) in the same way as the existing fare zones. However, when measured with Salton's [-@Salton:1986:IMI:576628] cosine measure, the zones defined using station importance metrics generally showed a high similarity with the existing fare zones (table `r table`). This was slightly surprising given the forced, equal distribution of the number of stations in each importance-based zone (not the case for the existing zones).

```{r show_cosine}
cosines %>% kable(caption = paste0("Table ", table, ": Cosine similarity measures between zones defined using importance-based metrics and the existing TfL payment zones."))
```

`r table <- table + 1`

Eigenvector centrality-based zone produced the most similar results to the existing zones, and closeness-based zones also showed a high similarity with the existing zones. This was intuitive: the current fare zones are defined in some way based on a station's proximity to central London (similar to closeness) and it's importance in daily London life (similar to Eigenvector centrality). It therefore made sense that these two measures showed the most similarity with the existing model.

### Financial differences

Using the approach outlined previously, daily fares were estimated using each of the three importance-based zoning approaches and compared with the daily fares using the current fare-zoning method. The results of this comparison are presented in figure `r figure`.

#### Figure `r figure`: Estimated daily Tfl Underground Oyster-PAYG-only fares under each fare-zoning approach

```{r daily_fares}
fare_estimations %>% 
    mutate(how = gsub("current", "Current", how),
           how = gsub("bet", "Betweenness", how),
           how = gsub("clo", "Closeness", how),
           how = gsub("deg", "Degree", how),
           how = gsub("eig", "Eigenvector", how)) %>% 
    ggplot(aes(x = daytype, y = total_fare_rev_scale)) +
    geom_bar(aes(fill = how), stat = "identity", position = "dodge",
             colour = "white") +
    scale_x_discrete(limits = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")) +
    scale_y_continuous(labels = scales::dollar_format(prefix = "£")) + 
    scale_fill_brewer(palette = "Dark2") +
    guides(fill = guide_legend(title = "", nrow = 2)) +
    xlab("") + 
    ylab("") +
    theme_jim
```

`r figure <- figure + 1`

The alternative approaches consistently out-performed the current zoning approach when considering expected daily, Oyster-PAYG fares. 

Zones created through closeness most closely matched the figures for the current zones, which was to be expected given the similarities in how they are defined. The increase in fares based on closeness zones relative to the current zone model was probably due to the equal distribution of stations to zones enforced by the decile-rank grouping approach creating a greater number of (more expensive) intra-zone journeys. 

Betweenness and degree centrality-based zones were found to perform "best", i.e. generated the highest fares. This is due to the fact that many of the stations had large differences in zone when moving from the importance-based to the current fare zone models ([appendix four](#four)). For example, Regent's Park station is currently in fare zone 1 (due to it's proximity to central London). However, as it is not a well-visited station on one of the less-busy lines (the Bakerloo), it is placed in to zone 10 by betweenness-based zoning. 

Eigenvector centrality-based zoning also out-performed the current fare zones which, when coupled with the founding that it produced results most similar to the existing zones, suggests it could be the most promising of the four approaches.

However, none of the differences in estimated fare were found to be statistically significant as measured by Tukey's Honest Significant Differences [-@tukey_comparing_1949] (table `r table`).

```{r show_tukey}
tukey %>% kable(col.names = c("Comparison", "Fare difference (£)", "p-Value"),
                caption = paste0("Table ", table, ": Tukey HSDs for financial differences in zoning approach"))
```

`r table <- table + 1`

### Daskin's Rankings

The rankings against the criteria set out by @daskin_quadratic_1988 are listed in table `r table`.

```{r show_importance_daskin_rankings}
kable(importance_daskin_rankings, 
      caption = paste0("Table ", table, ": Rankings of importance-based zones against the existing fare zones using Daskin's proposed criteria."))
```

`r table <- table + 1`

Whilst eigenvector-based methods performed the best in the other comparisons, they are likely too complicated for customers to understand, a significant negative mark against them in Daskin's criteria.

## Graph-community zones

### Geographical distribution

The graph communities discovered by the three algorithms employed generally showed distinct geographical separation from one another (figure `r figure`) (a factor found to be useful by @jansson_is_2012).

#### Figure `r figure`: The geographic distribution of existing and graph-community based zones. 

_The figure is interactive, select a community detection algorithm and toggle area names on/off with the buttons provided._

```{r show_graph_comms}
# Create plotting data
clusters <- station_cluster_stats %>% 
    left_join(station_details %>% 
                  select(name_cln, 
                         latitude, 
                         longitude,
                         zone_cln,
                         name),
              by = c("station" = "name_cln")) %>% 
    select(name, latitude, longitude, eb, walk, sg, zone_cln) %>% 
    gather(method, value, -name, -latitude, -longitude) %>% 
    rename(lat = latitude,
           lon = longitude) %>% 
    mutate(value = as.factor(value),
           popup = paste(name, "- Zone/Cluster", value))

# Set up colours
pal <- colorFactor(c(RColorBrewer::brewer.pal(8, "Dark2"), "#6a3d9a", "#c51b7d"), 
                   domain = levels(clusters$value))

# Make the map
leaflet(width = 800) %>% 
    # Add map tiles
    addProviderTiles("CartoDB.PositronNoLabels", group = "No area labels") %>% 
    addProviderTiles("CartoDB.Positron", group = "Show/hide area labels") %>% 
    # Add zones
    addCircles(data = clusters %>% filter(method == "zone_cln"), radius = 250, 
               stroke = F, fillColor = ~pal(value), fillOpacity = 1, 
               group = "Zones (default)", popup = ~popup) %>% 
    # Add edge-bet clusters
    addCircles(data = clusters %>% filter(method == "eb"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Edge Betweenness", popup = ~popup) %>%
    # Add walktrap clusters
    addCircles(data = clusters %>% filter(method == "walk"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Walktrap", popup = ~popup) %>%
    # Add spinglass clusters
    addCircles(data = clusters %>% filter(method == "sg"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Spinglass", popup = ~popup) %>%
    # Add a zone legend
    addLegend("bottomright", pal = pal, values = as.factor(1:10), opacity = 1, 
              title = "Zone/Cluster") %>% 
    # Add layers control box
    addLayersControl(baseGroups = c("Zones (default)", "Edge Betweenness", 
                                    "Walktrap", "Spinglass"),
                     overlayGroups = c("Show/hide area labels"),
                     options = layersControlOptions(collapsed = F))
```

`r figure <- figure + 1`

The geographical distribution of communities defined using edge betweenness show the most similarity with the existing zones. There is a clear central zone, surrounded by radial/branch zones which could be broken down by geographic region, perhaps using a similar approach to that used for London postcodes. 

The walktrap algorithm did not performed well in this setting: there is little intuitive distinction between communities when viewed geographically. 

Whilst not as geographically distinct as the edge-betweenness communities, the spinglass algorithm generated generated clear communities which could be used to create fare zones relatively simply, especially with the use of other data, such as socio-economic factors. 

### Theoretical properties

The density and separability [@yang_defining_2015] metrics for each approach (along with the existing zones) are shown in figure `r figure`.

Discounting the walktrap algorithm (based on its poor performance when viewing the results geographically), the best approach appears to be edge betweenness. It generates dense, separable communities. Spinglass also performed well, although the communities it detected were less dense and less separable than those for edge-betweenness.

Both edge betweenness and spinglass generated communities were more dense and more separable than the existing fare zones. The differences in density were not statistically significant when measured using Tukey's Honest Significant Differences (tables `r table` and `r table+1`), however edge betweenness produced significantly more separable clusters than the current zones. 

This results was not, however, particularly surprising as the existing zones were generated in such a different manner than the graph communities. 

#### Figure `r figure`: Community density and separability for communities detected with the edge-betweeness, walktrap, and spinglass algorithms. Also shown are the density and separability values for the existing fare zones.

```{r dens_sep, out.width = 800}
community_metrics %>% 
    mutate(Density = ifelse(is.na(Density), 0, Density)) %>% 
    select(zone1, Separability, Density, method) %>% 
    gather(measure, value, -zone1, -method) %>% 
    mutate(method = gsub("zone_cln", "Zones", method),
           method = gsub("eb", "Edge\nbetweenness", method),
           method = gsub("sg", "Spinglass", method),
           method = gsub("walk", "Walktrap", method)) %>% 
    ggplot(aes(x = method, y = value, fill = method)) +
    geom_boxplot() +
    scale_fill_brewer(palette = "Dark2") +
    facet_grid(measure~., scales = "free") +
    guides(fill = FALSE) + 
    xlab("") +
    ylab("") +
    theme_minimal()
```

`r figure <- figure + 1`

```{r show_tukey_dens}
tukey_den %>% kable(col.names = c("Comparison", "Difference", "p-Value"),
                caption = paste0("Table", table, ": Tukey HSDs for differences in community densities"))
```

`r table <- table + 1`

```{r show_tukey_sep}
tukey_sep %>% kable(col.names = c("Comparison", "Difference", "p-Value"),
                caption = paste0("Table", table, ": Tukey HSDs for differences in community separability"))
```

`r table <- table + 1`

### Daskin's Rankings

The rankings against the criteria set out by @daskin_quadratic_1988 are listed in table `r table`.

```{r show_community_daskin_rankings}
kable(community_daskin_rankings, caption = paste0("Table ", table, ": Rankings of graph community-based zones against the existing fare zones using Daskin's proposed criteria."))
```

`r table <- table + 1`

# Conclusions & Recommendations

#### Conclusions

Building on existing literature, this analysis has shown that novel approaches to the creation of fare zones could be used to help TfL change its approach to pricing on the London underground. 

Node importance measure-based zoning approaches showed good overlap with existing the zones, and could result in potentially higher fare revenues under the existing charge scheme. Community-detection algorithms were able to identify groups of stations with good graph-theoretical properties, and which showed a similar geographic distinction as the existing zones, highlighting the potential to make data-driven decisions on zoning. 

In both approaches, some methods proved better than others. For station importance-based zones, closeness and eigenvector centrality measures generated the best results (both in terms of similarity with existing zones, and using estimations of resulting fare revenue). The edge-betweenness and spinglass algorithms produced the best outcomes for graph community based zones when considering the geographical intuition of the resulting zones, and their evaluation against existing criteria.

Whilst promising, however, the results are not conclusive. Decisions about transport prices are greatly affected by social, economic, and political pressures. Therefore these results present only a small first-step in investigating alternative courses of action for TfL if it is to successfully manage the loss of its subsidy. 

#### Recommendation

Having concluded that methods such as these could offer a potential new avenue for designing fare models in the London Underground system, it is recommended that further analysis be undertaken. Collecting and using socio-economic, political, and administrative data could help to enhance the methods proposed here. 

Social, as well as technical and financial, considerations are critical and any solution proposed to help TfL manage the loss of its subsidy should be evaluated with this in mind. 

It is therefore recommended that TfL continue to provide their services as they currently plan to, whilst rigorously assessing alternative pricing and cost-reduction strategies that may help them to alleviate the effects of losing their subsidy.

***

# Appendices

## One

#### _Raw Data_ [↩ Back to text](#data-used)

__Graph structure__:

```{r raw_graph_struc, echo = F}
links %>% 
    select(line, station1, station2) %>% 
    sample_n(samples) %>% 
    kable(col.names = c("Line", "Station 1", "Station 2"))
```

__Station details__:

```{r raw_details, echo = F}
station_details %>% 
    select(name, latitude, longitude, zone_cln, total_lines) %>% 
    sample_n(samples) %>% 
    kable(col.names = c("Station", "Lat.", "Lon.", "Zone", "Total Lines"))
```


__Distances__:

```{r raw_distances, echo = F}
distances %>% 
    sample_n(samples) %>% 
    kable(col.names = c("Line", "Station 1", "Station 2", "Distance (km)"))
```

__Journeys__:

```{r raw_journeys, echo = F}
journeys %>% 
    sample_n(samples) %>% 
    select(daytype, startstn, endstation, enttimehhmm, extimehhmm, dfare) %>% 
    kable(col.names = c("Day", "From", "To", "Start time", "End time", "Fare (p)"))
```

## Two 

#### _Software packages used_ [↩ Back to text](#data-used)

The `R` language [@cite_r] was used to perform the data manipulation, modelling and visualisations for this report. The following `R` packages were also used:

* `igraph` [@csardi_igraph:_2015];
* `readr` [@wickham_readr:_2016];
* `readxl` [@wickham_readxl:_2016];
* `dplyr` [@wickham_dplyr:_2016];
* `tidyr` [@wickham_tidyr:_2016];
* `stringr` [@wickham_stringr:_2015];
* `purrr` [@wickham_purrr:_2016];
* `lsa` [@wild_lsa:_2015];
* `broom` [@robinson_broom:_2016];
* `ggplot2` [@wickham_ggplot2:_2016];
* `ggbeeswarm` [@clarke_ggbeeswarm:_2016];
* `rmarkdown` [@allaire_rmarkdown:_2016];
* `networkD3` [@gandrud_networkd3:_2016];
* `leaflet` [@cheng_leaflet:_2016]; and
* `DT` [@xie_dt:_2016].

<br>

## Three

#### _The geographic distribution of existing and importance-based fare zones. The figure is interactive, select a fare-zoning approach and toggle area names on/off with the buttons provided._ [↩ Back to text](#basic-similarity)

```{r show_imp_clust_dist, echo = F}
# Create plotting data
clusters <- station_importance_zones %>% 
    left_join(station_details %>% 
                  select(name_cln, 
                         latitude, 
                         longitude,
                         zone_cln,
                         name),
              by = c("station" = "name_cln")) %>% 
    select(name, latitude, longitude, bet, clo, deg, eig, zone_cln) %>% 
    gather(method, value, -name, -latitude, -longitude) %>% 
    rename(lat = latitude,
           lon = longitude) %>% 
    mutate(value = as.factor(value),
           popup = paste(name, "- Zone/Cluster", value))

# Set up colour palette
pal <- colorFactor(c(RColorBrewer::brewer.pal(8, "Dark2"), "#6a3d9a", "#c51b7d"), 
                   domain = levels(clusters$value))

# Make the map
leaflet(width = 800) %>% 
    addProviderTiles("CartoDB.PositronNoLabels", group = "No area labels") %>% 
    addProviderTiles("CartoDB.Positron", group = "Show/hide area labels") %>% 
    addCircles(data = clusters %>% filter(method == "zone_cln"), radius = 250, 
               stroke = F, fillColor = ~pal(value), fillOpacity = 1, 
               group = "Zones (default)", popup = ~popup) %>% 
    addCircles(data = clusters %>% filter(method == "bet"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Betweenness", popup = ~popup) %>%
    addCircles(data = clusters %>% filter(method == "eig"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Eigenvector centrality", popup = ~popup) %>%
    addCircles(data = clusters %>% filter(method == "clo"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Closeness", popup = ~popup) %>%
        addCircles(data = clusters %>% filter(method == "deg"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Degree centrality", popup = ~popup) %>%
    # Layers control
    addLayersControl(
        baseGroups = c("Zones (default)", "Betweenness", "Eigenvector centrality",
                       "Closeness", "Degree centrality"),
        overlayGroups = c("Show/hide area labels"),
        options = layersControlOptions(collapsed = F)
    ) %>% 
    addLegend("bottomright", pal = pal, values = as.factor(1:10), opacity = 1, 
              title = "Zone/Cluster")
```

<br>

## Four

#### _The list of stations and their associated, importance-based fare zones._ [↩ Back to text](#financial-differences)

```{r show_imp_fare_zones, echo = F}
# Create plotting data
station_importance_zones %>% 
    left_join(station_details %>% 
                  select(name_cln, 
                         zone_cln,
                         name),
              by = c("station" = "name_cln")) %>% 
    select(name, bet, clo, deg, eig, zone_cln) %>% 
    datatable(colnames = c("Station", "Betweenness", "Closeness", "Degree", 
                           "Eigenvector", "Existing Zone"),
              caption = "Fare zones based on node importance measures")
    
```

# References


