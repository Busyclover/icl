---
title: "The Network Structure of London Underground and Alternative Zoning Approaches"
author: "Jim Leach"
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    keep_md: yes
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
bibliography:
- refs.bib
- extra.bib
---

```{r knitr_setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
                      include = T, eval = T, echo = T)
figure <- 1
app <- 1
```

`r figure <- 1; app <- 1`

<p style="border:1.5px; border-style:solid; border-color:#000000; padding: 1em;">This `HTML` report should be viewed using a modern web browser such as Mozilla Firefox or Google Chrome. Printing is possible but will not produce an optimal reading experience. The `R` code used to perform this analysis can be viewed in this report using the _Code_ buttons to toggle code viewing.</p>

# Details

* Name: James Leach
* CID: 01135629
* Title: The Network Structure of London Underground and Alternative Zoning Approaches
* GitHub Repository: [jim89/icl/final_report](https://github.com/Jim89/icl/tree/master/final_report)
* Word Count: TODO

# Abstract

TODO

# Introduction

## Document Structure

Brief description of document structure

## Background

TODO: Start out with summary of TfL finances, then move on to brief literature summary of papers reviewed to date.

### Transport for London

[Transport for London](https://tfl.gov.uk/) ("TfL") is a government body responsible for almost all roads and buses, many train services, as well as other forms of public transport (including taxis and boats) in the UK capital. 

TODO: some brief network stats (stations, lines, journeys, etc), introduce Oyster

Operating in London, a city of around 8 million people [@census_comparing_2015], TfL also has a huge financial responsibility. In the 2015 financial year, TfL reported total revenue of £5.289 billion, of which £2.559 billion (62%) came directly from ticket fares on the London Underground [@tfl_annual_2015]. 

### TODO some title about "the problem"

It is generally the case that fares are set lower than the costs of running services [@montella_multimodal_2014], thus setting up a requirement for some kind of subsidy. Whilst setting fares lower than costs can be part of a capacity-management strategy [@turvey_simple_1975], this is likely not the case for TfL which has seen demand increase year-on-year. Indeed, based on current figures, it is possible that demand will outstrip capacity within the next fifteen to twenty years [@_tube_2016]. Therefore TfL also receives a significant portion of its yearly income in the form of cash from central government: an estimated 23% came from grants and subsidies in 2015/16 [@tfl_how_2015].

However, as noted by Cervero [-@cervero_flat_1981], subsidisation can lead a sequence of events that, over time, actually reduce revenue. Fares kept low through subsidisation results in fare revenue (relative to costs) decreasing over time. This fact (coupled with social and political pressure to maintain low fares) means that subsidisation continues to be a requirement. 

TfL will loose its operational subsidy from 2019 onwards [@tfl_annual_2015]. Therefore it must seek actions which can dramatically cut its costs, or grow its income. With the noted pressures of demand building, and politicians struggling to maintain price freezes [@_sadiq_2016] this may prove to be a difficult prospect for TfL. 

It must seek new and inventive strategies to maintain the infrastructure that is so critical to the smooth running of the capital whilst operating in an increasingly tight financial situation.

### Pricing public transport

The pricing of public transport has long been studied in the literature. Noting that pricing required a fine balancing of managing demand and capacity, Turvey [-@turvey_simple_1975] found that distance-based pricing was preferable to flat-fares with longer trips requiring larger fares. This principal seems intuitive (and indeed is the basis for the current fare zones employed by TfL) but much work has gone into understanding exactly how fares should be set up. 

Cervero [-@cervero_flat_1981] introduced the concept of 'faireness' in how customers subsidise each other. He found that, under certain models, short-distance customers could end up subsidising those on longer journeys (something he did not regard as fair) whereas other models (such as peak vs. off-peak, coupled with a distance component) were better at eliminating this effect. Cervero also discussed how the fare-model choice was highly dependent on the policy objective of the operator. For example, 'fare comprehension' fares would be greatly different to 'deficit reduction' fares.

Daskin et. al. [-@daskin_quadratic_1988] built on this work, and provide a list of requirements for pricing schemes. They should be:

* sensitive to the many issues involved in transport fares (both from the provider's _and_ the user's perspective);
* based on appropriate economic, political, and analytical theory;
* easy for the end-user to understand;
* feasible to implement; and
* basic to test and analyse in the presence of alternatives.

Daskin et. al. developed a model to test several alternative pricing models, and found that the results were highly dependent how strongly the price elasticity of demand was related to distance, with zone-based pricing most optimal when elasticity was not strongly related to trip distance. 

Sharaby and Shiftan [-@sharaby_impact_2012] have investigated how fare integration (for example, TfL's Oyster scheme) affected travel behaviour. Fare integration is defined as the ability to "transfer" within the transport system from one form of transport to another (e.g. from bus to train), or between multiple routes in one form of transport (e.g. changing bus routes), without incurring a cost penalty. They found that the use of fare integration, along with a zone-based pricing policy led to increased ticket purchases along with increased customer satisfaction. However, in their study, integrated pricing was also coupled with an overall reduction in fares and therefore potentially suggests that less integration could be beneficial for TfL. 
 
### Zone-based pricing

Zone-based pricing is the idea that a transport area (here, London) is divided in to zones based on some measure, either analytical or political/administrative in nature. Prices are then based on the number of zones crossed in a journey, generally the more zones crossed, the more expensive the journey. 

Montella and D'Acierno [-@montella_multimodal_2014] provide a brief overview how fare zones are created in transport networks and provide a four step process to finalising the fare zones:

> 1. Define initial zones
2. Optimise of fares across these zones
3. Evaluate the results
4. Use the results to find the best zone/fare combination

They note three main types of zones: concentric rings; circular rings/sectors (similar to concentric rings, but with radial "cuts" in the rings to define new zones); and alveolar zones (zones defined across the area, rather than radiating from a centre point). Transport for London uses a concentric rings appraoch, first introduced in the 1980's [@_london_2016]. As noted by Montella and D'Acierno, concentric ring zones are typically used in captial cities where there is a single, central district of increased importance surrounded by areas which decrease in importance the further they are from the centre. Fares are then charged based on how important the zones entered/exited are (i.e. trips to the centre are more expensive). 

Jansson and Angell [@-jansson_is_2012], have shown that highly complex arrangements of fare zones are not practical. They considered the case in Oslo, which previously had eighty-eight fare zones before simplifying down to just ten. They derived forumlae for the optimal price based on the operator's cost(s), the (social and financial) cost(s) associated with extra passengers using the service, and the journey distance. They foundd that the optimal price can be approximated with a zone-based system with travel to more important zones (in their context: '[zones] closer to the city centre') incurring higher fares. 

Jansson and Angell [@-jansson_is_2012] also described several alternative zoning approaches which they investigated. Their results were as follows:

* __Ring zones__ resulted in only small increases in price for travelling between zones, but generated a too high a number of zones in the city centre to compensate for this.
* __Dense, suburban, and rural zones__ suffered from a similar problem to ring zones: in order to keep "jumps" in fare for crossing zones minimal, too many zones had to be introduced in the city centre.
* __Large central, small surrounding zones__ was found to be successful, with simplified transit in the both the city centre (where the majority of journeys occurred) and in the suburbs. However, there were then significant jumps in fare for travelling between zones.
* __Three main zones__ spanning the entire area, whilst _very_ simple, was found to reduce fare revenue so much that extensive subsidisation was required.
* __Single zone, with distance-based pricing__ was simple to implement and easy for customers to understand, but resulted in high prices for short trips (typically those in the city centre) which would have been unpopular.
* __Munipal zones__ based on existing political boundaries were also easy for customers to understand, but resulted in some short journeys crossing many zones (and therefore being expensive), and long journeys remaining in just one zone (and therefore being cheap), countering one of the main aims of zone-based prices.

### Analytics for transport

## Analytical Objective

### TODO describe approach

This work sought to use graph-theoretical [@_graph_2016] methods for modelling the Transport for London ("TfL") Underground network. More explicitly, the aim was to investigate the creation of fare zones using a data-driven approach, and to compare the results of such approaches with the fare zones that exist today. Such comparisons were made in both financial terms, as well as using theoretical measures proposed in earlier work [@yang_defining_2015]. 

TODO: Describe two approaches. Go in to detail and provide the context

TODO: There are a range of community detection algorithms, each with varying performance characteristics [@lancichinetti_community_2009]. Such algorithms can be used to detect and label groups of "similar" nodes in a graph (with the definition of similarity varying across the algorithms). Typically, 

Typically, However, some of these algorithms have the advantage of being 

### TODO Academic justification

TODO brief review of Yeung et. al (2013) and Shimamoto (2008) for use of graph analysis in transport problems

## Data Used

The following data were obtained and used for the purposes of the analysis conducted in this assignment:

* Transport for London Underground Station adjacency, fare zone and line information available as open-source data hosted on [Wikimedia](https://commons.wikimedia.org/wiki/London_Underground_geographic_maps/CSV#Routes) [-@wikimedia_london_2009]. Referred to as the "raw graph structure data"; 
* Transport for London Underground inter-station distances [@tfl_distances_2013], made available by Transport for London under a Freedom of Information request in [2013](https://www.whatdotheyknow.com/request/distances_between_adjacent_oyste). Referred to as the "distances data"; and
* Sample journey data provided by Transport for London [-@tfl_oyster_2009]. The data cover 5% of all Oyster card journeys made in one week in November 2009. The data are available through the Transport for London [API](https://tfl.gov.uk/info-for/open-data-users/our-feeds#on-this-page-6). Referred to as the "journey data".

# Methodology

```{r setup, echo = FALSE}
# Load packages
library(igraph)
library(readr)
library(readxl)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(purrr)
library(DT)
library(networkD3)
library(leaflet)
library(knitr)

# Set up colour objects
bakerloo <- rgb(red = 137, green = 78, blue = 36, maxColorValue = 255)
central <- rgb(red = 220, green = 36, blue = 31, maxColorValue = 255)
circle <- rgb(red = 255, green = 206, blue = 0, maxColorValue = 255)
district <- rgb(red = 0, green = 114, blue = 41, maxColorValue = 255)
hc <- rgb(red = 215, green = 153, blue = 175, maxColorValue = 255)
jubilee <- rgb(red = 134, green = 143, blue = 152, maxColorValue = 255)
metropolitan <- rgb(red = 117, green = 16, blue = 86, maxColorValue = 255)
northern <- rgb(red = 0, green = 0, blue = 0, maxColorValue = 255)
picadilly <- rgb(red = 0, green = 25, blue = 168, maxColorValue = 255)
victoria <- rgb(red = 0, green = 160, blue = 226, maxColorValue = 255)
wc <- rgb(red = 118, green = 208, blue = 189, maxColorValue = 255)
dlr <- rgb(red = 0, green = 175, blue = 173, maxColorValue = 255)
overground <- rgb(red = 232, green = 106, blue = 16, maxColorValue = 255)

tfl_colour <- c(bakerloo, central, circle, district,
            hc, jubilee, metropolitan, northern, picadilly,
            victoria, wc, dlr, overground)

# Set up ggplot2 theme object for prettier plots
theme_jim <-  theme(legend.position = "bottom",
                    axis.text.y = element_text(size = 16, colour = "black"),
                    axis.text.x = element_text(size = 16, colour = "black"),
                    legend.text = element_text(size = 16),
                    legend.title = element_text(size = 16),
                    title = element_text(size = 16),
                    strip.text = element_text(size = 16, colour = "black"),
                    strip.background = element_rect(fill = "white"),
                    panel.grid.minor.x = element_blank(),
                    panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
                    panel.grid.minor.y = element_line(colour = "lightgrey", linetype = "dotted"),
                    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
                    panel.margin.y = unit(0.1, units = "in"),
                    panel.background = element_rect(fill = "white", colour = "lightgrey"),
                    panel.border = element_rect(colour = "black", fill = NA))
```

## Graph creation

### Prepare and clean data

The raw graph structure [@wikimedia_london_2009] and the distances data [@tfl_distances_2013] were loaded and some basic processing and cleaning (e.g. standardisation of the names of stations) was performed.

```{r load_and_clean}
# Load the data ----------------------------------------------------------------
### Graph structure
# Basic linkage data from Wiki
adjacency <- read_csv("./data/geo/adjacency.csv",
                      col_types = cols(station1 = col_integer(),
                                       station2 = col_integer(),
                                       line = col_integer()))

# Line lookup values
station_lk <- read_csv("./data/geo/station_lk.csv",
                       col_types = cols(line = col_integer(),
                                        name = col_character(),
                                        colour = col_character(),
                                        stripe = col_character()))

# Station details
station_details <- read_csv("./data/geo/stations_geo.csv",
                            col_types = cols(id = col_integer(),
                                             latitude = col_double(),
                                             longitude = col_double(),
                                             name = col_character(),
                                             display_name = col_character(),
                                             zone = col_double(),
                                             total_lines = col_integer(),
                                             rail = col_integer()))

### Distances data from FOI
# DLR name lookup
dlr_abbr <- read_excel("./data/distances/formatted/FOI Request Station Abbreviations_CLN.xls")

# Distances between stations
stations_dist <- read_excel("./data/distances/formatted/Inter Station Train Times_CLN.xls")

# DLR distance matrix
dlr_dist <- read_excel("./data/distances/formatted/Distance Martix DLR 2013_CLN.xlsx")


# Clean the data ----------------------------------------------------------------
# Set up clean lower-case name, and rounded zone-number in station details
station_details <- station_details %>% mutate(name_cln = tolower(name),
                                              zone_cln = ceiling(zone))

# Set up adjacency list with names of stations, rather than ID as keys
links <- adjacency %>% left_join(station_details %>% select(id, name),
                                 by = c("station1" = "id")) %>% 
    select(-station1) %>% 
    rename(station1 = name) %>% 
    left_join(station_details %>% select(id, name),
              by = c("station2" = "id")) %>% 
    select(-station2) %>% 
    rename(station2 = name) %>% 
    mutate(station1 = tolower(station1),
           station2 = tolower(station2),
           station1 = str_trim(station1),
           station2 = str_trim(station2))

# Clean up DLR distances table
dlr_dist_long <- dlr_dist %>% gather(station, dist, -Metres) %>% 
    rename(from = Metres,
           to = station) %>% 
    left_join(dlr_abbr, by = c("from" = "abbr")) %>% 
    select(-from) %>% 
    rename(station1 = station) %>% 
    left_join(dlr_abbr, by = c("to" = "abbr")) %>% 
    select(-to) %>% 
    rename(station2 = station) %>% 
    na.omit() %>% 
    filter(dist > 0) %>% 
    mutate(dist = dist / 1000,
           line = "Docklands Light Railway") %>% 
    select(line, station1, station2, dist) %>% 
    ungroup()

# Clean up station-station distances table
station_dist_long <- stations_dist %>% 
    group_by(Line, `Station from (A)`, `Station to (B)`) %>% 
    summarise(dist = median(`Distance (Kms)`)) %>% 
    rename(line = Line,
           dist = dist,
           station1 = `Station from (A)`,
           station2 = `Station to (B)`) %>% 
    ungroup()

# Standardise station names to match links data
distances <- bind_rows(dlr_dist_long, station_dist_long) %>% 
    mutate(station1 = tolower(station1),
           station2 = tolower(station2),
           station1 = str_trim(station1),
           station2 = str_trim(station2),
           station1 = gsub("edgware", "edgeware", station1),
           station2 = gsub("edgware", "edgeware", station2),
           station1 = gsub("regents park", "regent's park", station1),
           station2 = gsub("regents park", "regent's park", station2),
           station1 = gsub("piccadilly", "picadilly", station1),
           station2 = gsub("piccadilly", "picadilly", station2),
           station1 = gsub("st james park", "st. james's park", station1),
           station2 = gsub("st james park", "st. james's park", station2),
           station1 = gsub("kings cross|kings cross st pancras", "king's cross st. pancras", station1),
           station2 = gsub("kings cross|kings cross st pancras", "king's cross st. pancras", station2),
           station1 = gsub("earls court", "earl's court", station1),
           station2 = gsub("earls court", "earl's court", station2),
           station1 = gsub("highbury & islington", "highbury", station1),
           station2 = gsub("highbury & islington", "highbury", station2),
           station1 = gsub("paddington \\(.*\\)", "paddington", station1),
           station2 = gsub("paddington \\(.*\\)", "paddington", station2),
           station1 = gsub("st johns wood", "st. john's wood", station1),
           station2 = gsub("st johns wood", "st. john's wood", station2),
           station1 = gsub("queens park", "queen's park", station1),
           station2 = gsub("queens park", "queen's park", station2),
           station1 = gsub("heathrow 123", "heathrow terminals 1, 2 & 3", station1),
           station2 = gsub("heathrow 123", "heathrow terminals 1, 2 & 3", station2),
           station1 = gsub("heathrow four", "heathrow terminal 4", station1),
           station2 = gsub("heathrow four", "heathrow terminal 4", station2),
           station1 = gsub("hammersmith \\(.*\\)", "hammersmith", station1),
           station2 = gsub("hammersmith \\(.*\\)", "hammersmith", station2),
           station1 = gsub("st pauls", "st. paul's", station1),
           station2 = gsub("st pauls", "st. paul's", station2))
```

After ensuring that (wherever possible) all station-station distances were present, these data were joined with the data of the graph structure. The graph structure _data_ was then transformed in to a graph object, retaining the distance between adjacent stations as an edge attribute.

```{r create_initial_graph}
# Create "reversed" distances to account for possible different combos of stations
distances <- bind_rows(distances, distances %>% 
                           select(line, station2, station1, dist) %>% 
                           rename(station3 = station1,
                                  station1 = station2) %>% 
                           rename(station2 = station3))

# Add distances to station linkages data
links <- links %>% 
    left_join(distances %>% select(-line), by = c("station1" = "station1",
                                "station2" = "station2"))

# Clean up missing distances - set to average
links <- links %>%
    mutate(dist = round(ifelse(is.na(dist), 
                               mean(links$dist, na.rm = T), 
                               dist),
                        2)) %>% 
    distinct()


# Create "reversed" links to account for bi-directional travel!
links <- bind_rows(links, links %>% 
                            select(line, station2, station1, dist) %>% 
                            rename(station3 = station1,
                                   station1 = station2) %>% 
                            rename(station2 = station3))

# Clean up intermediate tables)
rm(dlr_abbr, dlr_dist, dlr_dist_long, station_dist_long, adjacency, distances,
   stations_dist)

# Chop out line and distinct
graph_data <- links %>% 
    distinct() %>% 
    select(-line)

# Make the graph - retain distance as edge attribute but do not weight edges
tfl_graph <- graph_data %>% 
    graph_from_data_frame(directed = TRUE)

# Clean up
rm(graph_data)
```

The journey data [@tfl_oyster_2009] were also loaded and filtered. These data contained a sample of _all_ journeys made with an Oyster card in one week of November 2009 (including bus journeys). As such these data were filtered to include only those journeys made on the London Underground, and only those that were fully pay-as-you-go (i.e. not using a daily/weekely/monthly travelcard). 

```{r load_filter_journeys, cache = T}
# Usage data
journeys <- read_csv("./data/journeys/Nov09JnyExport.csv",
                     col_types = cols(downo = col_integer(),
                                      daytype = col_character(),
                                      SubSystem = col_character(),
                                      StartStn = col_character(),
                                      EndStation = col_character(),
                                      EntTime = col_integer(),
                                      EntTimeHHMM = col_character(),
                                      ExTime = col_integer(),
                                      EXTimeHHMM = col_character(),
                                      ZVPPT = col_character(),
                                      JNYTYP = col_character(),
                                      DailyCapping = col_character(),
                                      FFare = col_integer(),
                                      DFare = col_integer(),
                                      RouteID = col_character(),
                                      FinalProduct = col_character()))
names(journeys) <- names(journeys) %>% tolower()

# Filter to just completed tube journeys that were PAYG
journeys <- journeys %>%
     filter(subsystem == "LUL",
            startstn != "Unstarted",
            endstation != "Unfinished",
            endstation != "Not Applicable",
            finalproduct == "PAYG")
```

Further cleaning and standardisation of station names was performed to ensure that they matched those present in the graph structure data.

```{r clean_journeys, cache = TRUE}
# Clean up station names
journeys <- journeys %>% 
    # Start station names
    mutate(start_cln = startstn,
           start_cln = gsub("Earls Court", "Earl's Court", start_cln),
           start_cln = gsub("Highbury", "Highbury & Islington", start_cln),
           start_cln = gsub("St James's Park", "St. James's Park", start_cln),
           start_cln = gsub("St Pauls", "St. Paul's", start_cln),
           start_cln = gsub("Kings Cross [MT]", "King's Cross St. Pancras", start_cln),
           start_cln = gsub("Piccadilly Circus", "Picadilly Circus", start_cln),
           start_cln = gsub("Hammersmith [DM]", "Hammersmith", start_cln),
           start_cln = gsub("Bromley By Bow", "Bromley-By-Bow", start_cln),
           start_cln = gsub("Canary Wharf E2", "Canary Wharf", start_cln),
           start_cln = gsub("Edgware Road [BM]", "Edgware Road (B)", start_cln),
           start_cln = gsub("Great Portland St", "Great Portland Street", start_cln),
           start_cln = gsub("Waterloo JLE", "Waterloo", start_cln),
           start_cln = gsub("Shepherd's Bush Mkt", "Shepherd's Bush (H)", start_cln),
           start_cln = gsub("Shepherd's Bush Und", "Shepherd's Bush (C)", start_cln),
           start_cln = gsub("Harrow On The Hill", "Harrow-on-the-Hill", start_cln),
           start_cln = gsub("Harrow Wealdstone", "Harrow & Wealdston", start_cln),
           start_cln = gsub("Heathrow Term [45]", "Heathrow Terminal 4", start_cln),
           start_cln = gsub("Heathrow Terms 123", "Heathrow Terminals 1, 2 & 3", start_cln),
           start_cln = gsub("Tottenham Court Rd", "Tottenham Court Road", start_cln),
           start_cln = gsub("High Street Kens", "High Street Kensington", start_cln),
           start_cln = gsub("Regents Park", "Regent's Park", start_cln),
           start_cln = gsub("Queens Park", "Queen's Park", start_cln),
           start_cln = gsub("St Johns Wood", "St. John's Wood", start_cln),
           start_cln = gsub("Wood Lane", "White City", start_cln),
           start_cln = gsub("Totteridge", "Totteridge & Whetstone", start_cln),
           start_cln = gsub("Watford Met", "Watford", start_cln),
           start_cln = tolower(start_cln)) %>% 
    # End station names
    mutate(end_cln = endstation,
           end_cln = gsub("Earls Court", "Earl's Court", end_cln),
           end_cln = gsub("Highbury", "Highbury & Islington", end_cln),
           end_cln = gsub("St James's Park", "St. James's Park", end_cln),
           end_cln = gsub("St Pauls", "St. Paul's", end_cln),
           end_cln = gsub("Kings Cross [MT]", "King's Cross St. Pancras", end_cln),
           end_cln = gsub("Piccadilly Circus", "Picadilly Circus", end_cln),
           end_cln = gsub("Hammersmith [DM]", "Hammersmith", end_cln),
           end_cln = gsub("Bromley By Bow", "Bromley-By-Bow", end_cln),
           end_cln = gsub("Canary Wharf E2", "Canary Wharf", end_cln),
           end_cln = gsub("Edgware Road [BM]", "Edgware Road (B)", end_cln),
           end_cln = gsub("Great Portland St", "Great Portland Street", end_cln),
           end_cln = gsub("Waterloo JLE", "Waterloo", end_cln),
           end_cln = gsub("Shepherd's Bush Mkt", "Shepherd's Bush (H)", end_cln),
           end_cln = gsub("Shepherd's Bush Und", "Shepherd's Bush (C)", end_cln),
           end_cln = gsub("Harrow On The Hill", "Harrow-on-the-Hill", end_cln),
           end_cln = gsub("Harrow Wealdstone", "Harrow & Wealdston", end_cln),
           end_cln = gsub("Heathrow Term [45]", "Heathrow Terminal 4", end_cln),
           end_cln = gsub("Heathrow Terms 123", "Heathrow Terminals 1, 2 & 3", end_cln),
           end_cln = gsub("Tottenham Court Rd", "Tottenham Court Road", end_cln),
           end_cln = gsub("High Street Kens", "High Street Kensington", end_cln),
           end_cln = gsub("Regents Park", "Regent's Park", end_cln),
           end_cln = gsub("Queens Park", "Queen's Park", end_cln),
           end_cln = gsub("St Johns Wood", "St. John's Wood", end_cln),
           end_cln = gsub("Wood Lane", "White City", end_cln),
           end_cln = gsub("Totteridge", "Totteridge & Whetstone", end_cln),
           end_cln = gsub("Watford Met", "Watford", end_cln),
           end_cln = tolower(end_cln))

# Add on ID-information and zone
journeys <- journeys %>% 
    left_join(station_details %>% select(name_cln, id, zone_cln), 
              by = c("start_cln" = "name_cln")) %>% 
    rename(start_id = id,
           start_zone = zone_cln) %>% 
    left_join(station_details %>% select(name_cln, id, zone_cln),
              by = c("end_cln" = "name_cln")) %>% 
    rename(end_id = id,
           end_zone = zone_cln)
```

### Estimate journey routes

In order to model the graph structure with edges weighted by trips between adjacent stations, it was necessary to estimate routes taken through the network based on sample journey data. These routes could then be used to correctly weight edges in the graph based on the number of journeys utilising each edge (e.g. a journey from A-D might use edges A-B, B-C, and C-D and each of these edges should be weighted). 

The journey data only provided the start and end point of the overall journey, and not the route taken. Therefore, it was necessary to determine the route for each journey. This was done by applying Dijkstra's [-@dijkstra_note_1959] algorithm to the intial graph structure, weighting the edges by the distance between adjacent stations found in the distances data. Whilst this method did not neccessarily return the _exact_ route taken by the customer on each journey, it provided a useful approximation in the absence of such data.

Journeys in the journey data were de-duplicated, ensuring that a route for a particular journey was only calculated once. (This was done as many, popular, journeys occurred multiple times, e.g. Oxford Circus to Victoria). A simple function was defined to apply Dijkstra's algorithm and return the route path in an ordered list (with the first element corresponding to the starting station, the second element the second station, and so on up to the final station). 

```{r get_routes, cache = T}
# Dedupe journeys
distinct_journeys <- journeys %>% select(start_cln, end_cln) %>% distinct()

# Define a function that will calcualte the path and return it as an ordered list
get_shortest_path <- function(node1, node2, graph = tfl_graph) {
    path <- shortest_paths(graph, node1, node2, weights = E(graph)$dist)$vpath
    path <- path[[1]]
    path <- path %>% as.list() %>% names()
    return(path)
}

routes <- distinct_journeys %>% 
    mutate(path = map2(start_cln, end_cln, get_shortest_path)) %>% 
    unnest(path)

```

### Aggregate routes

The routes calculated with this approach were then used to determine the number of daily trips between all adjacent stations in the graph. These daily trips were then to be used to weight the edges of the graph. 

For example, the route for a journey from Earl's Court to Pimlico was found to go from Earl's Court through Gloucester Road, South Kensington, Sloane Square, and Victoria to Pimlico. Therefore a single journey from Earl's Court to Pimlico needed to be used to add weight to several edges in the graph. 

It was therefore necessary to summarise and aggregate _journeys_ at a _route_ level in order to correctly weight the graph structure to be used for the analysis. Per journey, all pairs of adjacent stations in the path between the start and the end were determined.

```{r summarise_routes}
# Set up routes per journey   
routes <- routes %>% 
    group_by(start_cln, end_cln) %>% 
    mutate(to = lead(path, 1)) %>% 
    na.omit() %>% 
    rename(from = path)
```
   
The number of occurrences for all pairs of adjacent stations was then calculated. As the sample data covered 5% of journeys for a week long period, it was necessary to adjust the total counts to arrive at an approximate value for total daily trips between each pair of adjacent stations.

```{r summarise_route_journeys}    
# Total up all the routes and summarise trips between stations
routes <- journeys %>% 
    select(start_cln, end_cln) %>% 
    left_join(routes) %>% 
    group_by(from, to) %>% 
    summarise(daily_trips = n(),
              daily_trips = daily_trips / 7,
              daily_trips = ceiling(daily_trips * 20))
```

This summary was then joined with the graph structure data to add the daily trips between each pair of adjacent stations. Some pairs of adjacent stations in the graph structure were missing an esimation of the daily trips. This is because they were infrequently visited, outlying stations and no journeys were found to go through them. Such pairs of stations were set to have a number of trips equal to 25% of the average daily trips on their joining TfL tube line.

```{r add_}
# Join in with links data
links <- links %>% 
    left_join(routes, by = c("station1" = "from",
                                   "station2" = "to"))
## Fill in the blanks
# Find average trips per line
avg_per_line <- links %>% 
    group_by(line) %>% 
    summarise(avg_trips = mean(daily_trips, na.rm = T)*0.25)

# Fill in the blanks with a coalesce
links <- links %>% left_join(avg_per_line) %>% 
    mutate(daily_trips = coalesce(daily_trips, avg_trips)) %>% 
    select(-avg_trips)
```

### Create final graph structure

The graph object was then re-created, this time using the daily trips between adjacent stations as the edge weight attribute. A visualisation of the resulting graph structure is presented in figure `r figure`.

```{r create_final_graph}
# Chop out line and distinct, weight by daily trips
graph_data <- links %>% 
    distinct() %>% 
    rename(weight = daily_trips) %>% 
    select(-line)

# Make the graph - retain distance as edge attribute but do not weight edges
tfl_graph <- graph_data %>% 
    graph_from_data_frame(directed = TRUE)

# Clean up
rm(graph_data)
```

#### Figure `r figure`: TfL Underground graph structure

_The graph structure of the TfL Underground in a force directed [@kobourov_spring_2012] layout. Nodes are sized based on the number of visits per day and coloured based on the fare zone which they are assigned to. (Note: this figure is interactive)._

```{r show_graph}
d3_data <- igraph_to_networkD3(tfl_graph, group = rep(1, length(V(tfl_graph))))
d3_data$nodes <- d3_data$nodes %>% 
    left_join(station_details %>% select(name, name_cln, zone_cln),
              by = c("name" = "name_cln")) %>% 
    select(name, name.y, zone_cln) %>% 
    rename(name = name.y,
           name_cln = name,
           group = zone_cln)

# Get estimated visits
daily_visits <- journeys %>% 
    select(start_cln, end_cln) %>% 
    gather(x, name_cln) %>% 
    count(name_cln) %>% 
    mutate(n = n/7,
           n = n*20) %>% 
    ungroup() %>% 
    mutate(n = (n - mean(n))/sd(n))

# Tag stations
d3_data$nodes <- d3_data$nodes %>% left_join(daily_visits, by = "name_cln")

# Set up custom colour scale
ColourScale <- 'd3.scale.ordinal()
            .domain(["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"])
           .range(["#1B9E77", "#D95F02", "#7570B3", "#E7298A", "#66A61E", "#E6AB02", "#A6761D", "#666666", "#6a3d9a", "#c51b7d"]);'

# Make the plot
forceNetwork(Links = d3_data$links, Nodes = d3_data$nodes, Nodesize = "n",
             Source = 'source', Target = 'target', NodeID = 'name', 
             radiusCalculation = JS("d.nodesize+5"),
             Group = 'group', colourScale = JS(ColourScale),
             zoom = T, opacity = 1, legend = T, width = 800, bounded = F)
```

```{r include = F}
figure <- figure + 1
```


## Alternative zoning approaches

### Importance-based zones

#### Station importance

In order to determine the importance of a station, several common node properties were calcualted:

* __Degree__ [@opsahl_node_2010] measures the number of edges incident on a node. In this context it can be thought of as, for each station, how many other stations are connected to it directly.
* __Betweenness__ [@freeman_centrality_1979; @brandes_faster_2001] measures the number of paths going through each node. In this context it can be thought of as how likely it is that travellers will visit or pass through it on other journeys, or how much "connectivity" it provides to the network.
* __Closeness__ [@freeman_centrality_1979; @newman_scientific_2001] measures how many steps exist between each node and all other nodes in the network. In this context, it can be thought of as how central the station is in the overall network (i.e. does it lie at the end of a long branch, or close to the middle of London).
* __Eigenvector centrality__ [@bonacich_power_1987] measures the importance of a node based on how important it is, and how important its neighbours are. In this context it can be thought of as a combination of how connected a station is, and how connected its adjacent stations are.

```{r centralities}
# Degree centrality
deg_cent <- degree(tfl_graph, loops = TRUE)
V(tfl_graph)$deg <- deg_cent

# Betweenness centrality
bet_cent <- betweenness(tfl_graph, directed = T)
V(tfl_graph)$bet <- bet_cent

# Closeness centrality
clo_cent <- closeness(tfl_graph)
V(tfl_graph)$close <- clo_cent

# Eigenvector centrality
eig_cent <- eigen_centrality(tfl_graph, directed = T, scale = T)
V(tfl_graph)$eig <- eig_cent$vector
```

#### Define zones

In order to define fare zones based on station importance, it was necessary to group stations together. To make these zones more comparable to their real-life equivalents a simple decile rank was applied to each measure of station importance. 

```{r centrality_stats}
# Convert to data frame of centrality stats
station_centrality_stats <- data_frame(station = names(V(tfl_graph)),
                            deg = V(tfl_graph)$deg,
                            eig = V(tfl_graph)$eig,
                            bet = V(tfl_graph)$bet,
                            clo = V(tfl_graph)$close)

station_importance_zones <- station_centrality_stats %>% 
    gather(method, value, -station) %>% 
    group_by(method) %>% 
    mutate(zone = ntile(desc(value), 10)) %>% 
    select(-value) %>% 
    spread(method, zone)
```

The real life zones were not equally distributed (i.e. there were more zone 1 stations thatn zone 10). This was not the case for the decile-ranked, importance-based zones, representing a weakness of this approach.

### Graph-community zones

#### Discovering communities

In order to define zones based on communities of stations in the graph, three alternative algorithms were used:

* __Walktrap__ [@pons_computing_2005]. This algorithm detects communities by performing random walks on the graph. The algorithm records the nodes visited in each walk and uses the results to partition the graph in to communities of nodes that frequently occurred in the same walk. In this context, such communities can be imagined to be groups of stations located in close proximity, or on the same train line.
* __Edge betweenness__ [@newman_finding_2004]. This algorithm detects communities based on the _betweeness_ metric of the edges in the graph. The algorithm iteratively removes the edge with the largest betweeness value, dividing the graph into groups and ultimately into invididual nodes. Recording this process, it uses the results to construct communities. In this context, such communities will be defined by stations which serve as "hubs" connecting outlying branches of the graph to the more highly connected centre.
* __Spinglass__ [@reichardt_statistical_2006]. This algorithm uses a more abstract approach to community detection. Briefly, it uses ideas from statistical physics/mechanics to identify groups of nodes which are in some way "similar". In this context it is difficult to suggest a practical intuition which can be used to describe the algorithmic approach. 

```{r graph_communities}
# Hierarchical methods - use cut_at to chop up
# Use distances to help create clusters
clust_w <- cluster_walktrap(tfl_graph, steps = 10, weights = E(tfl_graph)$dist)
clust_eb <- cluster_edge_betweenness(tfl_graph, directed = T, weights = E(tfl_graph)$dist)
# n.b. spins sets number of clusters
clust_sg <- cluster_spinglass(tfl_graph, spins = 10, weights = E(tfl_graph)$dist)
```

As both the _walktrap_ and _edge betweenness_ algorithms are hierarchical, the results were used to create a specified number of communities. The _spinglass_ algorithm has a parameter which was used to pre-specify the number of communities. In each case, ten station communities were specified in order to draw comparisons with the existing ten fare zones. 

```{r station_cluster_stats}
# Create dataframe of results
station_cluster_stats <- data_frame(station = names(V(tfl_graph)),
                                   eb = cut_at(clust_eb, no = 10),
                                   walk = cut_at(clust_w, no = 10),
                                   sg = membership(clust_sg))
```

## Comparisons with reality

### Basic similarity measures

Salton's cosine similarity[-@Salton:1986:IMI:576628] measure was used to compare the differences in importance-based fare zones. Such comparisons were made both between each zoning approach, and with the existing fare-zones.

```{r cosine}
cosines <- station_importance_zones %>% 
         left_join(station_details %>% select(name_cln, zone_cln),
                    by = c("station" = "name_cln")) %>% 
         rename(Betweenness = bet,
                Closeness = clo,
                Degree = deg,
                Eigenvector = eig,
                Zone = zone_cln) %>% 
         select(-station) %>% 
         as.matrix() %>% 
         lsa::cosine() %>% 
         round(3)
```

### Financial approach

Using the journeys data, it was possible to calculate the total fares charged to Oyster pay-as-you-go customers on each day of the week.

```{r journey_summary}
# Group and total
journey_summaries <- journeys %>% 
    group_by(downo,
             daytype,
             start_zone,
             end_zone) %>% 
    summarise(journeys = n(),
              total_rev = sum(dfare, na.rm = TRUE) / 100) %>% 
    ungroup() %>% 
    mutate(journeys_scaled = 20 * journeys,
           total_rev_scaled = 20 * total_rev,
           rough_cpj = total_rev_scaled / journeys_scaled)
```

Using a similar approach, it was also possible to estimate the cost of a single trip from any one zone to any other single zone.

```{r zone_costs}
# Get rough zone-zone costs
zone_costs <- journeys %>% 
    group_by(start_zone,
             end_zone) %>% 
    summarise(journeys = n(),
              total_rev = sum(dfare, na.rm = TRUE) / 100) %>% 
    ungroup() %>% 
    mutate(journeys_scaled = 20 * journeys,
           total_rev_scaled = 20 * total_rev,
           cpj = total_rev_scaled / journeys_scaled) %>% 
    select(start_zone, end_zone, cpj)
```

These zone-zone costs were then used (along with the sample journeys data) to estimate the total daily fare received under each alternative zoning approach. This was done by: 

* assuming the cost of each zone-zone ticket (based on the sample journey data) remained constant under each model; 
* iteratively changing the zone of each station based on the alternative approach being considered; and
* calculating the total fares charged per day based on the fixed zone-zone costs and alternative zoning approach being considered.

```{r fare_estimations}
# Create function to ease iteration over approaches
calculate_daily_fares <- function(how) {
    # the `how` argument will be one of the methods
    # either "current", "bet", "deg", "eig", or "clo"

    # Return "current" fares if requested
if ( how == "current" ) {
    ans <- journey_summaries %>%
            group_by(downo, daytype) %>% 
            summarise(total_fare_rev = sum(total_rev),
                      total_fare_rev_scale = sum(total_rev_scaled)) %>% 
            mutate(how = "current") %>% 
            ungroup()
} else {
    # Calculate temporary zones for the desired metric
    temp_zones <- station_centrality_stats %>% 
        gather(measure, value, -station) %>% 
        group_by(measure) %>% 
        mutate(zone = ntile(desc(value), 10)) %>% 
        filter(measure == how) %>% 
        ungroup() %>% 
        select(station, zone)

    # Get journies data
    temp_journeys <- journeys %>% 
        select(start_cln, end_cln, ffare, dfare, downo, daytype)

    # Join on new zones based on importance metric
    temp_journeys_to_zones <- temp_journeys %>% 
        left_join(temp_zones, by = c("start_cln" = "station")) %>% 
        rename(start_zone = zone) %>% 
        left_join(temp_zones, by = c("end_cln" = "station")) %>% 
        rename(end_zone = zone)     
        
    # Join on pricing info from current zone-zone costs
    temp_journeys_to_zones_to_prices <- temp_journeys_to_zones %>% 
        left_join(zone_costs, by = c("start_zone", "end_zone"))
    
    # Create daily summary
    ans <- temp_journeys_to_zones_to_prices %>% 
        group_by(downo, daytype) %>% 
        summarise(total_fare_rev = sum(cpj)) %>% 
        mutate(total_fare_rev_scale = 20 * total_fare_rev) %>% 
        ungroup() %>% 
        mutate(how = how)
}    

# Push back the summary as the result
return(ans)
}

# Set up list of methods
methods <- c("current", "deg", "eig", "bet", "clo")

# Loop over each method/zoning approach
fare_estimations <- lapply(methods, calculate_daily_fares) %>% bind_rows()
```

The differences in estimated fares were also compared statistically using Tukey's [-@tukey_comparing_1949] "Honest Significant Difference" method.

```{r hsd}
fit <- aov(total_fare_rev_scale ~ how, data = fare_estimations)
tukey <- TukeyHSD(fit, conf.level = 0.95)
tukey <- broom::tidy(tukey) %>% 
            select(-term, -conf.low, -conf.high) %>% 
            mutate(comparison = gsub("clo", "Closeness", comparison),
                   comparison = gsub("bet", "Betweenness", comparison),
                   comparison = gsub("eig", "Eigenvector", comparison),
                   comparison = gsub("deg", "Degree", comparison),
                   comparison = gsub("current", "Existing Zones", comparison),
                   estimate = round(estimate, 0),
                   adj.p.value = round(adj.p.value, 3))
```


The results of this analysis are presented in the results section.

### Graph theoretical approach

As the station communities were more arbitrarily defined than the importance-based zones, comparisons of estimated fares were not made. Instead, metrics suggested by 
Yang and Leskovec [-@yang_defining_2015] were used to measure the "goodness" of the detected communities. The same metrics were calculated for the existing fare zones and the results compared.

Yang and Leskovec [-@yang_defining_2015] state that "good" communities are small, with many links between nodes _within_ the community and a small number of links connected to other communities.

Amongst other metrics, they define the _density_ and _separability_ of a community:

\begin{equation}
Density = \dfrac{m_S}{n_s(n_s-1)/2}
\end{equation}

\begin{equation}
Separability = \dfrac{m_S}{c_S}
\end{equation}

Where $n_S$ is the number of nodes _within_ the community, $m_S$ the number of edges entirely within the community and $c_S$ the number of edges link the community to another community.

These metrics were calculated for each of the communities detected using the three algorithms mentioned above, as well as for the existing fare zones.

```{r community_metrics}
get_cluster_stats <- function(how, data) {
# Function argument 'how' defines which community detection method (inc. current zone)
# should be used 
    
# Take created "zones" (clusters) and join in actual zones, gather into long table
    tmp_zones <- data %>% 
        left_join(station_details %>% select(name_cln, zone_cln),
                  by = c("station" = "name_cln")) %>% 
        gather(method, value, -station) %>% 
        filter(method == how) %>% 
        select(station, value) %>% 
        rename(zone = value)
    
    # Get stations per zone (n_s)
    stations_per_zone <- tmp_zones %>% count(zone)
    
    # Join in "zones" to links (edges)
    tmp_links <- links %>% 
        left_join(tmp_zones, by = c("station1" = "station")) %>% 
        rename(zone1 = zone) %>% 
        left_join(tmp_zones, by = c("station2" = "station")) %>% 
        rename(zone2 = zone)
    
    # Add zone-zone comparisons to get m_s and c_s
    tmp_links <- tmp_links %>% 
        mutate(ms = ifelse(zone1 == zone2, 1, 0),
               cs = ifelse(zone1 != zone2, 1, 0))
    
    # Aggregate across zone
    zone_summary <- tmp_links %>% 
        group_by(zone1) %>% 
        summarise(ms = sum(ms),
                  cs = sum(cs)) %>% 
        left_join(stations_per_zone, by = c("zone1" = "zone"))
    
    # Perform summaries
    zone_summary <- zone_summary %>% 
        mutate(Separability = ms / cs,
               Density = ms / (n*(n - 1) / 2),
               method = how)
    
    return(zone_summary)
}

methods <- c("zone_cln", "eb", "walk", "sg")
community_metrics <- lapply(methods, get_cluster_stats, 
                            data = station_cluster_stats) %>% bind_rows()
```

The statistical differences in these metrics were calculated for each of the community detection algorithms using Tukey's Honest Significant Differences.

```{r tukey_dens_sep}
fit_sep <- aov(Separability ~ method, community_metrics)
tukey_sep <- TukeyHSD(fit_sep, conf.level = .95) %>% 
             broom::tidy() %>% 
             select(-term, -conf.low, -conf.high) %>% 
             mutate(comparison = gsub("eb", "Edge betweenness", comparison),
                    comparison = gsub("walk", "Walktrap", comparison),
                    comparison = gsub("sg", "Spinglass", comparison),
                    comparison = gsub("zone_cln", "Existing Zones", comparison),
                    estimate = round(estimate, 3),
                    adj.p.value = round(adj.p.value, 4))

fit_den <- aov(Density ~ method, community_metrics)
tukey_den <- TukeyHSD(fit_den, conf.level = .95) %>% 
             broom::tidy() %>% 
             select(-term, -conf.low, -conf.high) %>% 
             mutate(comparison = gsub("eb", "Edge betweenness", comparison),
                    comparison = gsub("walk", "Walktrap", comparison),
                    comparison = gsub("sg", "Spinglass", comparison),
                    comparison = gsub("zone_cln", "Existing Zones", comparison),
                    estimate = round(estimate, 3),
                    adj.p.value = round(adj.p.value, 4))

```

# Results

## Importance-based zones

Station importance was found to vary substantially based on the metric used to calculate it. Figure `r figure` shows (normalised) station importance for each line in the TfL underground for each of the importance metrics used. Intuitively, in this setting, the four methods each measure different aspects of the TfL network. Closeness and eigenvector centrality measure how central a the proximity to central London and importance, respectively. Degree centrality could be imagined to measure the "usefulness" of each station, independant of the other stations it was connected to (with usefulness being directly proportional to the number of adjacent stations). Betweeness measures how much each station helps to connect the overall network, and could be used to identify critical points in the network. The analytical differences between these zones and the existing fare zones are presented below.

#### Figure `r figure`: Normalised station importance metrics

```{r importance_plot, fig.align='centre', out.width = 800, out.height=1000}
links %>% 
    select(line, station1, station2) %>% 
    gather(key, station, -line) %>% 
    select(-key) %>% 
    distinct() %>% 
    left_join(station_lk) %>% 
    left_join(station_centrality_stats) %>% 
    select(name, station, deg, eig, bet, clo) %>% 
    rename(Betweenness = bet,
           `Eigenvector Centrality` = eig,
           Closeness = clo,
           `Degree Centrality` = deg) %>% 
    gather(method, value, -name, -station) %>% 
    group_by(method) %>% 
    mutate(value_norm = (value - mean(value)) / sd(value),
           name = gsub(" Line", "", name),
           name = gsub("East London", "Overground", name)) %>% 
    ggplot(aes(x = name, y = value_norm, fill = name)) +
    geom_boxplot() +
    #geom_point(aes(colour = name), alpha = 0.75) + 
    scale_fill_manual(values = c(bakerloo, central, circle, district, dlr, hc,
                                 jubilee, metropolitan, northern, overground,
                                 picadilly, victoria, wc)) +
    scale_colour_manual(values = c(bakerloo, central, circle, district, dlr, hc,
                                 jubilee, metropolitan, northern, overground,
                                 picadilly, victoria, wc)) +
    facet_wrap(~method) +
    coord_flip() +
    xlab("") +
    ylab("") +
    guides(fill = FALSE,
           colour = FALSE) +
    theme_jim +
    theme(axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10))
```

```{r include = F}
figure <- figure + 1
```

### Basic similarity

The importance-based zones did not appear to be geographically distinct from one another (appendix TODO) in the same way as the existing fare zones. However, when measured with Salton's [-@Salton:1986:IMI:576628] cosine measure, the zones defined using station importance metrics showed a high similarity with each other, and with the exising fare zones. This was slightly surprising given the forced, equal distribution of the number of stations in each importance-based zone (not the case for the existing zones).

```{r show_cosine}
cosines %>% kable(caption = "Cosine similarity measures between zones defined using importance-based metrics and the existing TfL payment zones.")
```

Eigenvector centrality-based zoning produced the most similar results to the existing zones, and closeness-based zones also showed a high similarity with the existing zones. This was an intuitive result: the current fare zones are defined in some way based on a station's proximity to central London (similar to closeness) and it's importance in daily London life (similar to Eigenvector centrality). It therefore made sense that these two measures showed the most similarity with the existing model.

### Financial differences

Using the approach outlined previously, daily fares were estimated using each of the three importance-based zoning approaches and compared with the daily fares using the current fare-zoning method. The results of this comparison are presented in figure `r figure`.

#### Figure `r figure`: Estimated daily Tfl Underground Oyster-PAYG-only fares under each fare-zoning approach

```{r daily_fares}
fare_estimations %>% 
    mutate(how = gsub("current", "Current", how),
           how = gsub("bet", "Betweenness", how),
           how = gsub("clo", "Closeness", how),
           how = gsub("deg", "Degree", how),
           how = gsub("eig", "Eigenvector", how)) %>% 
    ggplot(aes(x = daytype, y = total_fare_rev_scale)) +
    geom_bar(aes(fill = how), stat = "identity", position = "dodge",
             colour = "white") +
    scale_x_discrete(limits = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")) +
    scale_y_continuous(labels = scales::dollar_format(prefix = "£")) + 
    scale_fill_brewer(palette = "Dark2") +
    guides(fill = guide_legend(title = "", nrow = 2)) +
    xlab("") + 
    ylab("") +
    theme_jim
```

```{r include = F}
figure <- figure + 1
```

The alternative approaches consistently out-performed the current zoning approach when considering expected daily fares (Tfl Underground, Oyster-PAYG only). 

Zones created through closeness measures most closely matched the figures for the current zones, which was to be expected given the similarities in how they are defined (based on distance to all other stations). The increase in fares based on closeness zones relative to the current zone model was probably due to the equal distribution of stations to zones enforced by the decile-rank approach to creating the zones. 

Betweenness and degree centrality-based zones were found to perform "best", i.e. generated the highest fares. This is due to the fact that many of the stations have large differences in zone when moving from the importance-based to the current fare zone models (appendix TODO). For example, Regent's Park station is currently in fare zone 1 due to it's proximity to central London. However, as it is not a well-visited station on one of the less-busy lines (the Bakerloo line), it is placed in to zone 10 for by betweenness-based zoning. Eigenvector centrality-based zoning also out-performed the current fare zones and was found to produce results most similar to the existing zones. 

However, none of the differences in estimated fare were found to be statistically significant as measured by Tukey's Honest Significant Differences [-@tukey_comparing_1949] (table TODO).

```{r show_tukey}
tukey %>% kable(col.names = c("Comparison", "Fare difference (£)", "p-Value"),
                caption = "Table TODO: Tukey HSDs for financial differences in zoning approach")
```

## Graph-community zones

### Geographical distribution

The graph communities discovered by the three algorithms employed generally showed distinct geographical separation from one another (figure `r figure`).


#### Figure `r figure`: The geographic distribution of existing and graph-community zones. 

_The figure is interactive, select a community detection algorithm and toggle area names on/off with the buttons provided._

```{r show_graph_comms}
# Create plotting data
clusters <- station_cluster_stats %>% 
    left_join(station_details %>% 
                  select(name_cln, 
                         latitude, 
                         longitude,
                         zone_cln,
                         name),
              by = c("station" = "name_cln")) %>% 
    select(name, latitude, longitude, eb, walk, sg, zone_cln) %>% 
    gather(method, value, -name, -latitude, -longitude) %>% 
    rename(lat = latitude,
           lon = longitude) %>% 
    mutate(value = as.factor(value),
           popup = paste(name, "- Zone/Cluster", value))

# Set up colours
pal <- colorFactor(c(RColorBrewer::brewer.pal(8, "Dark2"), "#6a3d9a", "#c51b7d"), 
                   domain = levels(clusters$value))

# Make the map
leaflet(width = 800) %>% 
    # Add map tiles
    addProviderTiles("CartoDB.PositronNoLabels", group = "No area labels") %>% 
    addProviderTiles("CartoDB.Positron", group = "Show/hide area labels") %>% 
    # Add zones
    addCircles(data = clusters %>% filter(method == "zone_cln"), radius = 250, 
               stroke = F, fillColor = ~pal(value), fillOpacity = 1, 
               group = "Zones (default)", popup = ~popup) %>% 
    # Add edge-bet clusters
    addCircles(data = clusters %>% filter(method == "eb"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Edge Betweenness", popup = ~popup) %>%
    # Add walktrap clusters
    addCircles(data = clusters %>% filter(method == "walk"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Walktrap", popup = ~popup) %>%
    # Add spinglass clusters
    addCircles(data = clusters %>% filter(method == "sg"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Spinglass", popup = ~popup) %>%
    # Add a zone legend
    addLegend("bottomright", pal = pal, values = as.factor(1:10), opacity = 1, 
              title = "Zone/Cluster") %>% 
    # Add layers control box
    addLayersControl(baseGroups = c("Zones (default)", "Edge Betweenness", 
                                    "Walktrap", "Spinglass"),
                     overlayGroups = c("Show/hide area labels"),
                     options = layersControlOptions(collapsed = F))
```

```{r include = F}
figure <- figure + 1
```

The geographical distribution of communities defined using edge betweenness show the most similarity with the exising zones. There is a clear central zone, surrounded by radial/branch zones which could be broken down by geographic region or using a similar approach to that used for London postcodes. 

The walktrap algorithm did not performed well in this setting: there is little intuitive distinction between communities when viewed geographically.

Whilst not as geographically distinct as the edge-betweenness communities, the spinglass algorithm generated generated clear communities which could be used to create fare zones relatively simply. 

### Theoretical properties

The density and separability [@yang_defining_2015] metrics for each approach (along with the existing zones) are showm in figure TODO.

Discounting the walktrap algorithm (based on its poor performance when viewing the results geographically), the best approach appears to be the edge betweenness algorithm. It generates dense communities which are separable from each other. Spinglass also performed well, although the communities it detected were less dense and less separable thant those for edge-betweenness.

Both edge betweenness and spinglass generated communities which were more dense and more separable than the existing fare zones. The differences in density were not statistically significant when measured using Tukey's Honest Significant Differences (tables TODO and TODO), however edge betweenness produced significantly more separable clusters than the current zones. 

This results was not, however, particularly surprising as the existing zones were generated in such a different manner than the graph communities. 

#### Figure `r figure`: Community density and separability for communities detected with the edge-betweeness, walktrap, and spinglass algorithms. Also shown are the density and separability values for the existing fare zones.

```{r dens_sep, out.width = 800}
community_metrics %>% 
    mutate(Density = ifelse(is.na(Density), 0, Density)) %>% 
    select(zone1, Separability, Density, method) %>% 
    gather(measure, value, -zone1, -method) %>% 
    mutate(method = gsub("zone_cln", "Zones", method),
           method = gsub("eb", "Edge\nbetweenness", method),
           method = gsub("sg", "Spinglass", method),
           method = gsub("walk", "Walktrap", method)) %>% 
    ggplot(aes(x = method, y = value, fill = method)) +
    geom_boxplot() +
    scale_fill_brewer(palette = "Dark2") +
    facet_grid(measure~., scales = "free") +
    guides(fill = FALSE) + 
    xlab("") +
    ylab("") +
    theme_jim
```

```{r include = F}
figure <- figure + 1
```

```{r show_tukey_dens}
tukey_den %>% kable(col.names = c("Comparison", "Difference", "p-Value"),
                caption = "Table TODO: Tukey HSDs for differences in community densities")
```

```{r show_tukey_sep}
tukey_sep %>% kable(col.names = c("Comparison", "Difference", "p-Value"),
                caption = "Table TODO: Tukey HSDs for differences in community separability")
```

# Conclusions

Some summary.

# Appendices

#### Appendix `r app`: _Software packages used_

The `R` language [@cite_r] was used to perform the data manipulation, modelling and visualisations for this report. The following `R` packages were also used:

* `igraph` [@csardi_igraph:_2015];
* `readr` [@wickham_readr:_2016];
* `readxl` [@wickham_readxl:_2016];
* `dplyr` [@wickham_dplyr:_2016];
* `tidyr` [@wickham_tidyr:_2016];
* `stringr` [@wickham_stringr:_2015];
* `purrr` [@wickham_purrr:_2016];
* `lsa` [@wild_lsa:_2015];
* `broom` [@robinson_broom:_2016];
* `ggplot2` [@wickham_ggplot2:_2016];
* `networkD3` [@gandrud_networkd3:_2016];
* `leaflet` [@cheng_leaflet:_2016]; and
* `DT` [@xie_dt:_2016].

```{r include = F}
app <- app + 1
```

<br>

#### Appendix `r app`: _Cleaned graph structure data._

```{r}
datatable(links, colnames = c("Line", "From", "To", "Distance (km)", "Daily trips"))
```

```{r include = F}
app <- app + 1
```

<br>

#### Appendix `r app`: _The geographic distribution of existing and importance-based fare zones. The figure is interactive, select a fare-zoning approach and toggle area names on/off with the buttons provided._

```{r}
# Create plotting data
clusters <- station_importance_zones %>% 
    left_join(station_details %>% 
                  select(name_cln, 
                         latitude, 
                         longitude,
                         zone_cln,
                         name),
              by = c("station" = "name_cln")) %>% 
    select(name, latitude, longitude, bet, clo, deg, eig, zone_cln) %>% 
    gather(method, value, -name, -latitude, -longitude) %>% 
    rename(lat = latitude,
           lon = longitude) %>% 
    mutate(value = as.factor(value),
           popup = paste(name, "- Zone/Cluster", value))

# Set up colour palette
pal <- colorFactor(c(RColorBrewer::brewer.pal(8, "Dark2"), "#6a3d9a", "#c51b7d"), 
                   domain = levels(clusters$value))

# Make the map
leaflet(width = 800) %>% 
    addProviderTiles("CartoDB.PositronNoLabels", group = "No area labels") %>% 
    addProviderTiles("CartoDB.Positron", group = "Show/hide area labels") %>% 
    addCircles(data = clusters %>% filter(method == "zone_cln"), radius = 250, 
               stroke = F, fillColor = ~pal(value), fillOpacity = 1, 
               group = "Zones (default)", popup = ~popup) %>% 
    addCircles(data = clusters %>% filter(method == "bet"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Betweenness", popup = ~popup) %>%
    addCircles(data = clusters %>% filter(method == "eig"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Eigenvector centrality", popup = ~popup) %>%
    addCircles(data = clusters %>% filter(method == "clo"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Closeness", popup = ~popup) %>%
        addCircles(data = clusters %>% filter(method == "deg"), radius = 250,
               stroke = F, fillColor = ~pal(value), fillOpacity = 1,
               group = "Degree centrality", popup = ~popup) %>%
    # Layers control
    addLayersControl(
        baseGroups = c("Zones (default)", "Betweenness", "Eigenvector centrality",
                       "Closeness", "Degree centrality"),
        overlayGroups = c("Show/hide area labels"),
        options = layersControlOptions(collapsed = F)
    ) %>% 
    addLegend("bottomright", pal = pal, values = as.factor(1:10), opacity = 1, 
              title = "Zone/Cluster")
```

```{r include = F}
app <- app + 1
```

<br>

#### Appendix `r app`: _The list of stations and their associated, importance-based fare zones._

```{r show_imp_fare_zones}
# Create plotting data
station_importance_zones %>% 
    left_join(station_details %>% 
                  select(name_cln, 
                         zone_cln,
                         name),
              by = c("station" = "name_cln")) %>% 
    select(name, bet, clo, deg, eig, zone_cln) %>% 
    datatable(colnames = c("Station", "Betweenness", "Closeness", "Degree", 
                           "Eigenvector", "Existing Zone"),
              caption = "Fare zones based on node importance measures")
    
```

```{r include = F}
app <- app + 1
```

# References


