---
title: "BS1808 Logisitics and Supply Chain Analytics Individual Assignment"
author: "Jim Leach"
date: "19 May 2016"
output:
  html_document:
    code_folding: hide
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

# Introduction

## Project overview

For this project data from a large fast-food restaurant chain in the U.S.A. were provided. The data covered approximately 4 months of data from March until June 2015 in four restaurants: two in Berkley, California, and two in New York, New York. The data provided transaction information (e.g. which menu items were purchased and in what quantities), ingredient lists for all menu items, and associated metadata (e.g. store location, store type etc). 

The goal of the project was to forecast demand for lettuce for each of the the four locations. The forecast period was two weeks following the end of the data (from 2015-06-16 to 2015-06-29). 

## This document

This document is split in to TODO distinct sections, TODO. 

The `R` code used to perform these analyses has been provided as supplementary files, and sections of the `R` code can be viewed in this report using the _Code_ buttons to toggle code viewing.

This `HTML` report is best viewed using a modern web browser such as Mozilla Firefox or Google Chrome.

```{r prep, echo = TRUE, message=FALSE}
# Set default knitr options
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# Load packages
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(forecast)
library(purrr)
library(knitr)

# Set up theme object for prettier plots
theme_jim <-  theme(legend.position = "bottom",
                    axis.text.y = element_text(size = 14, colour = "black"),
                    axis.text.x = element_text(size = 14, colour = "black"),
                    legend.text = element_text(size = 14),
                    legend.title = element_text(size = 14),
                    title = element_text(size = 16),
                    strip.text = element_text(size = 14, colour = "black"),
                    strip.background = element_rect(fill = "white"),
                    panel.grid.minor.x = element_blank(),
                    panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
                    panel.grid.minor.y = element_line(colour = "lightgrey", linetype = "dotted"),
                    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
                    panel.margin.y = unit(0.1, units = "in"),
                    panel.background = element_rect(fill = "white", colour = "lightgrey"),
                    panel.border = element_rect(colour = "black", fill = NA))

# Source the data from the databse
db <- src_postgres("lsca")

# Function that takes a string and converts it in to "proper case" (i.e.
# the first letter is capitalised, all remaining letters are lower case)
# N.b. for multi-word strings, only the first word will be affected
toproper <- function(x) { 
  first <- substring(x, 1, 1) %>% toupper()
  rest <- substring(x, 2) %>% tolower()
  whole <- paste0(first, rest)
  return(whole)
}

# Function that takes a string and converts it in to "proper case" (i.e.
# the first letter is capitalised, all remaining letters are lower case)
# N.b. for multi-word strings, only the first word will be affected
toproper <- function(x) { 
  first <- substring(x, 1, 1) %>% toupper()
  rest <- substring(x, 2) %>% tolower()
  whole <- paste0(first, rest)
  return(whole)
}

# Set lm tidy coef names
tidy_names <- c("Term", "Estimate", "Std. Error", "t-Stat.", "p-Value")
```

# Data preparation

## Loading data

The data were loaded in to a `Postgresql` database for storage and handling.

```{r load_data, eval = FALSE}
# Load packages for reading data
library(readr)
library(readxl)

# Read in the data
ingredients <- read_csv("./data/hw/ingredients.csv")
menu_items <- read_csv("./data/hw/menu_items.csv")
menuitem <-  read_csv("./data/hw/menuitem.csv") %>% mutate(date = lubridate::ymd(date))
portion_uom_types <- read_csv("./data/hw/portion_uom_types.csv")
pos_ordersale <- read_csv("./data/hw/pos_ordersale.csv") %>% mutate(date = lubridate::ymd(date))
recipe_ingredient_assignments <- read_csv("./data/hw/recipe_ingredient_assignments.csv")
recipe_sub_recipe_assignments <- read_csv("./data/hw/recipe_sub_recipe_assignments.csv")
recipes <- read_csv("./data/hw/recipes.csv")
store_restaurant <- read_excel("./data/hw/store_restaurant.xlsx")
sub_recipe_ingr_assignments <- read_csv("./data/hw/sub_recipe_ingr_assignments.csv")
sub_recipes <- read_csv("./data/hw/sub_recipes.csv")

# Set all field names to lowercase for easier typing later
colnames(ingredients) <- colnames(ingredients) %>% tolower()
colnames(menu_items) <- colnames(menu_items) %>% tolower()
colnames(menuitem) <- colnames(menuitem) %>% tolower()
colnames(portion_uom_types) <- colnames(portion_uom_types) %>% tolower()
colnames(pos_ordersale) <- colnames(pos_ordersale) %>% tolower()
colnames(recipe_ingredient_assignments) <- colnames(recipe_ingredient_assignments) %>% tolower()
colnames(recipe_sub_recipe_assignments) <- colnames(recipe_sub_recipe_assignments) %>% tolower()
colnames(recipes) <- colnames(recipes) %>% tolower()
colnames(store_restaurant) <- colnames(store_restaurant) %>% tolower()
colnames(sub_recipe_ingr_assignments) <- colnames(sub_recipe_ingr_assignments) %>% tolower()
colnames(sub_recipes) <- colnames(sub_recipes) %>% tolower()

# Load the data to the database
tmp <- copy_to(db, ingredients, temporary = FALSE)
tmp <- copy_to(db, menu_items, temporary = FALSE)
tmp <- copy_to(db, menuitem, temporary = FALSE)
tmp <- copy_to(db, portion_uom_types, temporary = FALSE)
tmp <- copy_to(db, pos_ordersale, temporary = FALSE)
tmp <- copy_to(db, recipe_ingredient_assignments, temporary = FALSE)
tmp <- copy_to(db, recipe_sub_recipe_assignments, temporary = FALSE)
tmp <- copy_to(db, recipes, temporary = FALSE)
tmp <- copy_to(db, store_restaurant, temporary = FALSE)
tmp <- copy_to(db, sub_recipe_ingr_assignments, temporary = FALSE)
tmp <- copy_to(db, sub_recipes, temporary = FALSE)
```

After loading the data to the database, `R` scripts were run to connect to the database and make it available in `R`.

```{r get_data}

# Set up sql command to get all relevant tables
sql <- sql("select * from information_schema.tables where table_schema = 'public'")

# Get the set of tables
tbls <- tbl(db, sql) %>% collect() %>% .$table_name

# Loop over table names to pull in the data
for(i in seq_along(tbls)) {
  assign(tbls[i], tbl(db, tbls[i]))
}

# Clean up
rm(sql, tbls, i)
```

As sales, recipes, sub-recipes and ingredients were all stored in separate tables, several steps were required to create a view of the demand for lettuce on a single day. 

## Ingredient requirements

Firstly, all recipies containing lettuce were found from the data, discarding other recipes (as they were not relevant). The amount of lettuce required for each recipe was found. As some recipies listed the amount in ounces and others in grams, the amount required for each recipe was standardised to be measure in grams. This produced a clean data set that listed the total amounts of lettuce required in each recipe. 

```{r recipies_to_ingredients, message = FALSE}
tom_let_recipes <- recipes %>% 
  left_join(recipe_ingredient_assignments) %>%
  left_join(ingredients) %>%
  select(recipeid,
         ingredientid,
         quantity,
         ingredientname,
         portionuomtypeid) %>%
  left_join(portion_uom_types) %>%
  collect() %>%
  mutate(tom = ifelse(grepl("tomato|Tomato", ingredientname), 1, 0),
         let = ifelse(grepl("lettuce|Lettuce", ingredientname), 1, 0)) %>%
  filter(tom == 1 | let == 1) %>% 
  select(recipeid,
         ingredientname,
         quantity,
         portiontypedescription,
         tom,
         let) %>% 
  mutate(quantity_clean = ifelse(portiontypedescription == "Gram",
                                 quantity,
                                 28.3495 * quantity),
         ingredient = ifelse(tom == 1, "tomato", "lettuce")) %>% 
  select(recipeid, ingredient, quantity_clean) %>% 
  spread(ingredient, quantity_clean) %>% 
  mutate(lettuce = ifelse(is.na(lettuce), 0, lettuce),
         tomato = ifelse(is.na(tomato), 0, tomato))
```

A similar process was performed for all _sub_-recipies containing lettuce. The amount of lettuce required for each subrecipe was found and standardised to grams. 

```{r subrecipes_to_ingredients, message = FALSE}
tom_let_sub_recipes <- sub_recipes %>% 
  left_join(sub_recipe_ingr_assignments) %>% 
  left_join(ingredients) %>% 
  select(subrecipeid,
         ingredientid,
         quantity,
         ingredientname,
         portionuomtypeid) %>% 
  left_join(portion_uom_types) %>%
  collect() %>% 
  mutate(tom = ifelse(grepl("tomato|Tomato", ingredientname), 1, 0),
         let = ifelse(grepl("lettuce|Lettuce", ingredientname), 1, 0)) %>%
  filter(tom == 1 | let == 1) %>% 
  select(subrecipeid,
         ingredientname,
         quantity,
         portiontypedescription,
         tom,
         let) %>% 
  mutate(quantity_clean = ifelse(portiontypedescription == "Gram",
                                 quantity,
                                 28.3495 * quantity),
         ingredient = ifelse(tom == 1, "tomato", "lettuce")) %>% 
  select(subrecipeid, ingredient, quantity_clean) %>% 
  spread(ingredient, quantity_clean) %>% 
  mutate(lettuce = ifelse(is.na(lettuce), 0, lettuce),
         tomato = ifelse(is.na(tomato), 0, tomato))
```

The linkage between recipe and sub-recipes was then used to calculate, for each recipe, the total amounts of lettuce required by the sub-recipe(s) beneath it.

```{r recipe_to_sub_reqs, message = FALSE}
recipe_to_sub_tom_let <- recipe_sub_recipe_assignments %>% 
  select(recipeid, subrecipeid, factor) %>% 
  inner_join(tom_let_sub_recipes, copy = T) %>% 
  mutate(lettuce = lettuce * factor,
         tomato = tomato * factor) %>% 
  select(recipeid, subrecipeid, lettuce, tomato) %>% 
  rename(sub_let = lettuce,
         sub_tom = tomato) %>% 
  collect() %>% 
  group_by(recipeid) %>% 
  summarise(sub_let = sum(sub_let, na.rm = T),
            sub_tom = sum(sub_tom, na.rm = T))
```

The linkage between recipe and sub-recipe was then used to calculate the _total_ lettuce requirements for each recipe, i.e. combining the requirements of each recipe, as well as the requirements of any sub-recipes beneath each recipe. These data can be seen in table one.

```{r recipes_to_tom_let, message = FALSE}
recipes_to_tom_let <- tom_let_recipes %>% 
  full_join(recipe_to_sub_tom_let) %>% 
  select(recipeid, lettuce, tomato, sub_let, sub_tom) %>% 
  rowwise() %>% 
  mutate(total_lettuce = sum(lettuce, sub_let, na.rm = T),
         total_tomato = sum(tomato, sub_tom, na.rm = T)) %>% 
  select(recipeid, total_lettuce) %>% 
  filter(total_lettuce > 0) %>% 
  collect() %>% 
  left_join(recipes, copy = T) %>% 
  select(recipeid, recipename, recipedescription, total_lettuce)


DT::datatable(recipes_to_tom_let, 
              colnames = c("Recipe ID", "Recipe name", "Description", "Lettuce (g)"),
              caption = "Table 1: Total lettuce requirements in grams for all recipes with at least some requiement for lettuce.", rownames = FALSE)
```

## Orders requirements

The lettuce requirements for each menu item were then determined, based on the recipe(s) requirements for each item.

```{r menu_items_to_reqs, message = FALSE}
menu_items_to_tom_let <- menu_items %>% 
  collect() %>% 
  inner_join(recipes_to_tom_let)
```

The linkage of orders to menu items was then used to determine the lettuce requirements for each order, based on the items selected in each order, and the number of each item selected (for example, requirements would double for the same item order twice within one order).

```{r orders_to_reqs, message = FALSE}
# Use menuitem to link ordersale md5 IDs to total lettuce and tomato requirements
let_tom_reqs <- menuitem %>%
  select(md5key_ordersale, quantity, plu, id, date) %>% 
  collect() %>% 
  inner_join(menu_items_to_tom_let, by = c("plu" = "plu", 
                                           "id" = "menuitemid")) %>% 
  mutate(lettuce = quantity * total_lettuce) %>% 
  select(md5key_ordersale, lettuce) %>% 
  group_by(md5key_ordersale) %>% 
  summarise(lettuce = sum(lettuce))

# Link lettuce and tomato requirements to sales
sales_to_demand <- pos_ordersale %>% 
  collect() %>% 
  inner_join(let_tom_reqs)
```

The daily demand of lettuce at each store was then determined via a simple summarisation process.

```{r daily_demand}
# Summarise by data to create ts
daily_demand <- sales_to_demand %>% 
  group_by(storenumber, date) %>% 
  summarise(lettuce = sum(lettuce)) %>% 
  mutate(store = as.character(storenumber)) %>% 
  left_join(store_restaurant, by = c("storenumber" = "store_number"), copy = T) %>% 
  mutate(store_address1 = stringr::str_trim(store_address1),
         store_city = stringr::str_trim(store_city),
         store_state = stringr::str_trim(store_state)) %>% 
  select(store, store_state, store_city, date, lettuce) %>% 
  ungroup()
```

## Visualising demand

The daily demand of lettuce at each of the four restaurants (each small multiple in the plot) in the data was visualised in order to understand the structure of the data a little better, in order that sensible forecasting methods could be applied.

Figure one shows that there does not appear to be a _strong_ trend in either the Californian or the New York restaurants. Restaurant 20974 initially shows very low demand, before picking up to show demand comparable to the other stores. There is clearly some seasonality in the demand within Californian restaurants which appears to be at a weekly level. Such a strong pattern of seasonality is not observed for the restaurants in New York.

```{r vis_let_ts, fig.cap = "Figure 1: Daily demand for lettuce, split by restaurant. The dashed grey line shows the train/test split point for fitting models (the last two weeks of the data).", fig.align="centre", out.width=800, out.height=1000}
daily_demand %>% 
  ggplot(aes(x = date, y = lettuce, colour = store_state)) +
  geom_line(size = 1.25) +
  scale_x_date(date_breaks = "2 week") +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  facet_grid(store ~ .) +
  guides(colour = guide_legend(title = "Store state")) +
  xlab("") +
  ylab("Demand for lettuce (g)") +
  theme_jim + 
  scale_y_continuous(labels = scales::comma) +
  theme(axis.text.x = element_text(angle = 45,
                                   vjust = 0.5,
                                   hjust = .5)) +
  geom_vline(xintercept = 16587, linetype = "dashed", colour = "darkgrey")
```

In order to compare forecasting methods it was necessary to select some data from the time series for testing/evaluation purposes. It was decided that the last two weeks of data from each series would be used to form a testing set of data. The dashed grey lines in figure one show this cut-off of training and testing data.

## Train and test split

Using the final two-week period of each series for testing forecasting approaches, the data were split in to a train and test set prior to fitting any models. The data were [nested](https://blog.rstudio.org/2016/02/02/tidyr-0-4-0/) by store in order to make fitting multiple time series models more straightforward.

```{r test_train}
# Split in to train and test
train <- daily_demand %>% 
        filter(date <= max(date) - days(14)) %>% 
        group_by(store, store_state, store_city) %>% 
        nest(.key = ts_data)

# Last 2 weeks = test
test <- daily_demand %>% 
        filter(date > max(date) - days(14)) %>% 
        group_by(store, store_state, store_city) %>% 
        nest(.key = ts_data)
```

The lettuce daily demand data for each store was converted in to an `R` `timeseries` object in order to fit the forecasting models.

```{r make_ts}
# First convert the time-series data in to actual R timeseries objects
# Daily deman is modelled with frequency = 7 as noted here http://robjhyndman.com/hyndsight/dailydata/
# By R. Hyndman (author of forecast!)
make_ts <- function(df) {
  series <- ts(df$lettuce, frequency = 7)
  return(series)
}

# Use some functional programming on the nested data to convert it to a timeseries
train <- train %>% 
         mutate(ts_ts = ts_data %>% map(make_ts))

test <- test %>% 
        mutate(ts_ts = ts_data %>% map(make_ts))
```

# Forecasting methods 

## Holt-Winters model {.tabset}

Exponential smoothing state space models (Hyndman et. al., 2002) were applied to the time series using the `ets` function. Despite the absense of obvious trends in the exploratory visualisations (figure one), the function was applied in a fully automated manner. This was done in order for potential trends to be identified by the model, should they exist in the data. 

```{r fit_ets}
# Function to fit the ets model to time series data with no trend
fit_ets <- function(ts_data) {
  ets(ts_data, model = "ZZZ")
}

# Apply the model to each store's timeseries
train <- train %>% 
          mutate(ets = ts_ts %>% map(fit_ets))
```

```{r make_plots, include = FALSE}
plot_ets <- function(data, row) {
  ets <- data$ets[[row]]
  store <- data %>% slice(row) %>% select(store, store_city) %>% paste(collapse = ", ")
  method <- ets$method
  title <- paste("Decomposition by", method, "for", store, sep = " ")
  state <- data %>% slice(row) %>% select(store_state)
  
  col <- ifelse(state == "California", "#1b9e77", "#d95f02")
  col <- rep(col, ets$x %>% length())
  
  autoplot(ets) + 
    scale_y_continuous(labels = scales::comma) +
    theme_jim + 
    ggtitle(title) +
    theme(title = element_text(size = 14))
}
```

The estimates from the ETS models were then used to specify a complete Holt-Winters model, using the `HoltWinters` function. In this way trends (and an estimation of the seasonal component) of each series detected by `ets` are passed to `HoltWinters` for assessment.

```{r fit_hw}
# Define function to run HW using output from the ets estimations
fit_hw <- function(ts_data, ets_model) {
  # Extract components from ets model
  l <- ets_model$par["l"]
  if (is.na(l)) { l <- NULL }
  
  b <- ets_model$par["b"]
  if (is.na(b)) { b <- NULL }
  
  alpha <- ets_model$par["alpha"]
  alpha_l <- ifelse(is.na(alpha), FALSE, TRUE) # Fit with alpha?
  if (is.na(alpha)) { alpha <- 0.3} # Set default value if not viable
  
  beta <- ets_model$par["beta"]
  beta_l <- ifelse(is.na(beta), FALSE, TRUE) # Fit with beta?
  if (is.na(beta)) { beta <- 0.1}
  
  gamma <- ets_model$par["gamma"]
  gamma_l <- ifelse(is.na(gamma), FALSE, TRUE) # Fit with gamma?
  if (is.na(gamma)) { gamma <- 0.1}
  
  seasonality <- ets_model$components[3]
  seasonality <- ifelse(seasonality == "A", "additive", 
                        ifelse(seasonality == "N", "additive", 
                               "multiplicative"))
  
  # Set logical alpha/beta/gamma parametes
  
  
  hw <- HoltWinters(ts_data, alpha = alpha_l, beta = beta_l, gamma = gamma_l,
                    seasonal = seasonality,
                    start.periods = 2,
                    l.start = l,
                    b.start = b,
                    optim.start = c(alpha = alpha,
                                    beta = beta,
                                    gamma = gamma))
  return(hw)
}  

# Apply the function to fit the HW models
train <- train %>% 
          mutate(hw = ts_ts %>% map2(ets, fit_hw))

```

The estimation of the components (i.e. level, trend and seasonal), where applicable, along with the fitted values are displayed in the figures below, one for each model (i.e. for each of the four stores). For neither of the Californian stores was a trend detected, whereas both New York stores have a trend that varies over the series. Each figure also displays the raw demand data in the first panel. The fitted values show a please correspondence with the raw data.

Further details on the merits of the Holt-Winters model are discussed in the model comparison section.

```{r plot_hw}
# Create function to plot HW fitted model, using store and city name for labels
plot_hw <- function(store, city, hw_object, ts) {
  # Create data from fitted part of the model - fortify for ggplot2 plotting
  data <- hw_object$fitted %>% fortify.zoo()
  
  ts_data <- ts %>% fortify.zoo() %>% setNames(c("Index", "raw"))
  
  data <- data %>% left_join(ts_data)
  
  # Set tidier column names
  colnames(data) <- colnames(data) %>% toproper() %>% gsub("Xhat", "Fitted", .)
  
  idx <- seq(ymd(min(daily_demand$date)), ymd(max(daily_demand$date)), length.out = nrow(data))
  data$Index <- idx
  
  # Create plotting data in format that ggplot2 likes
  data <- data %>% 
          gather(field, value, -Index) %>% 
          mutate(field = factor(field, levels = c("Raw", "Fitted", "Level", "Season", "Trend"))) %>% 
          na.omit()
 
  # Create title for the plot
  title <- paste0("Holt-Winters model estimation for store ", store, " (", city, ")")
    
  # Make the plot
  data %>% 
  ggplot(aes(x = Index, y = value, group = field, colour = field)) + 
    geom_line(size = 1.5, aes(colour = field)) +
    scale_colour_brewer(palette = "Set1") +
    facet_grid(field ~ ., scales = "free_y") + 
    scale_y_continuous(labels = scales::comma) +
    guides(colour = guide_legend(title = "")) + 
    xlab("Time") +
    ylab("Value") +
    ggtitle(title) +
    theme_jim +
    theme(title = element_text(size = 14),
          axis.text.y = element_text(size = 12))
} 

# Apply function to the data
train <- train %>% 
          mutate(hw_plot = pmap(list(store, store_city, hw, ts_ts), plot_hw))
```

```{r get_store_name, include = FALSE}
get_store_name <- function(row) {
  name <- train %>% slice(row) %>% select(store, store_city) %>% paste(collapse = "-")
  storename <- paste("Store", name)
  return(name)
}
```

### `r get_store_name(1)`

```{r display_hw_1, fig.cap = "Figure 2: Holt-Winters model estimates and fitted values", fig.align="center"}
train$hw_plot[[1]]
```

### `r get_store_name(2)`

```{r display_hw_2, fig.cap = "Figure 3: Holt-Winters model estimates and fitted values", fig.align="center"}
train$hw_plot[[2]]
```

### `r get_store_name(3)`

```{r display_hw_3, fig.cap = "Figure 4: Holt-Winters model estimates and fitted values", fig.align="center"}
train$hw_plot[[3]]
```

### `r get_store_name(4)`

```{r display_hw_4, fig.cap = "Figure 5: Holt-Winters model estimates and fitted values", fig.align="center"}
train$hw_plot[[4]]
```

## ARIMA model {.tabset}

Firstly, the log-transform of each time-series was performed in order to ensure that each had a constant variance.

```{r log_transform}
# Log-transform each series
train <- train %>% 
  mutate(l_ts = ts_ts %>% map(log))
```

The appropriate number of differences required for each series to be made stationary were then found using the `ndiffs` function. A similar approach was taken to find the number of _seasonal_ differences required, using the `nsdiffs` function. The results from these tests are displayed in table TODO below. KPSS, ADF, and PP refer to the KPSS, Augmented Dickey-Fuller and Phillips-Perron tests for simple differencing requirements. CH and OCSB refer to Cenova-Hansen and Osborn-Chui-Smith-Birchenhall tests for determining seasonal difference requirements.

```{r find_diffs}
train <- train %>% 
  mutate(# Determine difference reqs
         diffs_kpss = l_ts %>% map_dbl(ndiffs, alpha = 0.01, test = "kpss"),
         diffs_adf = l_ts %>% map_dbl(ndiffs, alpha = 0.01, test = "adf"),
         diffs_pp = l_ts %>% map_dbl(ndiffs, alpha = 0.01, test = "pp"),
         # Determine seasonal difference reqs
         sdiffs_ch = l_ts %>% map_dbl(nsdiffs, test = "ch", max.D = 2),
         sdiffs_ocsb = l_ts %>% map_dbl(nsdiffs, test = "ocsb", max.D = 2))

train %>% 
  select(store, store_city, diffs_kpss, diffs_adf, diffs_pp, sdiffs_ch, sdiffs_ocsb) %>% 
  kable(col.names = c("Store", "City", "KPSS", "ADF", "PP", "CH", "OCSB"),
        caption =  "Table TODO: Difference and seasonal difference requirements for each store's series")
```

Given that the KPSS and ADF methods indicated that no differencing was required for any of the four series and only one series (Elmhurst) is indicated to require differencing by the PP test, no differencing was performed in order to make the series stationary. 

However, given the strong pattern of seasonality observed in the Californian stores (figure one), the results from the OCSB method of detecting seasonal differenece requirements were used to perform _seasonal_ differencing.

The `auto.arima` function was used to fit an ARIMA model for each of the stores, using the differencing values identified above.

```{r diff_ts}
diff_ts_and_fit <- function(ts, diff) {
fitted <- auto.arima(ts, d = 0, D = diff)
  return(fitted)
}

train <- train %>%
  mutate(arima = map2(l_ts, sdiffs_ocsb, diff_ts_and_fit),
         arima_fitted = arima %>% map(forecast::fitted.Arima))
```

The raw data were then be compared to the fitted values from each ARIMA model to give an understanding of how well the models fit the data. In order to make comparisons, the exponential of the log-transformed values (returned by the `auto.arima` `fitted` method) from the models was taken.

These comparisons were visualised and are presented in the figures below.

```{r create_plot_arima}
plot_arima <- function(store, city, fitted, raw) {
# Set up objects to create plotting data
  fitted <- fitted %>% as.numeric() %>% exp()
  raw <- raw %>% as.numeric() %>% exp()
  idx <- idx <- seq(ymd(min(daily_demand$date)), ymd(max(daily_demand$date)), length.out = length(fitted))
 
 # Create plotting dataframe
  data <- data_frame(fitted = fitted, raw = raw, idx = idx) %>% 
          gather(key, value, -idx) %>% 
          mutate(key = ifelse(key == "fitted", "Fitted", "Raw"),
                 key = factor(key, levels = c("Raw", "Fitted")))
  
  # Create title for the plot
  title <- paste0("ARIMA model estimation for store ", store, " (", city, ")")
 
 data %>% 
   ggplot(aes(x = idx, y = value, colour = key, group = key)) +
   geom_line(size = 1.5) +
   scale_colour_brewer(palette = "Set1") +
   scale_y_continuous(labels = scales::comma) +
   guides(colour = guide_legend(title = "")) + 
   xlab("Time") +
   ylab("Value") +
   ggtitle(title) +
   theme_jim +
   theme(title = element_text(size = 14),
         axis.text.y = element_text(size = 12))
}  

train <- train %>% 
  mutate(arima_plot = pmap(list(store, store_city, arima_fitted, l_ts), plot_arima))

```

The ARIMA models show nice overlap with the raw demand data, with slightly better results for the two Berkley (CA) stores. Store 12631 (Ridgewood, NY) shows the estimated demand differ strongly in the ARIMA-fitted and real values. 

Further details on the merits of the ARIMA models are discussed in the model comparison section.

### `r get_store_name(1)`

```{r display_arima_1, fig.cap = "Figure 6: ARIMA model fitted values compared with raw demand.", fig.align="center"}
train$arima_plot[[1]]
```

### `r get_store_name(2)`

```{r display_arima_2, fig.cap = "Figure 7: ARIMA model fitted values compared with raw demand.", fig.align="center"}
train$arima_plot[[2]]
```

### `r get_store_name(3)`

```{r display_arima_3, fig.cap = "Figure 8: ARIMA model fitted values compared with raw demand.", fig.align="center"}
train$arima_plot[[3]]
```

### `r get_store_name(4)`

```{r display_arima_4, fig.cap = "Figure 9: ARIMA model fitted values compared with raw demand.", fig.align="center"}
train$arima_plot[[4]]
```


# Model diagnosis

There are two methods that can be applied when diagnosing the two modelling approaches. The first approach considers the models in isolation, and looks at in-sample measures of fit to estimate model quality. The second approach is to forecast using each model and compare the results with the _test_ data, summarising differences using common error metrics (such as mean-squared error, or mean absolute deviation). Both approaches have been taken and are discussed below.

## Holt-Winters



## ARIMA

## Test errors


