---
title: "Machine Learning Assignment One"
author: "Jim Leach"
date: "4 March 2016"
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
---

```{r loadPackages, echo = FALSE, message=FALSE,warning=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)

# Create some simple functions
normalise <- function(x){(x-mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE)}
range_single <- function(x){range(x)[2] - range(x)[1]}
to_proper <- function(x){paste0(toupper(substring(x, 1, 1)), substring(x, 2))}
```

# Introduction

Classification is a common task in a supervised machine learning setting. It involves understanding how a function of some input data $f(X)$ maps to a discrete (often binary) outcome, $Y$. In this exercise, the input data came from a clinical setting and related to bowel cancer. 

Features in the data corresponded to clinical measurements of potentially cancerous cellular forms found in the body. Examples include the radius of such forms, their smoothness, and measures of their symmetry. Data were also provided that detailed, for each data point, whether that point was malign (i.e. cancerous) or benign, as diagnosed by clinical experts.

Each feature in the input data (i.e. that data describing the cellular form) was comprised of numerical values.The output data (i.e. the clinical diagnosis) was mapped to the values of 0 (malign) or 1 (benign).

The objective was to develop a method which could help in the diagnosis of bowel cancer from similar clinical data where clinical diagnoses were not present/provided. Two alternative methods have been developed, and their successes reported.

## This document

This document is broken down in to three main sections. The first covers an overview of the input data, the second describes the process of developing each classification method, and the third presents the results obtained used both methods.

```{r getdata, echo = FALSE}
input <- read_csv("../data/created/input.csv", col_names = FALSE)
output <- read_csv("../data/created/output.csv", col_names = FALSE) %>% 
          setNames("label")
data <- bind_cols(input, output) 

knn <- read_csv("../data/created/knn_loss.csv") %>% 
        mutate(k = row_number())

svm <- read_csv("../data/created/svm_loss.csv") %>% 
  mutate(slack = 1:11)

```

```{r transformData, echo = FALSE}
knn_long <- knn %>% gather(key = "distance", value = "loss", -k) %>% 
                    mutate(distance = to_proper(distance))

svm_long <- svm %>% gather(key = "kernel", value = "loss", -slack) %>% 
            mutate(kernel = to_proper(kernel))

data_long <- data %>% gather(key = "field", value = "value", -label) %>% 
              group_by(field) %>% 
              mutate(value_norm = normalise(value)) %>% 
              ungroup() %>% 
              gather(key = "feature", value = "value", -label, -field) %>% 
              arrange(feature, field)
```

```{r setTheme, echo = FALSE}
theme <-   theme(legend.position = "bottom",
                 axis.text.y = element_text(size = 16, colour = "black"),
                 axis.text.x = element_text(size = 16, colour = "black", angle = -90),
                 legend.text = element_text(size = 16),
                 legend.title = element_text(size = 16),
                 title = element_text(size = 16),
                 panel.grid.minor.x = element_blank(),
                 panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
                 panel.grid.minor.y = element_blank(),
                 panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
                 panel.margin.y = unit(0.1, units = "in"),
                 panel.background = element_rect(fill = "white", colour = "lightgrey"),
                 panel.border = element_rect(colour = "black", fill = NA))
```


# Section 1 - The data

Before applying any machine learning algorithms, the data were examined and explored. 

To begin with, the balance of the two classes in the data was viewed to check if one class was much more dominant than the other, as seen in figure 1 below.

```{r class_freq_plot, echo = FALSE, fig.height = 4, fig.cap="Count of outcome classification labels for the data provided. The two values are 0 (malign) and 1 (benign)."}
ggplot(output, aes(x = as.factor(label))) +
  geom_bar(aes(fill = as.factor(label))) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  xlab("Output Label") +
  ylab("Count (records in data)") +
  ggtitle("Frequency plot of output data labels") +
  theme(legend.position = "none",
        axis.text.y = element_text(size = 16, colour = "black"),
        axis.text.x = element_text(size = 16, colour = "black"),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        title = element_text(size = 16),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
        panel.margin.y = unit(0.1, units = "in"),
        panel.background = element_rect(fill = "white", colour = "lightgrey"),
        panel.border = element_rect(colour = "black", fill = NA))  
```

It was decided that the two class labels were not so imbalanced as to pose a problem when creating the classifiers.

Additionally, the distribution of each input feature was viewed. This was performed as features with extremely large or small value ranges might have a very large influence on certain classification algorithms. Each non-normalised feature's distribution was plotted and is shown in figure 2 below.


```{r dataDist, echo = FALSE, fig.cap = "Non-normalised feature disribution, coloured by outcome label (i.e. malign or benign)."}
data_long %>% 
  arrange(feature, field) %>% 
  filter(feature == "value") %>% 
  ggplot(aes(x = field, y = value)) +
  geom_point(aes(colour = factor(label)), size = 2, alpha = .75) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  guides(colour = guide_legend(title = "Output Label")) +
  xlab("Input Feature") +
  ylab("Feature Values") +
  #ggtitle("Distribution of values for all non-normalised\n data input features") +
  theme
```

Due to the differences in range for several variables observed in figure 2 (e.g. feature 24 and feature 4) it was decided that all features should be normalised in to $Z$-scores using \eqref{eq:z}.

\begin{equation} \label{eq:z}
z = \dfrac{x - \mu_x}{\sigma_x}
\end{equation}

```{r dataDistNorm, echo = FALSE, fig.cap = "Normalised feature disribution, coloured by outcome label (i.e. malign or benign)."}
data_long %>% 
  arrange(feature, field) %>% 
  filter(feature == "value_norm") %>% 
  ggplot(aes(x = field, y = value)) +
  geom_point(aes(colour = factor(label)), size = 2, alpha = .75) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  guides(colour = guide_legend(title = "Output Label")) +
  xlab("Input Feature") +
  ylab("Feature Values (normalised)") +
  #ggtitle("Distribution of values for all normalised\n data input features") +
  theme
```

# Section 2 - Training Classifiers

## Classifier One - Support Vector Machine

### Parameter requirements

### Model selection

### Training the model

```{r svm, echo = FALSE, fig.cap = "Cross Validated Loss Estimates for a range of hyperparameters tested during training a support vector machine on the training data"}
xlabels <- c("0.00001", "0.0001", "0.001", "0.01", "0.1", "1", "10", "100", "1,000", "10,000", "100,000")

svm_long %>% 
  ggplot(aes(x = slack, y = loss, colour = kernel)) + 
  geom_line(aes(colour = kernel), size = 1.25) +
  geom_point(aes(colour = kernel), size = 2.75) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  guides(colour = guide_legend(title = "Kernel Function")) +
  ylab("Loss") + 
  xlab("Slack Parameter Value") +
  ggtitle("Average 10-fold cross-validated loss\n for a range of hyperparameters") +
  scale_x_discrete(labels = xlabels) +
  theme(legend.position = "bottom",
        axis.text.y = element_text(size = 14, colour = "black"),
        axis.text.x = element_text(size = 14, colour = "black", angle = -90),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        title = element_text(size = 16),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
        panel.margin.y = unit(0.1, units = "in"),
        panel.background = element_rect(fill = "white", colour = "lightgrey"),
        panel.border = element_rect(colour = "black", fill = NA))
```

## Classifier Two - k-Nearest Neighbours

### Parameter requirements

### Model selection

### Training the model

```{r knn, echo = FALSE}
knn_long %>%
  ggplot(aes(x = k, y = loss, colour = distance)) + 
  geom_line(aes(colour = distance), size = 1.25) +
  geom_point(aes(colour = distance), size = 2.75) +
  scale_x_continuous(breaks = seq(from = 0, to = 30, by = 2)) + 
  scale_y_continuous(breaks = seq(from = 0, to = max(knn_long$loss), by = 0.01)) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  xlab("k (in k-Nearest Neighbours)") +
  ylab("Loss") +
  ggtitle("Average 10-fold cross-validated loss\n for a range of hyperparameters") +
  guides(colour = guide_legend(title = "Distance Metric")) +
  theme(legend.position = "bottom",
        axis.text.y = element_text(size = 16, colour = "black"),
        axis.text.x = element_text(size = 16, colour = "black"),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        title = element_text(size = 16),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
        panel.margin.y = unit(0.1, units = "in"),
        panel.background = element_rect(fill = "white", colour = "lightgrey"),
        panel.border = element_rect(colour = "black", fill = NA))
```

# Section Three - Testing Classifiers

## Support Vector Machine

## k-Nearest Neighbours

## Comparison and discussion

