---
title: "Machine Learning Assignment One"
author: "Jim Leach"
date: "4 March 2016"
output: 
  pdf_document:
    fig_caption: yes
    keep_tex: yes
---

```{r loadPackages, echo = FALSE, message=FALSE,warning=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(knitr)

# Create some simple functions
normalise <- function(x){(x-mean(x, na.rm = TRUE))/sd(x, na.rm = TRUE)}
range_single <- function(x){range(x)[2] - range(x)[1]}
to_proper <- function(x){paste0(toupper(substring(x, 1, 1)), substring(x, 2))}
```

# Introduction

Classification is a common task in a supervised machine learning setting. It involves understanding how a function of some input data $f(X)$ maps to a discrete (often binary) outcome, $Y$. In this exercise, the input data came from a clinical setting and related to bowel cancer. 

Features in the data corresponded to clinical measurements of potentially cancerous cellular forms found in the body. Examples include the radius of such forms, their smoothness, and measures of their symmetry. Data were also provided that detailed, for each data point, whether that point was malign (i.e. cancerous) or benign, as diagnosed by clinical experts.

Each feature in the input data (i.e. that data describing the cellular form) was comprised of numerical values.The output data (i.e. the clinical diagnosis) was mapped to the values of 0 (malign) or 1 (benign).

The objective was to develop a method which could help in the diagnosis of bowel cancer from similar clinical data where clinical diagnoses were not present/provided. Two alternative methods have been developed, and their successes reported.

## This document

This document is broken down in to three main sections. The first covers an overview of the input data, the second describes the process of developing each classification method, and the third presents the results obtained used both methods.

```{r getdata, echo = FALSE}
input <- read_csv("../data/created/input.csv", col_names = FALSE)
output <- read_csv("../data/created/output.csv", col_names = FALSE) %>% 
          setNames("label")
data <- bind_cols(input, output) 

knn <- read_csv("../data/created/knn_loss.csv") %>% 
        mutate(k = row_number())

svm <- read_csv("../data/created/svm_loss.csv") %>% 
  mutate(slack = 1:11)

```

```{r transformData, echo = FALSE}
knn_long <- knn %>% gather(key = "distance", value = "loss", -k) %>% 
                    mutate(distance = to_proper(distance))

svm_long <- svm %>% gather(key = "kernel", value = "loss", -slack) %>% 
            mutate(kernel = to_proper(kernel))

data_long <- data %>% gather(key = "field", value = "value", -label) %>% 
              group_by(field) %>% 
              mutate(value_norm = normalise(value)) %>% 
              ungroup() %>% 
              gather(key = "feature", value = "value", -label, -field) %>% 
              arrange(feature, field)
```

```{r setTheme, echo = FALSE}
theme <-   theme(legend.position = "bottom",
                 axis.text.y = element_text(size = 16, colour = "black"),
                 axis.text.x = element_text(size = 16, colour = "black", angle = -90),
                 legend.text = element_text(size = 16),
                 legend.title = element_text(size = 16),
                 title = element_text(size = 16),
                 panel.grid.minor.x = element_blank(),
                 panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
                 panel.grid.minor.y = element_blank(),
                 panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
                 panel.margin.y = unit(0.1, units = "in"),
                 panel.background = element_rect(fill = "white", colour = "lightgrey"),
                 panel.border = element_rect(colour = "black", fill = NA))
```


# Section 1 - The data

Before applying any machine learning algorithms, the data were examined and explored. 

To begin with, the balance of the two classes in the data was viewed to check if one class was much more dominant than the other, as seen in figure 1.

```{r class_freq_plot, echo = FALSE, fig.height = 4, fig.cap="Count of outcome classification labels for the data provided. The two values are 0 (malign) and 1 (benign).", fig.height = 3.5}
ggplot(output, aes(x = as.factor(label))) +
  geom_bar(aes(fill = as.factor(label))) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  xlab("Output Label") +
  ylab("Count (records in data)") +
  ##ggtitle("Frequency plot of output data labels") +
  theme(legend.position = "none",
        axis.text.y = element_text(size = 16, colour = "black"),
        axis.text.x = element_text(size = 16, colour = "black"),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        title = element_text(size = 16),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
        panel.margin.y = unit(0.1, units = "in"),
        panel.background = element_rect(fill = "white", colour = "lightgrey"),
        panel.border = element_rect(colour = "black", fill = NA))  
```

It was decided that the two class labels were not so imbalanced as to pose a problem when creating the classifiers.

Additionally, the distribution of each input feature was viewed. This was performed as features with extremely large or small value ranges might have a very large influence on certain classification algorithms (e.g. $k$-Nearest Neigbours). Each non-normalised feature's distribution was plotted and is shown in figure 2.

```{r dataDist, echo = FALSE, fig.cap = "Non-normalised feature disribution, coloured by outcome label (i.e. malign or benign).", fig.height = 3.5}
data_long %>% 
  arrange(feature, field) %>% 
  filter(feature == "value") %>% 
  ggplot(aes(x = field, y = value)) +
  geom_point(aes(colour = factor(label)), size = 2, alpha = .75) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  guides(colour = guide_legend(title = "Output Label")) +
  xlab("Input Feature") +
  ylab("Feature Values") +
  #ggtitle("Distribution of values for all non-normalised\n data input features") +
  theme
```

Due to the differences in range for several variables observed in figure 2 (e.g. feature 24 and feature 4) it was decided that all features should be normalised in to $Z$-scores using \eqref{eq:z}.

\begin{equation} \label{eq:z}
z = \dfrac{x - \mu_x}{\sigma_x}
\end{equation}

After performing this normalisation, the distributions of values for each feature were more similar, as shown in figure 3.

```{r dataDistNorm, echo = FALSE, fig.cap = "Normalised feature disribution, coloured by outcome label (i.e. malign or benign).", fig.height = 3.5}
data_long %>% 
  arrange(feature, field) %>% 
  filter(feature == "value_norm") %>% 
  ggplot(aes(x = field, y = value)) +
  geom_point(aes(colour = factor(label)), size = 2, alpha = .75) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  guides(colour = guide_legend(title = "Output Label")) +
  xlab("Input Feature") +
  ylab("Feature Values (normalised)") +
  #ggtitle("Distribution of values for all normalised\n data input features") +
  theme
```

# Section 2 - Training Classifiers

## Classifier One - Support Vector Machine

The first classifier developed was a Support Vector Machines (SVM). A SVM was chosen as they are typically well-performing classifiers, and can model both linear and non-linear decision boundaries flexibly.

### Parameter requirements

In general, a support vector machine (with a _linear_ kernel) is seeking to find parameters, $w$, for each feature in the data via the generic optimisation presented in \eqref{eq:svm} (Weisemann, 2015).

\begin{equation} \label{eq:svm}
\min w^Tx
\end{equation}

In this instance, it can be seen that the parameter requirements will be one $w$ for each feature, plus $w_0$ (the intercept). Given that there were thirty features in the input data, this translates in to thirty one parameters required for the SVM. 

However, in reality a SVM optimisation is solved via the dual solution to the generic problem in \eqref{eq:svm}. When solving this dual problem it is the case that the only values that need to be retained once optimality has been reached are the support vectors themselves, and their corresponding coefficients (i.e. dual variables). Whilst the support vectors are not truly "parameters" (in the same sense as the $w$ values), for the purposes of this question they have been treated as such. 

Hence it is not possible to state _a priori_ the exact number of "parameters" required by a SVM. A general rule of thumb (given that the support vectors need to be retained) is that the "parameter" requirements are: the number of support vectors multiplied by the number of features in the input data, plus a dual variable for each support vector. 

Additionally, if a _non-linear_ kernel is chosen then there are potentially an infinite number of dimensions of the data. However, it is the case that the "parameters" are still the support vectors and their dual variables, and so the rule of thumb outlined above will still hold.

Other properties, e.g. the kernel to be used or the slack variable, are termed _hyperparameters_ and so are not included in this calculation. As the number of training points for each class was not radically different (figure 1), there was no need to account for such differences.


### Model selection

The optimal model for the SVM was selected via training the model using a range of hyperparameters. For each combination of hyperparameters used, 10-fold cross validation was used to give an estimate of the out-of-sample error (termed "loss"). Different kernels were tested, and the slack variable (i.e. the sensitivity/tolerance of the SVM to misclassification) was varied in line with recommendations on the `MATLAB` [help](http://uk.mathworks.com/help/stats/support-vector-machines-for-binary-classification.html#bsr5o3f) pages. Note that data were standardised as discussed above.

Figure 4 shows the average cross validated loss found for each combination of hyperparameters tested.
As seen in the figure, in this instance linear and polynomial kernels consistently performed better than Gaussian with optimal conditions being found for a linear kernel with a slack value of 100.

```{r svm, echo = FALSE, fig.cap = "Cross-validated loss (out-of-sample error) estimates for a range of hyperparameters tested when training a support vector machine. Note that these results come from training the SVM on only the training data, i.e. 50% of the data provided.", fig.height = 4.5}
xlabels <- c("0.00001", "0.0001", "0.001", "0.01", "0.1", "1", "10", "100", "1,000", "10,000", "100,000")

svm_long %>% 
  ggplot(aes(x = slack, y = loss, colour = kernel)) + 
  geom_line(aes(colour = kernel), size = 1.25) +
  geom_point(aes(colour = kernel), size = 2.75) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  guides(colour = guide_legend(title = "Kernel Function")) +
  ylab("Loss") + 
  xlab("Slack Parameter Value") +
  #ggtitle("Average 10-fold cross-validated loss\n for a range of hyperparameters") +
  scale_x_discrete(labels = xlabels) +
  theme(legend.position = "bottom",
        axis.text.y = element_text(size = 14, colour = "black"),
        axis.text.x = element_text(size = 14, colour = "black", angle = -90),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        title = element_text(size = 16),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
        panel.margin.y = unit(0.1, units = "in"),
        panel.background = element_rect(fill = "white", colour = "lightgrey"),
        panel.border = element_rect(colour = "black", fill = NA))
```

### Training the final model

The combination of hyperparameters found to have the lowest cross-validated error was chosen to be the "final" version of the SVM model using a "black box approach". That is, the optimal hyperparameters were chosen automatically by the `TrainClassifierX` function. (The results presented in figure 4 are however representative of the model selection when performed manually).

## Classifier Two - $k$-Nearest Neighbours

### Parameter requirements

The $k$-Nearest Neighbours algorithm is actually a non-parametric method. However, in order for it to function as a classifier there is information that must be retained once the model has been trained. This information is, in fact, the training data itself. For a new data point, $x_0$, the class is predicted using \eqref{eq:knn} (James, Witten, Hastie, Tibshirani, 2013) and hence it is seen that the whole training data are required to be kept.

\begin{equation} \label{eq:knn}
Pr(Y=j|X=x_0) = \dfrac{1}{K}\sum_{i\in\mathcal{N}_0}I(y_i = j)
\end{equation}

This means that (assuming _all_ of the provided data are used to train the classifier) for the input data with $`r nrow(input)`$ rows and $`r ncol(input)`$ columns, there is a requirement to keep $`r prod(dim(input))`$ points of information from the input data. The class of each input data point is required, leading to an additional $`r nrow(output)`$ points of data, for a total of $`r prod(dim(input))+nrow(output)`$ pieces of data that must be retained for the classifier to work.

Other properties, e.g. the distance metric to be used or the value of $k$, are termed _hyperparameters_ and so are not included in this calculation. As the number of training points for each class was not radically different (figure 1), there was no need to account for such differences.

### Model selection

Similar to the SVM, the optimal model for $k$-Nearest Neighbours was selected via training the model using a range of hyperparameters. Again, 10-fold cross validation was used to give an estimate of the out-of-sample error. Different distance metrics were used, and $k$ was varied from 0 to 30. Note that data were standardised as discussed above.

Figure 5 shows the average loss found for each combination of hyperparameters with 10-fold cross validation.
As seen in the figure, in this instance Cityblock, Euclidean and Minkowski distance metrics consistently outperformed Chebychev. The optimal value for $k$ was around 3 using the Cityblock distance metric.

```{r knn, echo = FALSE, fig.cap = "Cross-validated loss (out-of-sample error) estimates for a range of hyperparameters tested when training k-Nearest Neighbours on the data. Note that these results come from training the k-NN classifier on only the training data, i.e. 50% of the data provided.", fig.height = 4}
knn_long %>%
  ggplot(aes(x = k, y = loss, colour = distance)) + 
  geom_line(aes(colour = distance), size = 1.25) +
  geom_point(aes(colour = distance), size = 2.75) +
  scale_x_continuous(breaks = seq(from = 0, to = 30, by = 2)) + 
  scale_y_continuous(breaks = seq(from = 0, to = max(knn_long$loss), by = 0.01)) +
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  xlab("k (in k-Nearest Neighbours)") +
  ylab("Loss") +
  #ggtitle("Average 10-fold cross-validated loss\n for a range of hyperparameters") +
  guides(colour = guide_legend(title = "Distance Metric", 
                               nrow=2,
                               byrow=TRUE)) +
  theme(legend.position = "bottom",
        axis.text.y = element_text(size = 16, colour = "black"),
        axis.text.x = element_text(size = 16, colour = "black"),
        legend.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        title = element_text(size = 16),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
        panel.margin.y = unit(0.1, units = "in"),
        panel.background = element_rect(fill = "white", colour = "lightgrey"),
        panel.border = element_rect(colour = "black", fill = NA))
```

### Training the final model

Also as with the SVM, the exact combination of hyperparameters found to have the lowest cross-validated error was chosen to be the "final" version of the model using a "black box approach". That is, the optimal hyperparameters were chosen automatically by the `TrainClassifierX` function. (The results presented in figure 5 are however representative of the model selection when performed manually).

***

# Section Three - Testing Classifiers

In order to test each classifier, the provided data were split evenly in to a training and test set. The SVM and $k$-Nearest Neighbours classifiers were trained as described in the previous sections and the test set used once (and once only) to evaluate their true out-of-sample performance. 

## Support Vector Machine

```{r svm_conf, echo = FALSE}
svm_conf <- read_csv("../data/created/conf_svm.csv", col_names = FALSE)
svm_names <- read_csv("../data/created/conf_svm_names.csv", col_names = FALSE)
colnames(svm_conf) <- as.character(svm_names$X1)
rownames(svm_conf) <- as.character(svm_names$X1)
# kable(svm_conf)
accuracy_svm <- sum(diag(svm_conf %>% as.matrix())) / svm_conf %>% as.matrix() %>% sum()
```

The SVM classifier was able to successfully predict $`r round(100*accuracy_svm, 2)`$% of the samples in the test set. The confusion matrix for the SVM classifier is reported below.

```{r svm_confs_show, echo = FALSE}
kable(svm_conf, caption = "Confusion Matrix for SVM Classifier on test data set.")
```

## $k$-Nearest Neighbours

```{r knn_conf, echo = FALSE}
knn_conf <- read_csv("../data/created/conf_knn.csv", col_names = FALSE)
knn_names <- read_csv("../data/created/conf_knn_names.csv", col_names = FALSE)
colnames(knn_conf) <- as.character(knn_names$X1)
rownames(knn_conf) <- as.character(knn_names$X1)
# kable(svm_conf)
accuracy_knn <- sum(diag(knn_conf %>% as.matrix())) / knn_conf %>% as.matrix() %>% sum()
```

The SVM classifier was able to successfully predict $`r round(100*accuracy_knn, 2)`$% of the samples in the test set. The confusion matrix for the $k$-NN classifier is reported below.

```{r knn_confs_show, echo = FALSE}
kable(knn_conf, caption = "Confusion Matrix for k-NN Classifier on test data set.")
```

## Comparison and discussion

As can be seen from the results reported above, both classifiers have achieved a high accuracy on the test set. As such, it is difficult to single one out as a "better" classifier in this setting. The SVM is known to be a flexible method which generalises well (James, Witten, Hastie, Tibshirani, 2013, figure on page 25). Moreover, given that they generally have a small number-of-parameters requirement (certainly in contrast to $k$-NN) they may be a better choice in this setting, given the potential size of the data that might be collected (i.e very many patients and very many features for each patient).

# References

1. Wolfram Weisemann (2015), _Optimisation and Decision Models, [Lecture] MSc. Business Analytics Imperial College London, December 2015.   
2. James, Witten, Hastie, Tibshirani (2013), _An Introduction to Statistical Learning_, Springer Texts in Statistics. Springer New York, Springer.