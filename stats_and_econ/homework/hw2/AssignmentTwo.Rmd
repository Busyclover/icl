---
title: "Statistics and Econometrics - Assignment Two"
author: "Jim Leach"
date: "22 October 2015"
output: pdf_document
---

# Document purpose

The purpose of this document is to write up the response to the second assignment given as part of the _Statistics and Econometrics_ module. It is divided in to two sections, each covering a section of the assignment as given. Each major section is divided in to further subsections, one for each question in the assignment.

A number of `R` packages have been used throughout this assignment. These are loaded before the responses are given:

```{r load_pkg, echo = TRUE, message = FALSE}
library(dplyr)
library(magrittr)
library(broom)
library(knitr)
```

***

# Section one - ZIP Code Pricing Disparity

The data for this section are ZIP-code level prices for a number of items at fast food restaurants. The data also contain characteristics of each ZIP-code population. The ZIP-codes are from New Jersey and Pennsylvania.  The aim is to understand if fast-food restaurants charge higher prices in areas with a higher proportion of black residents.

## Loading the data

The data are loaded from the provided `.RData` file.

```{r s1_load}
load("./data/discrim.RData")
```


## 1a

The model to be developed uses the median income and the proportion of black residents to explain the median price of soda, with the population regression function given by \eqref{eq:prf1}.

\begin{equation} \label{eq:prf1}
log(psoda) = \beta_0 + \beta_1prpblck + \beta_2log(income) + u
\end{equation}

The proportion of black residents is given as a decimal proportion. In order to simplify the interpretation of the regression parameters, this is multiplied by 100 to give this as a percentage before the model is estimated.

This model is estimated and the results reported below.

```{r 1a}
data %<>% mutate(prpblck_cln = 100 * prpblck)
fit1a <- lm(log(psoda) ~ prpblck_cln + log(income), data = data)
fit1a_summary <- fit1a %>% summary
tidy_coef1a <- fit1a %>% tidy
ss1a <- data %>% nrow
r2_1a <- fit1a_summary$r.squared
kable(tidy_coef1a, caption = "Coefficient estimates from regression of soda prices on the proportion of black residents, and median income. Note that 'prpblck_cln' is the proportion of black residents expressed as a percentage.")
```

In addition to the parameter estimates given above, the model has an $R^2$ value of `r round(r2_1a, 4)` on a sample size of `r ss1a`. The SRF can be explicitly written as \eqref{eq:srf1}.

\begin{equation} \label{eq:srf1}
log(psoda)  = `r round(tidy_coef1a[1,2], 3)` + `r round(tidy_coef1a[2, 2], 3)`prpblck + `r round(tidy_coef1a[3, 2], 3)`log(income) + \hat{u}
\end{equation}

The interpretation of the $\beta_1$ ($prpblck$) coefficient is the change in the expected value of $log(psoda)$ upon a one unit change in the value of $prpblck$, holding $log(income)$ constant. Given that the $prpblack$ data has been converted in to a percentage, "one unit" in this instance refers to a 1 percentage point increase in the percentage of black residents.

To obtain the percentage change in $psoda$ the $\beta_1$ coefficient must be multiplied by 100. As such, the estimated percentage change in $psoda$ for an increase of 0.20/20% in $prpblck$ is given by 20 times the value of the coefficient (which is the expected change for a 1% increase), multiplied by 100. This value is: `r round(tidy_coef1a[2, 2]*20*100, 1)`%.

## 1b

```{r 1b}
fit1b <- lm(log(psoda) ~ prpblck_cln, data = data)
fit1b_summary <- fit1b %>% summary
fit1a_prpblck_coef <- fit1a_summary$coefficients[2, 1]
fit1b_prpblck_coef <- fit1b_summary$coefficients[2, 1]
```

If a simple linear regression estimate of $log(psoda)$ on $prpblck$ is performed, the resulting percentage change in the value of $psoda$ given a one unit increase in the percentage of black residents is `r round(fit1b_prpblck_coef*100, 2)`%.

The value from the model including income is `r round(fit1a_prpblck_coef*100, 2)`%. As such the discrimination effect is larger when income is controlled for. 

When income is ignored in the model, the discrimination effect appears smaller. This would suggest that the proportion/percentage of black residents has little effect on the median price of soda. 

However, when income _is_ controlled for, the discrimination effect is almost twice as large. This would suggest that, given a constant income, the proportion/percentage of black residents has an effect on the median price of soda.

## 1c

The variable $prppov$ is also expressed as a decimal proportion. Therefore it will be adjusted to a percentage, for the same reason as $prpblck$ was: coefficient interpretation. This variable is then added to the model from section 1a.

```{r 1c}
data %<>% mutate(prppov_cln = 100 * prppov)
fit1c <- lm(log(psoda) ~ prpblck_cln + log(income) + prppov_cln, data)
fit1c_summary <- fit1c %>% summary
fit1c_prpblck_coef <- fit1c_summary$coefficients[2, 1]
```

The new value for the $\hat\beta_{prpblck}$ coefficient is $`r round(fit1c_prpblck_coef, 4)`$, i.e. the expected percentage change in the median soda price given a one unit increase in the proportion/percentage of black residents is now `r round(fit1c_prpblck_coef*100, 3)`%. This is lower than when the proportion of residents living in poverty is not controlled for, i.e. $\hat\beta_{prpblck}$ has decreased.

## 1d

```{r 1d}
corr_income_pov <- cor(log(data$income), data$prppov, use = "complete.obs")
```

The correlation between $log(income)$ and $prppov$ is `r round(corr_income_pov, 3)`, i.e. there is a strong, negative correlation between them. From intuition, this is roughly expected, i.e. as the median income in a ZIP-code increases, it would be expected that the proportion of residents living in poverty would decrease.

## 1e

> "Because $log(income)$ and $prppov$ are so highly correlated, they have no business being in the same regression."

This is an issue of __(multi)collinearity__, a common topic in regression. [Consequences](https://en.wikipedia.org/wiki/Multicollinearity#Consequences_of_multicollinearity) of multicollinearity include potential imprecision in the parameter estimates for independant variables, redundancy of information in the model (which can lead to overfitting), and larger standard errors in affected coefficients (which may lead to a failure to reject the null hypothesis). (Wikipedia: 2015)

Therefore to assess the above statement, it is first useful to examine the coefficients from the model created in 1c that contains both variables:

`r kable(tidy(fit1c), caption = "Coefficient estimates from regression of soda prices on the proportion of black residents, proportion of residents in poverty, and median income. Note that 'prpblck_cln' and 'prppov_cln' are the proportion of black residents and those in poverty expressed as percentages, respectively.")`

It is seen that both $log(income)$ and $prppov$ have coefficients that remain statistically significant at the 5% level, and have (small) effects on $psoda$, as does $prpblck$. As such, for these two relatively simple models, there is little issue including both $log(income)$ and $prppov$.

However, in general it can be unwise to include highly correlated variables in a model due to the potential consequences noted above. Additionally, inclusion of an irrelevant variable will lead to larger variances in the $\hat\beta_j$ parameter estimates. For example, if two variables, $x_1$ and $x_2$ are both included in a model but are highly correlated, then it can be seen that either of these variables is irrelevant, given the presence of the other. Therefore the variance of the other $\hat\beta_j$ parameters will increase, and so it is unwise to include both in the model. This is an example of the principle that _inclusion of irrelevant variables is not harmless._

```{r s1_cleanup, echo = FALSE}
rm(list = ls())
```

***

\pagebreak

# Section Two - CEO Salaries


The data for this section contain CEO salaries and related metrics about each firm, including the annual sales, the return on equity, and the return on stock.

## Loading the data

The data are loaded from the provided `.RData` file.

```{r s2_load}
load("./data/ceosal1.RData")
```

The population regression function in \eqref{eq:prf2} will be modelled.

\begin{equation} \label{eq:prf2}
log(salary) = \beta_0 + \beta_1log(sales) + \beta_2roe + \beta_3ros + u
\end{equation}

## 2a

In terms of the model parameters, the null hypothesis that, after controlling for $sales$ and $roe$, $ros$ has no effect on CEO salary is given by \eqref{eq:2anull}, and the alternative that better stock market performance increases salary is given by \eqref{eq:2aalt}.

\begin{equation} \label{eq:2anull}
H_0 : \beta_{ros} = 0
\end{equation}

\begin{equation} \label{eq:2aalt}
H_a : \beta_{ros} > 0
\end{equation}

# 2b

```{r 2b}
fit2b <- lm(log(salary) ~ log(sales) + roe + ros, data)
fit2b_summary <- fit2b %>% summary
tidy_coef2b <- fit2b %>% tidy
ss2b <- data %>% nrow
r2_2b <- fit2b_summary$r.squared
kable(tidy_coef2b, caption = "Coefficient estimates from regression of log(CEO salary) on log(firm sales), return on equity, and return on stock")
```

The parameter estimates for the PRF in \eqref{eq:prf2} are given in the table above. In addition, the $R^2$ value for this model is `r round(r2_2b, 3)` on a sample size of `r ss2b`.

As such the SRF can be expressed by \eqref{eq:srf2b}.

\begin{equation} \label{eq:srf2b}
log(salary) = `r round(tidy_coef2b[1, 2], 3)` + `r round(tidy_coef2b[2, 2], 3)`log(sales) + `r round(tidy_coef2b[3, 2], 3)`roe + `r round(tidy_coef2b[4, 2], 4)`ros + \hat{u}
\end{equation}

1 basis point is defined as one hundredth of one percent, therefore 50 basis points is 50 hundredths of one percent, or 0.5%. $ros$ is expressed as a percentage, therefore the predicted percentage increase in salary if $ros$ increases by 50 basis points (0.5%) is given by $100\frac{\beta_{ros}}{2}$ which is a value of `r round((tidy_coef2b[4, 2]/2)*100, 3)`%. 

In short, $ros$ does not have a practically large effect on salary.

## 2c

Given the null hypothesis in \eqref{eq:2anull}, the $t$-statistic is given by \eqref{eq:t}

\begin{equation} \label{eq:t}
t_{\hat\beta_j} = \dfrac{\hat\beta_j}{se(\hat\beta_j)}
\end{equation}

Looking up the critical value at the 10% significance level for a one sided test in the normal table (given the large sample size) returns a value of 1.282.

The $t$-value can be calculated, or extracted from the linear model.

```{r 2c}
t_calc <- (tidy_coef2b[4, 2] - 0) / tidy_coef2b[4, 3]
t_lm <- tidy_coef2b[4, 4]
crit <- 1.282
```

These values are `r t_calc` and `r t_lm` respectively, i.e. identical. Given the critical value of `r crit` is larger than this $t$ value, in this instance the null would fail to be rejected at the 10% level.

## 2d

$ros$ would not be included in a final model explaining CEO compensation in terms of firm performance. This is due to two reasons: firstly it is not __statistically__ significant at even the 10% level (which is relatively high), and secondly it is not __economically__ significant, given the very small effect that it has on the expected value of CEO salary.

```{r s2_cleanup, echo = FALSE}
rm(list = ls())
```
