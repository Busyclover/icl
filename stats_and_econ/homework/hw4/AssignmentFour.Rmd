---
title: "Statistics and Econometrics - Assignment Four"
author: "Jim Leach"
date: "5 November 2015"
output: pdf_document
---

# Document purpose

The purpose of this document is to write up the response to the fourth assignment given as part of the _Statistics and Econometrics_ module. It is divided in to two sections, each covering a section of the assignment as given. Each major section is divided in to further subsections, one for each question in the assignment.

A number of `R` packages have been used throughout this assignment. These are loaded before the responses are given:

```{r load_pkg, echo = TRUE, message = FALSE}
library(dplyr)
library(magrittr)
library(broom)
library(knitr)
library(car)
library(lmtest)
library(sandwich)
```

***

# Section 1

In this section, some educational data from the [Michigan Department of Education](www.michigan.gov/mde) are provided. Relationships between some of the key variables in this data are explored and some models created.

```{r 1load}
load("./data/meap00_01.RData")
```

## 1a

```{r 1a}
fit_1a <- lm(math4 ~ lunch + log(enroll) + log(exppp), data)
fit_1a_summary <- fit_1a %>% summary
fit_1a_tidy <- fit_1a %>% tidy
```

\begin{equation} \label{eq:1a}
math4 = \beta_0 + \beta_1 lunch + \beta_2 log(enroll) + \beta_3 log(exppp) + u
\end{equation}

The population regression function in \eqref{eq:1a} was estimated using OLS and the following sample regression function was found.

\begin{equation} \label{eq:1a_srf}
math4 = `r round(fit_1a_tidy[1, 2], 3)` + `r round(fit_1a_tidy[2, 2], 3)`lunch + `r round(fit_1a_tidy[3, 2], 3)`log(enroll) + `r round(fit_1a_tidy[4, 2], 3)`log(exppp) + \hat{u}
\end{equation}

The model had an $R^2$ value of `r round(fit_1a_summary$r.squared, 3)` on a sample size of `r nrow(data)`.

The OLS regression coefficient estimates, along with their standard errors, $t$-statistics and $p$-values are given below.

```{r 1a_standard_se}
fit_1a_tidy %>% kable(caption = "OLS coefficient estimates and standard errors", 
                      col.names = c("Coefficient", "Estimate", "Std. Error",
                                    "t-Statisic", "p-Value"))
```

Additionally, the robust standard errors have been calculated for SRF given by \eqref{eq:1a_srf} and are reported below.

```{r 1a_robust_se}
vcov_1a <- fit_1a %>% vcovHC(type = "HC1")
robust_se_1a <- fit_1a %>% coeftest(vcov = vcov_1a)
robust_se_1a_tidy <- robust_se_1a %>% tidy
robust_se_1a_tidy %>% kable(caption = "OLS coefficient estimates and robust standard errors",
                            col.names = c("Coefficient", "Estimate", "Robust Std. Error",
                                          "t-Statistic", "p-Value"))
```

Generally, the robust standard errors are slightly larger than the usual standard errors, resulting slightly larger $p$-values. As expected, the point-estimates of the coefficients remain identical.

## 1b

The White test for heteroskedasticity follows the following steps:

1. Conduct "usual" OLS regression (as done in part 1a);
2. Save the residuals and the fitted values;

```{r 1b_get_resids_and_fits}
resids <- fit_1a$residuals
resids_sqr <- resids^2

fitted <- fit_1a$fitted.values
fitted_sqr <- fitted^2
```

3. Estimate the model $\hat{u}^2 = \delta_0 + \delta_1\hat{y} + \delta_2\hat{y}^2 + error$ and save the $R^2$ value, $R_{\hat{u}^2}^2$;

```{r 1b_fit}
fit_1b <- lm(resids_sqr ~ fitted + fitted_sqr)
fit_1b_rs <- summary(fit_1b)$r.squared
```

4. Compute the test statistic given by $F = \dfrac{R_{\hat{u}^2}^2 / 2}{(1-R_{\hat{u}^2}^2)/n-3} \textasciitilde F_{2, n-3}$

```{r 1b_get_f_manual}
f_manual <- (fit_1b_rs/2)/((1-fit_1b_rs)/(nrow(data)-3))
p_manual <- pf(f_manual, 2, (nrow(data)-3), lower.tail = FALSE)
```

The value of the $F$ statistic was calculated to be `r f_manual`. The corresponding $p$ value of $`r p_manual`$ was significant even below the 1% level. 

Given the null $H_0: \delta_0 = \delta_1 = \delta_2 = 0$, it was possible to reject the null and conclude that there is a relationship between the errors and the original variables in the model - i.e. that there is evidence of heteroskedasticity. 

The F-statisic can also be computed using the `linearHypothesis` function from the `car` package to assess the significance of the $\delta_1$ and $\delta_2$ coefficients.

```{r 1b_get_f_auto}
test_1b <- linearHypothesis(fit_1b, c("fitted = 0" , "fitted_sqr = 0"))
f_auto <- test_1b$F[2]
p_auto <- test_1b$"Pr(>F)"[2]
```

This returns an F-statistic of `r f_auto` and a corresponding $p$-value of $`r round(p_auto, 3)`$, i.e. the same values as those calculated manually.

## 1c

The following SRF was estimated.

\begin{equation} \label{1c_srf}
log(\hat{u}_i^2) = \alpha_0 + \delta_1\widehat{math4_i} + \delta_2\widehat{math4_i^2}
\end{equation}

```{r 1c_fit_init}
fit_1c <- lm(log(resids_sqr) ~ fitted + fitted_sqr)
```

The fitted values, $\hat{g}_i$ were saved and used to obtain the weights via $\hat{h}_i = exp(\hat{g}_i)$.

```{r 1c_get_weights}
gi_1c <- fit_1c$fitted.values
hi_1c <- exp(gi_1c)
```

These weights were then used to obtain the WLS estimates for the PRF given in \eqref{eq:1a}.

```{r 1c_wls}
fit_1c_wls <- lm(math4 ~ lunch + log(enroll) + log(exppp), data, weights = (1/hi_1c))
fit_1c_wlc_summary <- fit_1c_wls %>% summary
fit_1c_wlc_tidy <- fit_1c_wls %>% tidy
fit_1c_wlc_tidy %>% kable(caption = "Weighted-Least-Squares estimates of regression of math4 on lunch, log(enroll) and log(exppp)",
                          col.names = c("Coefficient", "Estimate", "Std. Error",
                                        "t-Statistic", "p-Value"))
```

This leads to the weighted-least-squares SRF of:

\begin{equation} \label{eq:1c_srf}
math4 = `r round(fit_1c_wlc_tidy[1, 2], 3)` + `r round(fit_1c_wlc_tidy[2, 2], 3)`lunch + `r round(fit_1c_wlc_tidy[3, 2], 3)`log(enroll) + `r round(fit_1c_wlc_tidy[4, 2], 3)`log(exppp) + \hat{u}
\end{equation}

The model had an $R^2$ value of `r round(fit_1c_wlc_summary$r.squared, 3)` on a sample size of `r nrow(data)`.

The estimates from OLS are similar but not identical to those obtained from WLS. For example, the coefficient for $lunch$ remains very similar with a value of approximately `r round(fit_1c_wlc_tidy[2, 2], 1)` in both OLS and WLS. More significant changes are observed for the $enroll$ and $exppp$ coefficients. $enroll$ decreased in magnitude from `r round(fit_1a_tidy[3, 2], 3)` to `r round(fit_1c_wlc_tidy[3, 2], 3)`, and $exppp$ increased in magnitude from `r round(fit_1a_tidy[4, 2], 3)` to `r round(fit_1c_wlc_tidy[4, 2], 3)`. Additionally, the $exppp$ coefficient is statistically significant at the 1% level in the WLS model, which was not the case for OLS.

## 1d

The robust standard errors for the WLS model (which allow for some misspecification of the variance function) were calculated.

```{r 1d_rse}
vcov_1d <- fit_1c_wls %>% vcovHC(type = "HC1")
robust_se_1d <- fit_1c_wls %>% coeftest(vcov = vcov_1d)
robust_se_1d_tidy <- robust_se_1d %>% tidy
robust_se_1d_tidy %>% kable(caption = "WLC coefficient estimates and robust standard errors",
                            col.names = c("Coefficient", "Estimate", "Robust Std. Error",
                                          "t-Statistic", "p-Value"))
```

These values do no differ greatly from the usual WLS errors. However, using the robust standard errors, the WLS coefficient for $enroll$ is no longer statistically significant at the 1% level (it remains so at the 2% level, though).

## 1e

For estimating the effect of spending (given by the $exppp$ model) on $math4$, it would appear that the weighted-least-squares model is more precise. Considering the robust standard errors for both OLS and WLS, the former, OLS, has a value of `r round(robust_se_1a_tidy[4, 3], 3)` whereas the latter, WLS, has a value of `r round(robust_se_1d_tidy[4, 3], 3)`.

> Precision: the quality, condition, or fact of being exact and accurate ([ref](https://www.google.co.uk/search?client=ubuntu&channel=fs&q=precision&ie=utf-8&oe=utf-8&gfe_rd=cr&ei=wyw7VpjSI9PH8gfA4ZaYAg))

Given the definition of precision and the relation of standard error with the standard deviation (which helps to describe the width of a distribution), it is fair to say that WLS is more precise. Given the smaller standard error associated with WLS, the confidence interval for the $exppp$ estimate will be smaller (given by $interval \approx exppp \pm 1.96SE_{exppp}$) and hence the WLS estimate is more precise.

***

\pagebreak

# Section 2

In this section, some educational data from the [Michigan Department of Education](www.michigan.gov/mde) are provided. Relationships between some of the key variables in this data are explored and some models created.

```{r 2load}
load("./data/jtrain.RData")
```

## 2a

\begin{equation} \label{eq:2a}
log(scrap) = \beta_0 + \beta_1grant + u
\end{equation}

Considering the PRF given by \eqref{eq:2a} there are several reasons why unobserved factors in $u$ might be correlated with $grant$ (a dummy variable indicating if the firm received a grant). Such reasons could include, but are not strictly limited to:

* The firm's performance (either or otherwise) may be tied to whether it receives a training grant;
* If the firm received a training  grant previously, it may be more or less likely to do so again;
* Training grants may be awarded based on the firm's size (e.g. number of employeers);
* Training grants may only be awarded to firms in unions, or only those that are not unionised; and
* Training grants may be dependent on the total hours of training conducted by the firm in the past.

## 2b

The PRF in \eqref{eq:2a} was estimated using OLS and the results reported:

```{r 2b_fit}
fit_2b_data <- data %>% filter(d88 == 1) %>% select(lscrap, grant, lscrap_1) %>% na.omit
fit_2b <- lm(lscrap ~ grant, data = fit_2b_data)
fit_2b_summary <- fit_2b %>% summary()
fit_2b_tidy <- fit_2b %>% tidy
fit_2b_tidy %>% kable(caption = "OLS estimates of regression of the log of scrap rate on grant status for the year 1988",
                      col.names = c("Coefficient", "Estimate", "Robust Std. Error",
                                          "t-Statistic", "p-Value"))
```

Therefore the SRF is given by \eqref{eq:2b}

\begin{equation} \label{eq:2b}
log(scrap) = `r round(fit_2b_tidy[1, 2], 3)` + `r round(fit_2b_tidy[2, 2], 3)`grant + \hat{u}
\end{equation}

Additionally, the $R^2$ value for this model was $`r fit_2b_summary$r.squared`$ on a sample size of `r nrow(fit_2b_data)`.

Looking at the SRF given by \eqref{eq:2b}, it appears that receiving a training grant does not significantly lower a firm's scrap rate, for the year 1988. 

## 2c

The explanatory variable $log(scrap_{87})$ was added to the model.

```{r 2c_fit}
fit_2c <- update(fit_2b, . ~ . + lscrap_1)
fit_2c_summary <- fit_2c %>% summary
fit_2c_tidy <- fit_2c %>% tidy
grant_coef <- fit_2c_tidy[2, 2]
grant_se <- fit_2c_tidy[2, 3]
fit_2c_tidy %>% kable(caption = "OLS estimates of regression of the log of scrap rate on grant status for the year 1988, and scrap rate for the year 1987",
                      col.names = c("Coefficient", "Estimate", "Robust Std. Error",
                                          "t-Statistic", "p-Value"))
```

Therefore the SRF is given by \eqref{eq:2c}

\begin{equation} \label{eq:2c}
log(scrap) = `r round(fit_2c_tidy[1, 2], 3)`  `r round(fit_2c_tidy[2, 2], 3)`grant + `r round(fit_2c_tidy[3, 2], 3)`log(scrap_{87}) + \hat{u}
\end{equation}

Additionally, the $R^2$ value for this model was $`r fit_2c_summary$r.squared`$ on a sample size of `r nrow(fit_2b_data)`.

The estimated effect of grant has changed. It is now _negative_ (indicating that having a grant lowers the scrap rate) with a value of `r round(fit_2c_tidy[2, 2], 3)`. The interpretation of this coefficient (using information sourced [here](http://davegiles.blogspot.co.uk/2011/03/dummies-for-dummies.html))  is that, when $grant$ switches from 0 to 1 (i.e. when a firm receives a grant), the expected percentage change in the scrap rate is given by \eqref{eq:2c_dummy} and has a value of `r round(100*(exp(grant_coef)-1), 2)`%.

\begin{equation} \label{eq:2c_dummy}
\begin{split}
grant_{0\rightarrow 1};\quad \%\delta scrap = 100 * exp(grant)-1 \\
grant_{1\rightarrow 0};\quad \%\delta scrap = 100 * exp(-grant)-1
\end{split}
\end{equation}

```{r 2c_hyp}
t <- grant_coef / grant_se
p <- pt(t, fit_2c$df.residual)
```

It is seen that with a $t$ statistic of `r round(t, 2)` and a $p$-value of `r round(p, 3)`, the coefficient for $grant$ is significant at the 5% level using the one-sided alternative $H_1\quad:\quad\beta_{grant}<0$.

## 2d

The null hypothesis is given that $H_0:\beta_{log(scrap_{87})}=1$. In order to test this a two-sided $t$-test was performed.

```{r 2d}
t_scrap <- (fit_2c_tidy[3, 2] - 1) / fit_2c_tidy[3, 3]
t_abs <- abs(t_scrap)
c <- abs(qt(.05/2, fit_2c$df.residual))
p_scrap <- pt(t_abs, fit_2c$df.residual, lower.tail = FALSE)
```

The $p$-value reported by the `pt` function when `lower.tail = FALSE` is $P[X > x]$. For a two-sided test, the null may be rejected when the _absolute value_ of $t$ is greater than some critical value. 

The absolute value of the $t$ statistic for the $\beta_{log(scrap_{87})}$ parameter against the null that it is one was found to be `r round(t_abs, 3)`. Comparing this to the absolute of the critical value at the 5% signficance level in the two sided test of `r round(c, 3)`, it was found to be higher. Therefore the null hypothesis $H_0:\beta_{log(scrap_{87})}=1$ can be rejected at the 5% significane level.

The associated $p$ value was found to be $`r round(p_scrap, 5)`$.

The rejection of the null $H_0:\beta_{log(scrap_{87})}=1$ is backed up by the fact that the 95% confidence interval for $\beta_{log(scrap_{87})}$ does not contain the value 1 : [`r round(confint(fit_2c)[3, ], 3)`].

## 2e 

```{r 2e}
vcov_2c <- fit_2c %>% vcovHC(type = "HC1")
robust_se_2c <- fit_2c %>% coeftest(vcov = vcov_2c)
robust_se_2c_tidy <- robust_se_2c %>% tidy
grant_rse <- robust_se_2c_tidy[2, 3]
robust_se_2c_tidy %>% kable(caption = "OLS coefficient estimates and robust standard errors",
                            col.names = c("Coefficient", "Estimate", "Robust Std. Error",
                                          "t-Statistic", "p-Value"))
```

As use of the robust standard errors does not change the point estimates of the coefficients, the SRF for this model remains that given by \eqref{eq:2c}. As such the interpretation of the $grant$ coefficient remains the same as in 2c.

### $grant$ coefficient

The robust standard error for the $grant$ variable remained relatively constant. Previously its value was `r round(grant_se, 3)` and it has changed only to `r round(grant_rse, 3)`. 

To understand the exact effect that this has on the significant of a simple test can be performed as previously.

```{r 2e_hyp}
t_grant_rse <- grant_coef / grant_rse
p_grant_rse <- pt(t_grant_rse, fit_2c$df.residual)
```

It is seen that with a $t$ statistic of `r round(t_grant_rse, 2)` and a $p$-value of `r round(p_grant_rse, 3)`, the coefficient for $grant$ is significant at the 5% level using the one-sided alternative $H_1\quad:\quad\beta_{grant}<0$, even when using robust standard errors.

### $\beta_{log(scrap_{87})}$ coefficient

```{r 2e_scrap}
t_scrap_rse <- (robust_se_2c_tidy[3, 2] - 1) / robust_se_2c_tidy[3, 3]
t_abs_rse <- abs(t_scrap_rse)
c <- abs(qt(.05/2, fit_2c$df.residual))
p_scrap_rse <- pt(t_abs_rse, fit_2c$df.residual, lower.tail = FALSE)
```

Performing the same analysis for the $\beta_{log(scrap_{87})}$ coefficient as was done in 2d, it is seen that the $\beta_{log(scrap_{87})}$ remains significant at the 5% level against the null $H_0:\beta_{log(scrap_{87})}=1$ with a $p$ value of `r round(p_scrap_rse, 3)`. Moreoever, the 95% confidence interval still does not contain the value 1: [`r round(robust_se_2c_tidy[3, 2] + c(-1, 1)*1.96*robust_se_2c_tidy[3, 3], 3)`].

