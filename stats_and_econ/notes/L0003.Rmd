---
title: "Statistics and Econometrics - Lecture 3"
author: "Jim Leach"
date: "13 October 2015"
output:   
  html_document:     
    theme: readable     
    toc: true     
    toc_float: true
---

# Ordinary least squares (OLS)

## Basic definition

We term the data to be assessed a __random sample__ of _independant_ observations, i.e. ${(x_i, y_i), i = 1, 2, ...n}$. Following on from this, it is possible to write the PRF at an observation level:

\begin{equation} \label{eq:obs_lvl_prf}
y_i = \beta_0 + \beta_{1}x_i + u_i
\end{equation}

Note that $i$ is the observation index.

From this, estimates for the parameters are noted as $(\hat{\beta_0}, \hat{\beta{1}})$. The _hat_ symbol denotes that these parameters are _estimates_, rather than the true population values.

### Objective

The __sum of squared residuals__ (SSR) is given by \eqref{eq:ssr} below. 

\begin{equation} \label{eq:ssr}
SSR = \sum_{i=1}^n \hat\mu_{i}^2 = \sum_{i=1}^{n}(y_i - \hat\beta_0 - \hat\beta_{1}x_i)^2
\end{equation}

The SSR measures the difference between the regression line and the individual observations of the dependant variable, $y$. The objective of OLS is to choose parameters, $\beta$ so as to minimise the value of the SSR.

That is, the values of $(\hat{\beta_0}, \hat{\beta{1}})$ can be said to be minimisers of the SSR. 

### First Order Conditions

In order to minimise the regression equation to find the values of $\hat\beta_0$ and $\hat\beta_1$ it is necessary to take the first order derivatives of the SSR with respect to the two parameters, set each equation to be equivalent to 0 and then solve for $\beta_0$ and $\beta_1$ respectively. 

\begin{equation} \label{eq:ssr_beta0}
\frac{\partial SSR}{\partial\hat\beta_0} = -2 . \sum_{i=1}^{n} (y_i - \hat\beta_0 - \hat\beta_{1}x_i) = 0
\end{equation}

\begin{equation} \label{eq:ssr_beta1}
\frac{\partial SSR}{\partial\hat\beta_1} = -2 . \sum_{i=1}^{n} (y_i - \hat\beta_0 - \hat\beta_{1}x_i)x_i = 0
\end{equation}

Solving \eqref{eq:ssr_beta0} and \eqref{eq:ssr_beta1} yields the following definitions of the parameters:

$\hat\beta_i = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2}$, and $\hat\beta_0 = \bar{y} - \hat\beta_{1}\bar{x}$.

(Recall that $\bar{x} = \frac{1}{n}\sum_{i=1}^2x_i$)

It is also noted that the OLS solution has the (relatively trivial) requirement that $\sum_{i=1}^n(x_i - \bar{x})^2 > 0$

This is trivial as it simply requires that not all the $x_i$ values in the sample are the same, even one differing value will ensure that this condition is true.

## Sample Regression Function

Once the OLS calculations for the parameters have been performed, the __sample regression function__ (SRF) can be written as

\begin{equation} \label{eq:srf}
\hat{y} = \hat\beta_0 + \hat\beta_{1}x
\end{equation}

The SRF is an estimate of the true population regression function.

### Interpreting OLS Estimates

As with the PRF, the interpretation of SRF parameters is relatively simple. The $\hat\beta_1$ coefficient is interpreted as the change in the $\hat{y}$ (the estimated value of $y$) when $x$ increases by one unit. 

Note that the true value of the dependant can be expressed as

\begin{equation} \label{eq:yi}
y = \hat{y} + \hat{u}
\end{equation}

or alternatively,

\begin{equation} \label{eq:y}
y = E(y|x) + u
\end{equation}

In this way it is imagined that the SRF is a good estimate of PRF and represents the "average" values of the true regression parameters, with an increasing accuracy as $n$ tends to $\infty$.

## SRF and Sums of Squares

Once the SRF has been derived and calculated, several further definitions are possible. Recall from \eqref{eq:yi} that each $y_i$ can be represented by a combination of the estimated value of y, and the residual.

It is therefore useful to define several ways to measure variations in $y$ from $\bar{y}$ (the mean $y$).

* SST - Total sum of squares (total variation in $y_i$): $SST = \sum_{i=1}^n (y_i - \bar{y})^2$

* SSE - Explained sum of squares (variation in $\hat{y_i}$): $SSE = \sum_{i=1}^n (\hat{y_i} - \bar{y})^2$

* SSR - Sum of squared residuals (variation in $\hat{u_i^2}$): $SSR = \sum_{i=1}^n \hat{u_i^2}$

Combining these forumlae shows that $SST = SSE + SSR$, i.e. the total variation in the observed $y$ values can be explained by a combination of the variation in the $\hat{y}$ (estimate) values and the residuals ($u$).

### An Assessment of Fit - $R^2$

To determine the extent to which the x values explain the variation in $y$ a new metric is defined - $R^2$.

The $R^2$ metric describes the fraction of variation in $y$ that is explained by the SRF:

\begin{equation} \label{eq:r2}
R^2 = \frac{SSE}{SST} = 1- \frac{SSR}{SST}
\end{equation}

In general, a larger $R^2$ means a better fit for the model (but be cautious of overfitting), and remember that $0 \leq R^2 \leq 1$

## Changing units of measurement

* If y is multiplied by a constant, c:
  
> The OLS intercept and slope estimates are also multiplied by c (recall the definition of $\hat\beta_0$ and $\hat\beta_1$).
  
* If x is multipled by a constant, c:
  
> The OLS slope will be divided by the same constant, c, (or multiplied by $1/c$) and the intercept estimate will be unaffected (recall that the intercept describes on a $y$ value).

* The $R^2$ value does not change when varying the units of measurement (as it is described only in terms of $y$, so varying the units of $x$ has no effect, and varying the units of $y$ affects both $SSR$ and $SST$ in the same way).

## Non-linearity

The OLS requires only that the regression model is __linear in parameters $(\hat\beta_0, \hat\beta_1)$. As such, non linear relationships in $x$ and $y$ can be easily accomodated.

An easy way to to do this is with $\log$ relationships between $y$ and $x$. However, it must be remembered that if $\log$ relationships are used, the interpretation of the SRF parameters will change slightly. The table below describes these interpretations.



\begin{center}
\begin{tabular}{ c c c  c}
 Model & Dependant Variable & Independant Variable & Interpretation of $\beta_1$ \\ \hline
 Level - level & y & x & $\Delta y = \beta_1 \Delta x$ \\
 Level - log & y & $log (x)$ & $\Delta y = ( \beta_1 / 100 ) \% \Delta x$ \\
 Log - level & $log (y)$ & x & $\% \Delta y = ( 100 \beta_1 ) \Delta x$ \\
 Log - log & $log(y)$ & $log(x)$ & $\% \Delta y = \beta_1 \% \Delta x$
\end{tabular}
\end{center}

## Properties of OLS Estimators

There are a few key properites of OLS estimators that are important to know:

* The data used to generate the SRF are a realisation of a __random sample__;
* The OLS parameter estimates are therefore themselves random values. 
* To make inferences about the population parameters we need to understand the __mean__ and __variance__ of the OLS estimators.

### SRF Assumptions:

1. __Linear in parameters__ - in the population model, $y$ is truly related to $x$ in a way that is linear in the parameters;
2. __Random sampling__ - ${(x_i, y_i), i = 1, 2, ..., n}$ with  $n > 2$ is a random sample from the population;
3. __Sample variation__ - the sample values, $x_i$ are not the same value; and
4. __Zero conditional mean__ - $E(u|x) = 0$ for any given value of $x$.

### Estimator properties

Following on from these 4 assumptions, several properties of the OLS estimators can be stated:

#### Theorem 2.1

* The OLS estimators are unbiased, i.e. $E(\hat\beta_1) = \beta_1$ and $E(\hat\beta_0) = \beta_0$.

The unbiased OLS estimates are "centered" around the population parameters ($\beta$ values). Therefore, on average, they correctly estimate the population values.

It is noted that:

\begin{equation}
\hat\beta_1 = \beta_1 + \frac{\sum_{i=1}^n (u_i - \bar{u})(x_i - \bar{x})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{equation}

I.e., the OLS estimate for $\hat\beta_1$ is composed of the population value ($\beta_1$) plus an error term. This error term comprises the covariance of errors, $u$ and $x$ values, divided by the variance in the $x$ values. As such, it can be seen that a large variance (i.e. range) of $x$ values is desireable to get a good estimate for $\hat\beta_1$.

## Homo- and Heteroskedasticity

The homoskedasticity assumption (often noted as assumption 5 following on from the four above) states that:

\begin{equation}
Var(u_i | x_i) = \sigma ^ 2
\end{equation}

I.e. the variation in the errors (a function of the population model) is not related to the $x$ values, implying that $Var(u_i) = \sigma ^ 2$.

If this assumption does not hold, i.e. $Var(u_i)$ is found to depend on the value of $x_i$ then the errors are said to exhibit __heteroskedasticity__.

#### Theorem 2.2

Theorem 2.1 stated that the OLS parameter estimates had expected values of the population parameters. However, a description of the _variance_ of the OLS parameters is needed. This is theorem 2.2, under this:

\begin{equation}
Var(\hat\beta_1) = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}
\end{equation}

and

\begin{equation}
Var(\hat\beta_0) = \frac{\sigma^2 n^{-1} \sum_{i=1}^n x_{i}^2}{\sum_{i=1}^n  (x_i - \bar{x})^2}
\end{equation}

In essence, theorem 2.2 stats that the larger the value of $\sigma^2$, the greater the variability in the parameters estimated from OLS. Similarly, the larger the variance in $x$, the smaller the variance in the parameters.

##### Estimation of $\sigma^2$

Given that $y$ can be described in multiple ways (\eqref{eq:yi}, \eqref{eq:y}), i.e. in terms of the residuals or the errors, it is possible to write:

\begin{equation}
\hat{u}_i = u_i - (\hat\beta_0 - \beta_0) - (\hat\beta_1 - \beta_1)x_i
\end{equation}

That is, the errors in the PRF can be estimated from the residuals. Following from this, the best estimator of $\sigma^2$ is:

\begin{equation}
\hat\sigma^2 = \frac{SSR}{n-2} = \frac{\sum_{i=1}^2\hat{u}_i^2}{n-2}
\end{equation}

This yields a new __Theorem__. Under assumptions one to five (four base assumptions, plus homoskedasticity assumption): $E(\hat\sigma^2 = \sigma^2)$.

$\hat\sigma = \sqrt{\hat\sigma^2}$ is the __standard error of the regression__ and it used in forming the standard errors of the SRF parameters.






 