---
title: "Statistics and Econometrics - Lecture 5"
author: "Jim Leach"
date: "20 October 2015"
output: pdf_document
---

# Inference in Regression

## OLS Estimate Distribution

### Motivation

The main goal of MLR is to gain knowledge about the unobservable population parameters ($\beta$'s).

OLS provides the point estimates of the parameters and, being unbiased, will get these right, on average. 

However to know the "true values" or estimate if a hypothesis is supported, more than the mean and variances of the $\hat\beta_j$ values is required.

### Normality Assumption

* __MLR6__ Normality - _The disturbance, $u$, is independant of all explanatory variables and is normally distributed with mean zero and variance $\sigma^2$_.

\begin{equation}
u ~ Normal(0, \sigma^2)
\end{equation}

This is both a strong and a restrictive assumption. Further more it implies MLR4 (ZCM - $E(u|x) = 0$) and MLR5 (homoskedasticity: $Var(u|x) = \sigma^2$).

Combined, MLR assumptions one to six are termed the __classic linear model__ assumptions (CLM).

Under CLM, OLS produces the _minimum variance, unbiased estimators_ (i.e. not just the BLUE estimators, but the best estimators from all unbiased methods, including non-linear estimators.).

Following from this, CLM implies \eqref{eq:clm_imp}

\begin{equation} \label{eq:clm_imip}
y|\textbf{x} ~ Normal(\beta_0 + \beta_1x_1 + ... + \beta_kx_k, \sigma^2)
\end{equation}

I.e. the $y$ value has the same shape as $u$ and therefore is also normally distributed with variance due to the errors.

It is often the case that MLR6 does __not__ hold true (e.g. for many models, a negative value of $y$ is not realistic).

As stated, MLR6 is restrictive. However for large-sample models, the inference methods presented below can be used without it holding true.

## Sampling Distribution of OLS

#### Theorem 4.1 - Normal Sampling Distribution

_Under CLM_: $\hat\beta_j ~ Normal(\beta_j, Var(\hat\beta_j))$, _where the variance is given by:_ $Var(\hat\beta_j) = \frac{\sigma^2}{SST_j(1-R_j^2)}$

This implies that:

\begin{equation}
\frac{\hat\beta_j - \beta_j}{sd(\hat\beta_j)} ~ Normal(0, 1)
\end{equation}

Recall that the value of $\sigma^2$ has to be estimated:

\begin{equation}
se(\hat\beta_j) = \frac{\hat\sigma}{\sqrt{SST_j(1-R_j^2)}}
\end{equation}

This is termed the __standard error__ of $\hat\beta_j$ and leads to:

##### Theorem 4.2 - $t$-distribution

Following from theorem 4.1, under CLM the followig is also true:

\begin{equation} \label{eq:t42}
\frac{\hat\beta_j - \beta_j}{se(\hat\beta_j)} ~ t_{n-k-1}
\end{equation}

Here, $k+1$ is the number of unknown parameters in the population model and $n-k-1$ is the __degrees of freedom__.

Theorem 4.2 is critical for driving the ability to perform hypothesis testing as described here.

## Testing Hypothesis

### Simple Null

The simplest null hypothesis is given by:

\begin{equation}
H_0 : \beta_j = 0
\end{equation}

If this null hypothesis holds true it implies that, holding other variables constant, the $x_j$ value has no effect on $y$.

### Testing the simple null

To test the simple null hypothesis, a __$t$-statistic__ is developed:

\begin{equation}
t_{\hat\beta_j} = \frac{\hat\beta_j}{se(\hat\beta_j)}
\end{equation}

(This is simlar to \eqref{eq:t42} given $H_0 : \beta_j = 0$). Recall that when the degrees of freedom (n - k - 1) are large (>120) the $t$ distribution approaches the standard normal.

The $t$-statistic, along with a rejection rule (e.g. __alternative hypothesis__ and the chosed __significance level__) are used to decide whether to reject the null.

The __significance level__ is defined as the probability of rejecting $H_0$ when it is true and has typical values of 1%, 5%, or 10%.

The __alternative hypothesis__ can be:

* $H_1 : \beta_j > 0$ or $H_1 : \beta_j < 0$ i.e. one-sided; or
* $H_1 : \beta_j \neq 0$, i.e. two-sided.

### One-Sided Alternatives

1. Pick a significance level;
2. Look up the (1-$\alpha$) percentile in a $t$ distribution with n - k - 1 degrees of freedom and call this $c$, the critical value.
3. Reject the null if the calculated $t$ statistic is > c for testing $H_1 : \beta_j > 0$ or less than c for testing $H_1 : \beta_j < 0$.

### Two-Sided Alternative

1. The critical value here is instead based on (1-$\alpha$/2) percentile in a $t$ distribution with n-k-1 df.
2. Reject $H_0$ if the __absolute value__ of the $t$ statistic is greater than c, i.e. $|t_{\hat\beta_j}| > c$

### Some terminology

Unless otherwise stated, the alternative is assumed to be two-sided. If the null is rejected, it is typically stated that "$x_j$ is statistically significant (or different fromm 0) at the $\alpha$ level".

## $p$-Values and testing other hypotheses

Most software (R included) will compute a $p$ value from a $t$ score, assuming a two-sided test. To calculate the one-sided alternative, simply divide the two-sided by 2.

More generally, and $t$ statistic can be computed for: $H_0 : \beta_j = a_j$ and the releated statistic given by: $t = \frac{\hat\beta_j - a_j}{se(\hat\beta_j)}$.

## Economic vs. Statistical significance

__Statistical__ significance is when an explanatory variable has a $t$ value that is sufficiently large, i.e. beyond the critical value, c.

__Economic__ significane is when the size of the estimate, $\hat\beta_j$ is large relative to the size of $y$.

An _important_ x should be both statistically and economically significant.

## Confidence Intervals

The confidence interval for $\hat\beta_j$ is based on theorem 4.2 and \eqref{eq:t42}.

The (1-$\alpha$)% CI is defined below, where c is the (1-$\alpha$/2) percentile in a $t_{n-k-1}$ distribution

\begin{equation}
\hat\beta_j \pm c . se(\hat\beta_j) = [\hat\beta_j - c . se(\hat\beta_j), \hat\beta_j + c . se(\hat\beta_j)]
\end{equation}

The 95% confidence interval is interpreted as the range of values between which the true population parameter $\beta_j$ will lie 95% of the time if many random samples are drawn.

When df is large (>120) then the $t$ distribution can be replaced with a standard normal (and as such, the 95% CI is about $\hat\beta_j \pm 1.96 . se(\hat\beta_j)$)

The width of the CI depends on the standard error, and the critical value.

* A higher confidence level requires a larger C and a wider interval.
* A large standard error produces a wider confidence interval.

### Two-sided tests

In a two-sided test, i.e. $H_0 : \beta_j = a_j$, $H_0$ can be reject at the $\alpha$ significance level if (and only if) the (1-$\alpha$)% CI does not contain $a_j$.

## Linear Combination of Parameters

At times it can be important to assess if $x_1$ has the same effect on $y$ as does $x_2$, i.e. to test:

\begin{equation}
H_0 : \beta_1 - \beta_2 = 0; H_1: \beta_1 - \beta_2 \neq 0
\end{equation}

Conceptually, a $t$-test could be used (of the form $t = \frac{\hat\beta_1-\hat\beta_2}{se(\hat\beta_1-\hat\beta_2)})$). However $se(\hat\beta_1-\hat\beta_2)$ is usually not available.

### Re-parameterise

A solution is to re-parameterise the model to give \eqref{eq:repar}.

\begin{equation} \label{eq:repar}
log(y) = \beta_0 + \theta x_1 + \beta_2(x_2 + x_1) +u
\end{equation}

Where $\theta = \beta_1 - \beta_2$.

The hypothesis can then be rewritten as:

\begin{equation}
H_0: \theta = 0; H_1 \theta \neq 0
\end{equation}

It is said that the parameter of interest $\theta$ has been isolated by re-parameterisation. Given that the OLS output will provide both $\hat\theta$ and $se(\hat\theta)$, this hypothesis can be tested.

Note that both $\beta_0$ and $\beta_2$ remain in the equation and so will remain unchanged.

The OLS output will provide the value for $\theta$, which is then used to determine if $x_1$ and $x_2$ have the same effect on $y$. 

## Testing Multiple Linear Restrictions - the $F$ test

It is also at times desirable to test the effect of a group of variables on $y$ (with the remaining variables as a control).

This is often performed using __exclusion restrictions__ - i.e. whether a group of parameters are all equal to zero. Generally, multiple linear restrictions involve more than one linear restriction on parameters.

### Testing Exclusions

The first step is define an __unrestricted model__, e.g. \eqref{eq:ur}

\begin{equation} \label{eq:ur}
y = \beta_0 + \beta_1 x_1+ ... + \beta_k x_k + u
\end{equation}

$q$ restrictions are then applied under the null hypothesis: $H_0 : \beta_{k-q-1} = 0, ... , \beta_k = 0$. (The alternative is simple that $H_0$ is not true.)

Under this null, the __restricted modell__ \eqref{eq:r} is defined:

\begin{equation} \label{eq:r}
y = \beta_0 + \beta_1 x_1 + ... + \beta_{k-q} x_{k-q} + u_{(r)}
\end{equation}

It is not sufficient to simply test each $t$ statistic separately as an assessment of the $q$ parameters' _joint_ significant is being sought. I.e., it is possible for none to be individually significant but for the group to have a combined significance.

In order to perform this test, the __restricted__ and __unrestricted__ models are interpreted separately. Intuitively, a (large) changein the $SSR$ will indicate that variables should/shouldn't be included. Therefore a test statistic is defined:

\begin{equation} \label{eq:f}
F = \dfrac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \approx F_{q, n-k-1}
\end{equation}

Where $q$ is the number of restrictions (or $df_r = dr_{ur}$) and $n-k-1$ is the degrees of freedom in the unrestricted model.

### The $F$-Statistic

The F-stat is always positive (as the $SSR_r$ cannot be less than $SSR_{ur}$). Similar as to a $t$ test, an $F$ distribution is used to define a critical value above which the null hypothesis is reject (at a specified significance level). Essentially, $H_0$ is rejected if the increase in SSR moving from the unrestricted to the restricted model is "large enough".

N.b. when $q$ = 1, $H_0$ can be tested with either a $t$ or an $F$ statistic.

### The $R^2$ form

Recall that $SSR_r = SST(1-R_r^2)$. This allows \eqref{eq:f} to be rewritten as:

\begin{equation} \label{eq:f_r}
F = \dfrac{(R_{ur}^2 - R_r^2)/q}{(1-R^2_{ur})/(n-k-1))}
\end{equation}

This equation, \eqref{eq:f_r}, is termed the __R-squared form__ of the $F$ statistic.

If $H_0$ is rejected, it is said that the $x_{k-q-1}, ..., x_k$ parameters are __jointly statistically significant__, if it is not they are __jointly insignificant__. 

The $p$-value for a $F$ test is the probability of the $F$ distribution having a value of or greater than the observed value.

### Practical tip

These tests can be performed easily in R using the `linearHypothesis()` function from the `cars` package. 

### F Test for Overall Signifince

When q = k, the null is routinely tested by most software packages, including R. This is known as an __F test for overall significance__.

The null in this instance is that _none_ of the explanatory variables has an effect on $y$, therefore the restricted model is $y = \beta_0 + u$.

Under this null, the $F$ stat has an $F_{k, n-k-1}$ distribution. Further, as the R-squared value under the null is zero, the $F$ stat is given by \eqref{eq:f_simpl}

\begin{equation} \label{eq:f_simpl}
F = \dfrac{R^2 /k}{(1-R^2)/(n-k-1)}
\end{equation}

## Final note - reporting regression results

A good practice for the minimum reporting standards is to report:

* Estimated coefficients _and_ standard errors;
* R-squared;
* Sample size;
* Equation form of SRF (if the number of equations is small) or a table of parameters for multiple models/complex models.


