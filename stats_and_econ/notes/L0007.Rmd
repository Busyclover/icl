---
title: "Statistics and Econometrics - Lecture 7"
author: "Jim Leach"
date: "27 October 2015"
output: pdf_document
---

# Further topics in MLR

## Linear Probability Model

If the dependant variable is binary (i.e. $y = 0$ or $y=1$) then the interpretation of the $\beta$ coefficients changes.

Firstly, it is possible to re-write the generic PRF to give \eqref{eq:lpm} (noting that the ZCM assumption still holds).

\begin{equation} \label{eq:lpm}
P(y=1|x) = E(y|x) = \beta_0 + \beta_1 x_1 + ... + \beta_kx_k
\end{equation}

The value $P(y=1|x)$ is termed the __reponse probability__ and the overall regression model is termed a __linear probability model (LPM)__.

In the LPM, the $\beta$ parameters are interpreted as the change in the _probability of success_ given a one unit increase in the related independant variable, i.e. $\Delta P(y=1|x) = \beta_j\Delta x_j$. 

### Shortcomings of LPM

It is possible that when using the LPM, the predicted probability can be outside $[0, 1]$. I.e., the linear function is not suitable for modelling probablities and more advanced models should be used (e.g. logit and probit). 

Further, for LPM it is the case that

\begin{equation}
Var(u|x) = Var(y|x) = P(y=1|x)[1-P(y=1|x)]
\end{equation}

I.e., MLR5 (homoskedasticity assumption) does not hold as the variance of the error term is seen to depend on $x$. This does _not_ cause estimation bias but it _does_ invalidate the standard error calculations.

***

## Self-selection problems

Typically, dummy variables are used when a _program effect_ is being investigated, e.g. the effectiveness of a drug, new job training etc. 

Typically individuals are divided in to two groups, a _control_ and a _treatment_, the test results used to measure the program effect.

However, it is the case that individuals often chose whether (or not) to participate in a program. This can lead to __self selection__ problems. These problems can only be eliminated if it is possible to control for everything that may be correlated with the outcome of interest.

Often this is not possible and therefore $E(u|participation = 1) \neq E(u|participation = 0)$, i.e. the ZCM is violated and the estimate of the program effect will be biased.

## Large sample (Asymptotic) Inference

In practice MLR6 ($u\textasciitilde Normal(0, \sigma^2)$) is too strong an assumption and does not hold in general. However, MLR1-4 imply that the OLS estimators are unbiased, and MLR1-6 (CLM) imply that they are normally distributed.

The central limit theorem (CLT) provides and answer to this condundrum - i.e. how is inference performed without MLR6. In short, when $n$ is large, the OLS estimators themselves are approximately normally distributed.

Therefore it is possible to write \eqref{eq:t} even when $u$ is not normally distributed.

\begin{equation} \label{eq:t}
\dfrac{\hat\beta_j - \beta_j}{se(\hat\beta_j)} \textasciitilde t_{n-k-1}
\end{equation}

It is also known that $t_{n-k-1}$ approaches Normal(0, 1) as n increases. Therefore, how large must $n$ be to get a good approximation? In short it depends on the distribution of $u$, but in general an n of 30 is acceptable.

__$t$ and $F$ testing, and confidence intervals can all be carried out exactly the same even when MLR6 does not hold__.

(Note that even if normality in $u$ is not required, homoskedasticity still _is_ required.)

***

## Functional forms: Log, Quadratic, Interactions

OLS can be used to describe relationships that are not strictly linear by using nonlinear function of $x$ and $y$ (recall that the _parameters_ are still a linear function for $y$).

### Logarithmic form

Logarithms can be used in multiple ways in a linear regression, each having a slightly different effect on the coefficient interpretaion. 

These interpretations are given by the below table.

\begin{center}
\begin{tabular}{ c c c  c}
 Model & Dependant Variable & Independant Variable & Interpretation of $\beta_1$ \\ \hline
 Level - level & y & x & $\Delta y = \beta_1 \Delta x$ \\
 Level - log & y & $log (x)$ & $\Delta y = ( \beta_1 / 100 ) \% \Delta x$ \\
 Log - level & $log (y)$ & x & $\% \Delta y = ( 100 \beta_1 ) \Delta x$ \\
 Log - log & $log(y)$ & $log(x)$ & $\% \Delta y = \beta_1 \% \Delta x$
\end{tabular}
\end{center}

Log models are useful for a number of reasons:

1. They are invariant to the scale of variables since they measure _percentage_ changes;
2. For models with $y>0$ (i.e. y is never negative), then the conditional distribution of $y$ with repsect to $x$ is often heteroskedastic/skewed (i.e. the variance of $y$ changes with $x$). Taking logs can eliminate the heteroskedastic and/or the skewness problems; and
3. Taking logs of a variable can narrow the range, limiting the effect of outliers.

Variables that are often used in log form include any values that are, by definition, positive such as wage, salary, sales, market value etc (often these are Â£/$ ammounts). Variables that are very large (e.g. populations, total number of employees) are also often found in log form.

Variables that are more likely to be used in level form include those measured in years and those that are proportions (e.g. unemployment rate). This is usually due to the fact that the coefficient estimation for such variables is very simple in the level form.

### Quadratic form

\begin{equation} \label{eq:quad}
y = \beta_0 + \beta_1 x + \beta_2 x^2 +u
\end{equation}

The PRF given in \eqref{eq:quad} is a typical _quadratic_ form of a linear regression. In this instance, it is not possible to interpret $\beta_1$ alone as the coefficient that measures the change in y with respect to x. $\beta_2$ must be taken in to account as well.

\begin{equation} \label{eq:quad_diff}
\Delta \hat{y} \approx (\hat\beta_1 + 2 \hat\beta_2 x)\Delta x
\end{equation}

\eqref{eq:quad_diff} describes how y varies with x (it is obtained by taking the first differential of \eqref{eq:quad} with respect to x). As can be seen, y depends on _both_ $\beta_1$ _and_ $\beta_2$. 

In this instance, if $\hat\beta_1 >0$ and $\hat\beta_2 <0$ then as x increases, the second term will become larger: i.e. at low values of x, y will increase. However as x increases, this growth will slow and eventually reverse to reduce y at high values of x. It is possible to define the point at which this "turn" occurs: $x^{*} = |\hat\beta_1 / (2\hat\beta_2)|$

(If $\hat\beta_1 <0$ and $\hat\beta_2 >0$ then this pattern would be reversed and y would decrease with x at low x, before turning to increase with x at high x).

### Interaction terms

\begin{equation} \label{eq:interact}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + u
\end{equation}

In the PRF given in \eqref{eq:interact} it is not possible to interpret $\beta_1$ alone as measuring how y varies with $x_1$. $\beta_3$ must be taken in to account, too. Taking derivatives, it is found that \eqref{eq:interact_deriv} describes how y changes with $x_1$.

\begin{equation} \label{eq:interact_deriv}
\Delta y = (\beta_1 + \beta_3 x_2)\Delta x_1
\end{equation}

Interpreting the coefficients in \eqref{eq:interact} can be tricky. Generally, a coefficient is interpreted as the change in $y$ given a 1-unit change in the independent variable assuming that the other variable included in the interaction has a zero-value (which may be practically absurb). For example from \eqref{eq:interact_deriv} it can be said that $\beta_1$ is the change in $y$ given a one unit change in $x_1$ when $x_2$ is zero (i.e. removing the second term of the brackets). The coefficient for the interaction term measures a change within a change, i.e. how does $x_1$ affect the value of $y$ conditional on the value of $x_2$.

***

## Model comparison - Adjusted R-Squared

$R^2$ is the proportion of variance in $y$ that is explained by the model, i.e. by the combination of $x$ values. Recall, however, that $R^2$ always increases as more regressors are added to the model. Therefore, to compare different models using $R^2$ values, it is necessary to account for the __model size__ (the number of regressors).

Given that $R^2 = 1 - SSR/SST$ and the degrees of freedom in SSR and SST are given by $n-k-1$ and $n-1$ respectively, it is possible to adjust the sums of squares based on the degrees of freedom in them.

\begin{equation} \label{eq:r2_adjust}
\bar{R}^2 = 1 - \dfrac{SSR/(n-k-1)}{SST/(n-1)}
\end{equation}

Equation \eqref{eq:r2_adjust} describes the __adjusted R-squared__ property of the model. This is routinely reported in OLS output. 

Two models can be compared using the adjusted R-squared figure, as long as they have the __same outcome__, and that outcome is the the __same functional form__.

***

## Confidence Intervals for Prediction

Given a regression model, it is often desirable to develop a confidence interval for _predictions_ made with that model (as well as for the parameters). However, care must be taken to also account for the error term of the regression. I.e., it is not possible to simply substitute $x$ values in to the SRF and calcualte a $y$ value. 

\begin{equation} \label{eq:theta}
\hat\theta_0 = \hat\beta_0 + \hat\beta_1 c_1 + ... + \hat\beta_k c_k
\end{equation}

Where $\theta_0 = E(y|x_1 = c_1, ..., x_k = c_k)$

If this is done, the standard error of the predicted value ($\hat\theta_0$) will not be accurate.

It is possible to rewrite \eqref{eq:theta} to give $\beta_0 = \theta_0 - \beta_1 c_1 - ... \beta_k c_k$. Using this in the SRF to obtain $y$ gives:

\begin{equation}
y = \theta_0 + \beta_1(x_1 - c_1) + ... + \beta_k(x_k - c_k) + u
\end{equation}

Now, the OLS estimator of $\theta_0$ and its standard error are the intercept and its standard error when regressing $y_i$ on $(x_{i1} - c_1), ..., (x_{ik} - c_k)$.

For a single estimate of $y$, it is usual to take values for $c_i$ that are equivalent to the _average_ values of the corresponding $x_i$.

### Predicting $y$ vs. Predicting $E(y|x)$

The standard error for the average value of y (i.e. that described above) is not the same as the standard error for a particular outcome of y. When predicting a particular outcome, the variance in the unobserved error term must be accounted for. 

Defining the prediction error as $\hat{e}$, under CLM the standard error is given by \eqref{eq:se_e}

\begin{equation} \label{eq:se_e}
se(\hat{e}) = [se(\hat\theta_0)^2 + \hat\sigma^2]^{1/2}
\end{equation}

Following this, the 95% interval for prediction (for a large sample) is given by \eqref{eq:se_pred}

\begin{equation} \label{eq:se_pred}
\hat\theta_0 \pm 1.96 . [se(\hat\theta_0)^2 + \hat\sigma^2]^{1/2}
\end{equation}

### Prediction $y$ in a log model

In the log-level model, is is possible to write that $logy\equiv log(y)$. Given this, the predicted value of $\hat{y}$ needs to be scaled accordingly by an estimate of the expected value of $exp(u)$ (as $\hat{y}\neq exp(\widehat{logy})$).

Knowing that $u$ ~ Normal(0, $\sigma^2$) then $E(exp(u)) = exp(\sigma^2 / 2)$. As such, the prediction of $y$ is given by \eqref{eq:y_log_p}

\begin{equation} \label{eq:y_log_p}
\hat{y} = exp(\hat\sigma^2 / 2)exp(\widehat{logy})
\end{equation}

If u is _not_ normally distributed, then $n^{-1} \sum_{i=1}^nexp(\hat{u_i})$ is used as a sample estimate of E[exp(u)] and therefore:

\begin{equation}
\hat{y} = n^{-1} \sum_{i=1}^nexp(\hat{u_i})exp(\widehat{logy})
\end{equation}

Note that the R function `predict` does not automatically account for E[(exp(u))] and so the answers must be manually adjusted using the RSE from the `lm` output. 


