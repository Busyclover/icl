---
title: "Statistics and Econometrics - Lecture 7"
author: "Jim Leach"
date: "27 October 2015"
output: pdf_document
---

# Further topics in MLR

## Linear Probability Model

If the dependant variable is binary (i.e. $y = 0$ or $y=1$) then the interpretation of the $\beta$ coefficients changes.

Firstly, it is possible to re-write the generic PRF to give \eqref{eq:lpm} (noting that the ZCM assumption still holds).

\begin{equation} \label{eq:lpm}
P(y=1|x) = E(y|x) = \beta_0 + \beta_1 x_1 + ... + \beta_kx_k
\end{equation}

The value $P(y=1|x)$ is termed the __reponse probability__ and the overall regression model is termed a __linear probability model (LPM)__.

In the LPM, the $\beta$ parameters are interpreted as the change in the _probability of success_ given a one unit increase in the related independant variable, i.e. $\Delta P(y=1|x) = \beta_j\Delta x_j$. 

### Shortcomings of LPM

It is possible that when using the LPM, the predicted probability can be outside $[0, 1]$. I.e., the linear function is not suitable for modelling probablities and more advanced models should be used (e.g. logit and probit). 

Further, for LPM it is the case that

\begin{equation}
Var(u|x) = Var(y|x) = P(y=1|x)[1-P(y=1|x)]
\end{equation}

I.e., MLR5 (homoskedasticity assumption) does not hold as the variance of the error term is seen to depend on $x$. This does _not_ cause estimation bias but it _does_ invalidate the standard error calculations.

## Self-selection problems

Typically, dummy variables are used when a _program effect_ is being investigated, e.g. the effectiveness of a drug, new job training etc. 

Typically individuals are divided in to two groups, a _control_ and a _treatment_, the test results used to measure the program effect.

However, it is the case that individuals often chose whether (or not) to participate in a program. This can lead to __self selection__ problems. These problems can only be eliminated if it is possible to control for everything that may be correlated with the outcome of interest.

Often this is not possible and therefore $E(u|participation = 1) \neq E(u|participation = 0)$, i.e. the ZCM is violated and the estimate of the program effect will be biased.

## Large sample (Asymptotic) Inference

In practice MLR6 ($u\textasciitilde Normal(0, \sigma^2)$) is too strong an assumption and does not hold in general. However, MLR1-4 imply that the OLS estimators are unbiased, and MLR1-6 (CLM) imply that they are normally distributed.

The central limit theorem (CLT) provides and answer to this condundrum - i.e. how is inference performed without MLR6. In short, when $n$ is large, the OLS estimators themselves are approximately normally distributed.

Therefore it is possible to write \eqref{eq:t} even when $u$ is not normally distributed.

\begin{equation} \label{eq:t}
\dfrac{\hat\beta_j - \beta_j}{se(\hat\beta_j)} \textasciitilde t_{n-k-1}
\end{equation}

It is also known that $t_{n-k-1}$ approaches Normal(0, 1) as n increases. Therefore, how large must $n$ be to get a good approximation? In short it depends on the distribution of $u$, but in general an n of 30 is acceptable.

__$t$ and $F$ testing, and confidence intervals can all be carried out exactly the same even when MLR6 does not hold__.

(Note that even if normality in $u$ is not required, homoskedasticity still _is_ required.)

## Functional forms: Log, Quadratic, Interactions

OLS can be used to describe relationships that are not strictly linear by using nonlinear function of $x$ and $y$ (recall that the _parameters_ are still a linear function for $y$).

### Logarithmic form

Logarithms can be used in multiple ways in a linear regression, each having a slightly different effect on the coefficient interpretaion. 

These interpretations are given by the below table.

\begin{center}
\begin{tabular}{ c c c  c}
 Model & Dependant Variable & Independant Variable & Interpretation of $\beta_1$ \\ \hline
 Level - level & y & x & $\Delta y = \beta_1 \Delta x$ \\
 Level - log & y & $log (x)$ & $\Delta y = ( \beta_1 / 100 ) \% \Delta x$ \\
 Log - level & $log (y)$ & x & $\% \Delta y = ( 100 \beta_1 ) \Delta x$ \\
 Log - log & $log(y)$ & $log(x)$ & $\% \Delta y = \beta_1 \% \Delta x$
\end{tabular}
\end{center}

Log models are useful for a number of reasons:

1. They are invariant to the scale of variables since they measure _percentage_ changes;
2. For models with $y>0$ (i.e. y is never negative), then the conditional distribution of $y$ with repsect to $x$ is often heteroskedastic/skewed (i.e. the variance of $y$ changes with $x$). Taking logs can eliminate the heteroskedastic and/or the skewness problems; and
3. Taking logs of a variable can narrow the range, limiting the effect of outliers.

Variables that are often used in log form include any values that are, by definition, positive such as wage, salary, sales, market value etc (often these are Â£/$ ammounts). Variables that are very large (e.g. populations, total number of employees) are also often found in log form.

Variables that are more likely to be used in level form include those measured in years and those that are proportions (e.g. unemployment rate). This is usually due to the fact that the coefficient estimation for such variables is very simple in the level form.

### Quadratic form







