---
title: "Statistics and Econometrics - Lecture 8"
author: "Jim Leach"
date: "09 November 2015"
output: pdf_document
---

# Heteroskedasticity

## A definition

Recall the Gauss-Markov assumptions:

1. Linear (in parameter) model;
2. Random sample;
3. No perfect-collinearity;
4. ZCM;
5. _Homoskedasticity_, $Var(u_i | x_i) = \sigma^2$ for all $i$.

I.e., conditional on all the $x$'s, the variance of the unobserved error, u, is _constant_.

If that is __not__ true (i.e. if the variance of _u_ is different for different $x$ values), then the errors are __heteroskedastic__.

### Issues

Recall that even if homoskedasticity is not assumed, the OLS estimators are unbiased. MLR5 (homoskedasticity) is only required for the formula of the OLS estimator variances. This, however, _is_ important (for inference) and, as such, if there is heteroskedasticity in the data, then the _standard errors_ for the estimates _are_ biased. If the standard errors are biased, then the usual $t$ and $F$ statistical tests cannot be used to draw inferences.

## Heteroskedasticity-Robust Inference

It is possible to adjust OLS estimate standard errors to ensure the $t$ of $F$ stat is valid even with heteroskedasticty. This is knows as the __heteroskedasticity-robust procedure__.

The term "robust" is used as the $t$/$F$ stat is valid regardless of the _type_ of heteroskedasticity present in the sample (including no heteroskedasticity).

### Variance with Heteroskedasticty

For the simple regression $y_i = \beta_0 + \beta_1 x_i + u_i$ is is known that $\hat\beta_1 = \beta_1 + \dfrac{\sum_{i=1}^n (x_i - \bar{x_i})}{\sum_{i=1}^n (x_i - \bar{x_i})^2}$, and $Var(\beta_1) = \dfrac{\sum_{i=1}^n (x_i - \bar{x_i})^2 \sigma_{i}^2}{SST_{x}^2}$

When $\sigma_{i}^2 \neq \sigma^2$ then a valid esimator for the variance term is given by $Var(\beta_1) = \dfrac{\sum_{i=1}^n (x_i - \bar{x_i})^2 \hat{u_{i}^2}}{SST_{x}^2}$

Generalising to multiple linear regression with $k$ parameters, a valid estimator of $Var(\hat\beta_j)$ is given by \eqref{eq:var_robust}

\begin{equation} \label{eq:var_robust}
\widehat{Var}(\hat\beta_j) = \dfrac{\sum_{i=1}^2 \hat{r}_{ij}^2 \hat{u_{i}^2}}{SSR_j^2}
\end{equation}

Here, $\hat{r_{ij}}$ is the $i_{th}$ residual from regressing $x_j$ on all other independant variables, and $SSR_j$ is the sum of squared residuals from such a regression.



### Robust Standard Errors

The robust standard erros, r.se($\hat\beta_j$) is given by the square root of the estimate of variance. 

The _robust t stat_ is given by \eqref{eq:r_t}

\begin{equation} \label{eq:r_t}
t = \dfrac{\hat\beta_j - a_j}{rse(\hat\beta_j)}
\end{equation}

It should be noted that robust standard errors only have asymptotic justification, i.e. with small sample sizes, robust $t$ statistics will not have a distribution close to $t$ and inferences will not be correct. 

In R, the robust standard errors can be computed using the `vcovHC` function from the `sandwich` package. The `coeftest` function from `lmtest` package can then be used to perform inference on the robust standard errors returned by `vcovHC`.

The $robust F stat$ must be computed using a formula different from the original one. This is easily obtained using `linearHypothesis` in `R` with the `white.adjust` option specified.

## Testing for Heteroskedasticty

It is important to test for heteroskedasticity as:

* Robust standard errors have only asymptotic justifcation (so testing is important with small samples);
* OLS estimates are no longer BLUEs in the its presence;
* Other estimators (i.e. non-OLS) are better when the form of it is known.

A hypothesis can be tested such that $H_0 : Var(u|x_1, x_2, ..., x_k) = \sigma^2$ which is equivalent to $H_0 : E(u^2|x_1, x_2, ..., x_k) = E(u^2) = \sigma^2$.

### The Breusch-Pagan Test

In this test, it is assumed that there is a linear relationship between $u^2$ and $x_j$, i.e.

\begin{equation}
u^2 = \delta_0 + \delta_1 x_1 + \delta_k x_k + v
\end{equation}

The null of homoskedsaticity is that $H_0 : \delta_1 = \delta_2 = \delta_k = 0$

The Breush-Pagan test is performed as follows:

1. Run OLS on $y = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k +u$ and save the squared residuals, $u_i^2$;
2. OLS $u_i^2 = \delta_0 + \delta_1 x_1 + ... + \delta_k x_k$ and save the R-squared value, $R_{\hat{u^2}}^2$;
3. Compute the test-statistic:

\begin{equation} \label{eq:bp_f}
F = \dfrac{R_{\hat{u^2}}^2 / k}{(1 - R_{\hat{u^2}}^2)/(n-k-1)}
\end{equation}

4. Reject the null if the $F$ given by \eqref{eq:bp_f} is too large (or has too small a p-value)

The BP test can be performed in R using `lmtest::bptest(model)` - note that it returns $\chi^2$ values, not F statistics, but the $p$ value is still useful.

### The White Test

The Breusch-Pagan test will detect linear forms of heteroskedasticty. The White test allows for nonlinearites by using squares and crossproducts of the independent variables. 

To perform the White test:

1. OLS $y = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k +u$ and save the residuals, $u_i$, and the fitted values $\hat{y}$;
2. OLS $u_i^2 = \delta_0 + \delta_1 \hat{y} + \delta_2 \hat{y}^2$ and save the R-squared, $R_{\hat{u^2}}^2$;
3. Compute the test stat given by \eqref{eq:white_f}

\begin{equation} \label{eq:white_f}
F = \dfrac{R_{\hat{u^2}}^2 / 2}{(1 - R_{\hat{u^2}}^2)/(n-3)}
\end{equation}

4. Reject the null if the $F$ given by \eqref{eq:white_f} is too large (or has too small a p-value)

The White test can be performed in R using `lmtest::bptest(model, ~ fitted_values + I(fitted_values^2))` - note that it returns $\chi^2$ values, not F statistics, but the $p$ value is still useful.

## Weighted Least Squares

If the specific form of the heteroskedastictity is known, then more efficient estimates can be obtained than through "usual" OLS.

This is known as __weighted least squares__ in which homoskedastic error are used. 

To derive this, consider a special case where the heterskedasticty is a positive multiplicative factor, h(x): $Var(u|x) = \sigma^2 h(x)$. All that is needed is to understand what $h(x_i)\equiv h_i$ looks like. 

It is the case that $E(u_i / \sqrt{h_i}|x_i) = 0$ as $h_i$ is only a function of $x_i$ and Var($u_i / \sqrt{h_i}|x_i$) = $\sigma^2$

Hence if the whole regession equation is divided by by $\sqrt{h_i}$ then a homoskedastic model is fitted.

### WLS Example

Consider the regession given by $y = \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik} + u_i$

The WLS model, regaining homoskedasticity is given by $y/\sqrt{h_i} = \beta_0/\sqrt{h_i} + \beta_1 x_{i1}/\sqrt{h_i} + ... + \beta_k x_{ik}/\sqrt{h_i} + u_i/\sqrt{h_i}$ which satisfies the Gauss-Markov assumptions 1 to 5. 

The process for WLS without intercept is to regress $y/\sqrt{h_i}$ on {$1/\sqrt{h_i}, x_{i1}/\sqrt{h_i}, ..., x_{ik}/\sqrt{h_i}$}.

Estimating the transformed equation with OLS is an example of __generalised least squares__ where each residual is weighted by $1/\sqrt{h_i}$. The GLS estimates will be BLUE, and all that is needed is to know $h_i$.

### Feasible GLS (FGLS)

Feasible GLS estimates h(x) in order to make GLS possible (and feasible).

Typically, it is assumed that the model is flexible, i.e. Var(u|x) = $\sigma^2 exp(\delta_0 + \delta_1 x_1 + ... + \delta_k x_k)$.

The flexiblity assumption implies that $u^2 = \sigma^2 exp(\delta_0 + \delta_1 x_1 + ... + \delta_k x_k).v$ where E(v|x) = 1 and is assumed to be independent of x. 

Taking the log gives $log(u^2) = \alpha_0 + \delta_1 x_1 + ... + \delta_k x_k + e$ where E(e|x) = 0 and is independent of x. 

This can be estimated with OLS using $\hat{u}$ as an esimate of u. 

### FGLS Procedure

1. OLS $y_i = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k + u_i$ and save the residuals, $\hat{u}_i$;
2. OLS $log(\hat{u}^2) = \alpha_0 + \delta_1 x_1 + ... + \delta_k x_k + e$ and save the fitted values, $\hat{g}_i$;
3. Set $\hat{h}_i = exp(\hat{g}_i)$ and estimate the regression using WLS weights 1/$\hat{h}_i$

### FGLS Estimates

The FGLS estimates still estimate the original parameters in the OLS model, $\beta_j$ and so the interpretation remains the same. Recall that OLS is still unbiased, it is just that FGLS is used for efficiency. 

The associated $t$ and $F$ statistics have the same distribution(s) under FGLS when using large enough samples. 

If the h(x) function is wrong then it can be sensible to combine FGLS with robust standard errors. Indeed it is often bettwe to use FGLS with an incorrect h(x) than to ignore heterskedasticty all together.