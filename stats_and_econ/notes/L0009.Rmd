---
title: "Statistics and Econometrics - Lecture 9"
author: "Jim Leach"
date: "09 November 2015"
output: pdf_document
---

# Model Specification and Data Issues

## Functional form misspecification

It is possible for non-linear relationships to be modelled using a linear regression, e.g. level-log models, quadratics, and interactions.

A regression is termed _misspecified_ if the functional form is incorrect and it fails to properly account for the relationship between the dependent and explanatory variables. Misspecification __causes bias__.

Sometimes it is simply possible to use common sense/interpretation to identify misspecification, e.g. whether or not x should affect y in percentage (log) or absolute (level) terms, or for the derivative of $x_i$ to vary with x (quadratic) or with $x_2$ (interactions), or to be fixed. 

However, if the misspecification is caused by omitting a non-linear function of one or more of the regressors then a specific test can be performed.

### REgression Specification Error Test (RESET)

The key idea of the RESET test is that when the model $y = \beta_0 + \beta_1 x_1 + ...$ is correct (i.e. it satisfies ZCM), no functions of the x's should be significant when added to the model. 

Similar to a white test, the _squared_ and _cubed_ fitted values (which are functions of x) should be insignificant. 

Following from this, the following is the procedure of the RESET test:

1. OLS the original model and save the fitted values, $\hat{y}$;
2. Test $H_0 : \delta_1 = 0, \delta_2 = 0$ in the expanded model $y = \beta_0 + \beta_1 x_1 + ... + \delta_1 \hat{y}^2 + \delta_2 \hat{y}^3 + error$. The F-stat follows an $F_{2, n-k-3}$ distribution under the null;
3. Reject $H_0$ when F > c ($F_{2, n-k-3}$ critical value)

Note that when testing more than one model at once with RESET, it is possible that RESET rejects both models, or neither.

In R, RESET tests can be performed using `lmtest::resettest(model, type = "fitted")`.

## Tests against Non-nested models

If it can be said that Model B is a restricted version of Model A, then Model A "nests" Model B. Nested models can be tested using the standard exclusion F tests. However, for non-nested models this is not possible.

There are two tests for testing non-nested models:

### Comperehensive Model

In this approach, a __comprehensive model__ is created that nests both of the two non-nested models. This approach may be used if it is uncertain whether x variables should be in log-form or not, creating two competing models:

\begin{equation} \label{eq:non_log}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u
\end{equation}

\begin{equation} \label{eq:log}
y = \beta_0 + \beta_1 log(x_1) + \beta_2 log(x_2) + u
\end{equation}

The comprehensive model is written as:

\begin{equation} \label{eq:comprehensive}
y = \gamma_0 + \gamma_1 x_1 + \gamma_2 x_2 + \gamma_3 log(x_1) + \gamma_4 log(x_2) = u
\end{equation}

The acceptance of the null $H_0 : \gamma_1 = \gamma_2 = 0$ suppports \eqref{eq:log} whereas the acceptance of the null $H_0 : \gamma_3 = \gamma_4 = 0$ supports the first model.

### Davidson-MacKinnon Test

Here, the competing models are also given by \eqref{eq:non_log} and \eqref{eq:log}. In this instance, the fitted values from these two models are termed $g$ and $h$ respectively. 

If the first model \eqref{eq:non_log} is correct then $h$ should be insignificant in \eqref{eq:dm}. I.e. rejecting $H_0 : \theta = 0$ is a rejection of the first model, \eqref{eq:non_log}.

\begin{equation} \label{eq:dm}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \theta h + u
\end{equation}.

__A clear winner may not emerge.__

In R, DM tests can be performed using `lmtest::jtest(model1, model2)`.

## Proxy Variables

A model may also be misspecified due to the absence of data on an important x variable. The omitted variable bias can be reduced by using a _proxy_ variable.

A good proxy must be related to the unobserved variable. For example, in the model $y = \beta_0 + \beta_2 x_2^* + u$ $x_2^*$ is unobserved. However, a proxy variable, $x_2$ may be available whereby $x_2^* = \delta_0 + \delta_2 x_2 + v_2$. Substituting for $x_2$, is is necessary to define the conditions under which $\beta_1$ will still be an unbiased estimator.

A valid proxy is one for which:

1. ZCM holds for observed, unobserved and the proxy, i.e. $E(u | x_1, x_2^*, x_2) = 0$; and
2. If $x_2$ is controlled for, the conditional mean of $x_2^*$ does not depend on $x_1$, i.e. $E(x_2^* | x_1, x_2) = E(x_2^* | x_2) = \delta_0 + \delta_2 x_2$.

That is, u is uncorrelated with any of the x terms, and $v$ is uncorrelated with $x_1$ and $x_2$. Moreover, condition 2 implies that $z_2^* = \delta_0 + \delta_2 x_2 + v_3$ and thus:

\begin{equation}
y = (\beta_0 + \beta_2\delta_0) + \beta_1 x_1 + \beta_2\delta_2 x_2 + (u + \beta_2 v_2)
\end{equation}

The OLS estimators are unbiased as long as conditions 1 and 2 hold true. Without these conditions, biased estimators can result. The bias will depend on the signs of $\beta_j$ and $\delta_j$ and may, however, still be smaller than the omitted variable bias, though.

### Lagged dependent variables

If there are unobserved variables that cannot be reasonably proxied, it may be possible to include lagged dependent variables to account for omitted variables taht contributed to both past and current levels of y. However (obviously) past and current $y$ must be related for this to make sense (e.g. a model that predicts crime rate might use historic crime rate(s) as a lagged dependent variable).

***

## Data Issues

### Missing data

If any observation is missing data on one or more variable, it cannot be used. If this "missing-ness" is random, then the 1-5 Gauss Markov assumptions are not violated and the only consequence is a reduction in the sample size. 

Problems can arise if the data are missing in a systematic way as then the sample is potentially __nonrandom__ which violates the second Gauss-Markov assumption. 

### Non-random samples

__Exogenous sample selection__ - if the sample data are chosen based on an explanatory variable then the OLS estimators will still be unbiased under MLR1, 3 and 4. Whilsts the data may be non-random, assumption 4  (ZCM) still holds as $E(u | x_j) = 0$ for any subset of $x_j$

__Endogenous sample selection__ - if the sample is chosen based on a dependant variable, y, the OLS estimators will be biased. This is because not only is the data non-random, but $E(y | x_j > c) \neq E(y | x_j \leq c)$ (where c is a value used to sample on the dependent variable.) As such, MLR4 (ZCM) fails.

### Outliers

Outliers are "unusual" observations that are far from the "centre" of the data. OLS is generally sensiitive to outliers as large residuals, once squared, add weight to SSR.

Outliers may be simple data entry issues, so it is advised to check summary stats alongside domain-knowledge and common sense. It is not unreasonable to "fix" or exclude observations that are confirmed outliers. 

A good rule of thumb for determining outliers is if the value is in the range $3sd_{below} \leq x \leq 3sd_{above}$.

It is often advised to report OLS results with and without suspected outliers in order to demonstrate their effect(s).





